{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZSTA4NqD1_v",
        "outputId": "26b59b21-bdba-4855-acad-53b1cf4da9cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting job application workflow...\n",
            "üöÄ CoordinatorAgent 'root_agent' initiating job search for 'Senior Software Engineer - Cloud' and application process for candidate 'Alice Wonderland'...\n",
            "‚öôÔ∏è Delegating job search to job_search_agent...\n",
            "üîé Job search complete. Found 5 jobs:\n",
            "- üíº Product Manager - Senior Software Engineer - Cloud at Innovate Co. (Salary: $140,000)\n",
            "- üíº UX Designer - Senior Software Engineer - Cloud at Creative Hub (Salary: $110,000)\n",
            "- üíº Data Scientist - Senior Software Engineer - Cloud at Data Inc. (Salary: $130,000)\n",
            "- üíº DevOps Engineer - Senior Software Engineer - Cloud at Cloud Solutions (Salary: $125,000)\n",
            "- üíº Software Engineer - Senior Software Engineer - Cloud at Tech Corp (Salary: $120,000)\n",
            "‚úÖ Selected job based on skills and salary expectations: 'Data Scientist - Senior Software Engineer - Cloud' at 'Data Inc.' (Score: 30).\n",
            "üì¨ Delegating application submission to candidate_agent for job ID JOB-3595...\n",
            "üéâ Application submission status: success. Message: Application for job JOB-3595 submitted.\n",
            "Job application workflow completed.\n"
          ]
        }
      ],
      "source": [
        "import asyncio\n",
        "import nest_asyncio\n",
        "import random\n",
        "import re # Import regex for salary parsing\n",
        "\n",
        "# Ensure nest_asyncio is applied if not already done in the session\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Assuming Tool, Agent are defined in previous cells and are available in scope.\n",
        "# Re-defining them here for clarity in this single block if notebook state is reset\n",
        "class Tool:\n",
        "    def __init__(self, func, name, description):\n",
        "        self.func = func\n",
        "        self.name = name\n",
        "        self.description = description\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, name, instruction, tools: list):\n",
        "        self.name = name\n",
        "        self.instruction = instruction\n",
        "        self.tools = tools\n",
        "\n",
        "# Helper function to parse salary from string\n",
        "def parse_salary(salary_str: str) -> int:\n",
        "    if not salary_str: return 0\n",
        "    # Remove non-numeric characters except comma, then remove comma, then convert to int\n",
        "    numeric_str = re.sub(r'[^\\\\d,]', '', salary_str)\n",
        "    numeric_str = numeric_str.replace(',', '')\n",
        "    try:\n",
        "        return int(numeric_str)\n",
        "    except ValueError:\n",
        "        return 0\n",
        "\n",
        "# Re-create dummy functions for independent execution of this cell if needed\n",
        "def job_search(query: str, max_results: int = 5) -> list:\n",
        "    sample_jobs = [\n",
        "        {\"title\": f\"Software Engineer - {query}\", \"company\": \"Tech Corp\", \"location\": \"San Francisco, CA\", \"description\": \"Develop and maintain software, good with Python, Cloud.\", \"salary\": \"$120,000\"},\n",
        "        {\"title\": f\"Data Scientist - {query}\", \"company\": \"Data Inc.\", \"location\": \"New York, NY\", \"description\": \"Analyze large datasets, using AI/ML, good with Python.\", \"salary\": \"$130,000\"},\n",
        "        {\"title\": f\"Product Manager - {query}\", \"company\": \"Innovate Co.\", \"location\": \"Seattle, WA\", \"description\": \"Define product vision and roadmap.\", \"salary\": \"$140,000\"},\n",
        "        {\"title\": f\"UX Designer - {query}\", \"company\": \"Creative Hub\", \"location\": \"Austin, TX\", \"description\": \"Design user interfaces, on Cloud platforms.\", \"salary\": \"$110,000\"},\n",
        "        {\"title\": f\"DevOps Engineer - {query}\", \"company\": \"Cloud Solutions\", \"location\": \"Remote\", \"description\": \"Manage infrastructure and deployments on Cloud.\", \"salary\": \"$125,000\"}\n",
        "    ]\n",
        "    random.shuffle(sample_jobs)\n",
        "    return sample_jobs[:max_results]\n",
        "\n",
        "def submit_application(job_id: str, candidate_profile: dict) -> dict:\n",
        "    if random.random() < 0.9:\n",
        "        return {\"status\": \"success\", \"message\": f\"Application for job {job_id} submitted.\", \"job_id\": job_id, \"candidate_name\": candidate_profile.get('name')}\n",
        "    else:\n",
        "        return {\"status\": \"failure\", \"message\": f\"Failed to submit application for job {job_id}.\", \"job_id\": job_id, \"candidate_name\": candidate_profile.get('name')}\n",
        "\n",
        "# Re-create Tool instances\n",
        "job_search_tool = Tool(\n",
        "    func=job_search,\n",
        "    name=\"job_search\",\n",
        "    description=\"Searches for job listings based on a query and returns a list of job details.\"\n",
        ")\n",
        "submit_application_tool = Tool(\n",
        "    func=submit_application,\n",
        "    name=\"submit_application\",\n",
        "    description=\"Submits a job application with a given job ID and candidate profile.\"\n",
        ")\n",
        "\n",
        "# Re-create Agent instances\n",
        "job_search_agent = Agent(\n",
        "    name=\"job_search_agent\",\n",
        "    instruction=\"I search for jobs based on a given query.\",\n",
        "    tools=[job_search_tool]\n",
        ")\n",
        "candidate_agent = Agent(\n",
        "    name=\"candidate_agent\",\n",
        "    instruction=\"I manage candidate profiles and submit job applications.\",\n",
        "    tools=[submit_application_tool]\n",
        ")\n",
        "\n",
        "# Redefine CoordinatorAgent with the full run_live method (job selection and error handling)\n",
        "class CoordinatorAgent(Agent):\n",
        "    def __init__(self, name: str, instruction: str, tools: list = None, sub_agents: list = None):\n",
        "        super().__init__(name, instruction, tools if tools is not None else [])\n",
        "        self.sub_agents = sub_agents if sub_agents is not None else []\n",
        "\n",
        "    async def run_live(self, query: str, candidate_profile: dict):\n",
        "        yield f\"üöÄ CoordinatorAgent '{self.name}' initiating job search for '{query}' and application process for candidate '{candidate_profile.get('name')}'...\"\n",
        "\n",
        "        # 2a. Find the job_search_agent\n",
        "        job_search_agent_found = next((agent for agent in self.sub_agents if agent.name == \"job_search_agent\"), None)\n",
        "        if not job_search_agent_found:\n",
        "            yield \"‚ùå Error: job_search_agent not found.\"\n",
        "            return\n",
        "        job_search_tool_instance = next((tool for tool in job_search_agent_found.tools if tool.name == \"job_search\"), None)\n",
        "        if not job_search_tool_instance:\n",
        "            yield \"‚ùå Error: job_search tool not found for job_search_agent.\"\n",
        "            return\n",
        "\n",
        "        # 2b. Simulate job_search_agent using its job_search_tool with error handling\n",
        "        yield f\"‚öôÔ∏è Delegating job search to {job_search_agent_found.name}...\"\n",
        "        jobs = []\n",
        "        try:\n",
        "            jobs = job_search_tool_instance.func(query, max_results=5) # Increased max_results to have more options\n",
        "        except Exception as e:\n",
        "            yield f\"‚ùå Error during job search: {e}\"\n",
        "            return\n",
        "\n",
        "        # 2c. Yield message about jobs found\n",
        "        yield f\"üîé Job search complete. Found {len(jobs)} jobs:\" \\\n",
        "              + \"\\n\" + \"\\n\".join([f\"- üíº {job['title']} at {job['company']} (Salary: {job.get('salary', 'N/A')})\" for job in jobs])\n",
        "\n",
        "        if not jobs:\n",
        "            yield \"‚ö†Ô∏è No jobs found, unable to apply.\"\n",
        "            return\n",
        "\n",
        "        # 2d. Enhanced Job Selection Logic\n",
        "        candidate_skills = [s.lower() for s in candidate_profile.get('skills', [])]\n",
        "        salary_expectation = candidate_profile.get('salary_expectation', 0)\n",
        "\n",
        "        def score_job(job: dict) -> int:\n",
        "            score = 0\n",
        "            job_text = (job.get('title', '') + ' ' + job.get('description', '')).lower()\n",
        "            job_salary = parse_salary(job.get('salary', ''))\n",
        "\n",
        "            # Skill matching\n",
        "            matched_skills_count = sum(1 for skill in candidate_skills if skill in job_text)\n",
        "            score += matched_skills_count * 10 # Each skill match adds 10 points\n",
        "\n",
        "            # Salary matching\n",
        "            if job_salary >= salary_expectation:\n",
        "                score += 5 # Meeting salary expectation adds 5 points\n",
        "\n",
        "            # Prioritize 'Senior Engineer' in title if specifically searched for and available\n",
        "            if \"senior engineer\" in job.get('title', '').lower() and \"senior engineer\" in query.lower():\n",
        "                score += 2 # Small boost for direct title match\n",
        "\n",
        "            return score\n",
        "\n",
        "        scored_jobs = []\n",
        "        for job in jobs:\n",
        "            scored_jobs.append((score_job(job), job))\n",
        "\n",
        "        # Sort by score in descending order\n",
        "        scored_jobs.sort(key=lambda x: x[0], reverse=True)\n",
        "\n",
        "        selected_job = None\n",
        "        if scored_jobs and scored_jobs[0][0] > 0: # If at least one job has a positive score\n",
        "            selected_job = scored_jobs[0][1]\n",
        "            yield f\"‚úÖ Selected job based on skills and salary expectations: '{selected_job['title']}' at '{selected_job['company']}' (Score: {scored_jobs[0][0]}).\"\n",
        "        else: # Fallback if no job scores positively or scored_jobs is empty\n",
        "            # Original fallback logic: try to find a 'Senior Engineer' job or take the first one\n",
        "            selected_job = next((job for job in jobs if \"Senior Engineer\" in job['title']), jobs[0])\n",
        "            yield f\"‚ö†Ô∏è No jobs matched criteria strongly. Falling back to default selection: '{selected_job['title']}' at '{selected_job['company']}' (Score: 0).\"\n",
        "\n",
        "        # Assign a dummy job_id for demonstration\n",
        "        job_id = f\"JOB-{random.randint(1000, 9999)}\"\n",
        "\n",
        "        # 2e. Find the candidate_agent\n",
        "        candidate_agent_found = next((agent for agent in self.sub_agents if agent.name == \"candidate_agent\"), None)\n",
        "        if not candidate_agent_found:\n",
        "            yield \"‚ùå Error: candidate_agent not found.\"\n",
        "            return\n",
        "        submit_application_tool_instance = next((tool for tool in candidate_agent_found.tools if tool.name == \"submit_application\"), None)\n",
        "        if not submit_application_tool_instance:\n",
        "            yield \"‚ùå Error: submit_application tool not found for candidate_agent.\"\n",
        "            return\n",
        "\n",
        "        # 2f. Simulate candidate_agent using its submit_application_tool with error handling\n",
        "        yield f\"üì¨ Delegating application submission to {candidate_agent_found.name} for job ID {job_id}...\"\n",
        "        application_status = {}\n",
        "        try:\n",
        "            application_status = submit_application_tool_instance.func(job_id, candidate_profile)\n",
        "        except Exception as e:\n",
        "            yield f\"‚ùå Error during application submission: {e}\"\n",
        "            return\n",
        "\n",
        "        # 2g. Yield application submission status\n",
        "        yield f\"üéâ Application submission status: {application_status.get('status', 'unknown')}. Message: {application_status.get('message', 'An unknown error occurred.')}\"\n",
        "\n",
        "\n",
        "# Re-instantiate the CoordinatorAgent with the new class definition\n",
        "root_agent = CoordinatorAgent(\n",
        "    name=\"root_agent\",\n",
        "    instruction=\"I orchestrate the job search and application process by coordinating between the job search and candidate agents to find suitable jobs and submit applications.\",\n",
        "    sub_agents=[job_search_agent, candidate_agent]\n",
        ")\n",
        "\n",
        "# Sample data for direct execution\n",
        "sample_candidate_profile = {\n",
        "    \"name\": \"Alice Wonderland\",\n",
        "    \"email\": \"alice@example.com\",\n",
        "    \"resume_link\": \"https://example.com/alice_resume.pdf\",\n",
        "    \"experience\": \"10+ years as Software Engineer, 5 years in Cloud Architecture\",\n",
        "    \"skills\": [\"Python\", \"Cloud\", \"AI/ML\", \"Data Analysis\", \"DevOps\"],\n",
        "    \"salary_expectation\": 135000\n",
        "}\n",
        "sample_job_query = \"Senior Software Engineer - Cloud\"\n",
        "\n",
        "async def run_workflow_without_streamlit(job_query, candidate_profile):\n",
        "    print(\"Starting job application workflow...\")\n",
        "    async for step_output in root_agent.run_live(job_query, candidate_profile):\n",
        "        print(step_output)\n",
        "    print(\"Job application workflow completed.\")\n",
        "\n",
        "# Run the workflow\n",
        "if __name__ == '__main__':\n",
        "    # Ensure an event loop is running. nest_asyncio.apply() usually handles this for Colab.\n",
        "    asyncio.run(run_workflow_without_streamlit(sample_job_query, sample_candidate_profile))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84d010b1"
      },
      "source": [
        "# Task\n",
        "Create a new workflow that analyzes a candidate's resume against a job description to identify skills gaps. This workflow should use the Gemini 2.5 Flash model for analysis and replace the previous job search and application submission functionality. Provide a dummy `resume_path` and `job_url` to demonstrate the new workflow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20308e9e"
      },
      "source": [
        "## Add necessary imports and LLM setup\n",
        "\n",
        "### Subtask:\n",
        "In the new cell, add imports for PDF processing libraries (e.g., `PyPDF2` or `fitz`), web scraping (e.g., `requests`, `BeautifulSoup`), and initialize the `google.generativeai` client with the Gemini 2.5 Flash model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cb33859"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires adding necessary imports for PDF processing, web scraping, and setting up the Google Generative AI client with the Gemini 2.5 Flash model. I will create a code block that imports the specified libraries and initializes the generative AI client.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "0f889931",
        "outputId": "66cf1680-3fc9-4d98-9bdf-b6e037a2e5ec"
      },
      "source": [
        "import google.generativeai as gen\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import PyPDF2\n",
        "import os\n",
        "\n",
        "# Configure the Google Generative AI client\n",
        "# Make sure to set your GOOGLE_API_KEY in your environment variables\n",
        "GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')\n",
        "if not GOOGLE_API_KEY:\n",
        "    raise ValueError(\"GOOGLE_API_KEY environment variable not set.\")\n",
        "gen.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "# Load the Gemini 1.5 Flash model\n",
        "llm_model = gen.get_model('gemini-1.5-flash')\n",
        "\n",
        "print(\"Imports and Google Generative AI client configured successfully.\")\n",
        "print(f\"Loaded LLM model: {llm_model.name}\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'PyPDF2'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3614097378.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mPyPDF2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'PyPDF2'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfbc776a"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code block failed because the `PyPDF2` library was not found. I need to install `PyPDF2` using `!pip install` before importing it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47c30681",
        "outputId": "aec0320d-010a-4e59-b883-903457bb22fc"
      },
      "source": [
        "!pip install PyPDF2\n",
        "print(\"PyPDF2 installed successfully.\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n",
            "PyPDF2 installed successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0ccdfef"
      },
      "source": [
        "**Reasoning**:\n",
        "Since PyPDF2 is now installed, I need to re-run the code block that imports all necessary libraries and initializes the Google Generative AI client, as this was the original intent of the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "f7232b25",
        "outputId": "537f600b-df43-4c1d-8083-17172ad2d601"
      },
      "source": [
        "import google.generativeai as gen\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import PyPDF2\n",
        "import os\n",
        "\n",
        "# Configure the Google Generative AI client\n",
        "# Make sure to set your GOOGLE_API_KEY in your environment variables\n",
        "GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')\n",
        "if not GOOGLE_API_KEY:\n",
        "    raise ValueError(\"GOOGLE_API_KEY environment variable not set.\")\n",
        "gen.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "# Load the Gemini 1.5 Flash model\n",
        "llm_model = gen.get_model('gemini-1.5-flash')\n",
        "\n",
        "print(\"Imports and Google Generative AI client configured successfully.\")\n",
        "print(f\"Loaded LLM model: {llm_model.name}\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "GOOGLE_API_KEY environment variable not set.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3614097378.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mGOOGLE_API_KEY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GOOGLE_API_KEY'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mGOOGLE_API_KEY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GOOGLE_API_KEY environment variable not set.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mGOOGLE_API_KEY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: GOOGLE_API_KEY environment variable not set."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "856a58a8"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code block failed because the `GOOGLE_API_KEY` environment variable was not set. To fix this, I will add a line to set a placeholder `GOOGLE_API_KEY` environment variable directly within the code, ensuring the `gen.configure()` call can proceed. The user should replace `\"YOUR_GOOGLE_API_KEY_HERE\"` with their actual key.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "08d182f5",
        "outputId": "ac286461-6702-48bc-e4e9-333980ae6ed6"
      },
      "source": [
        "import google.generativeai as gen\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import PyPDF2\n",
        "import os\n",
        "\n",
        "# Configure the Google Generative AI client\n",
        "# Make sure to set your GOOGLE_API_KEY in your environment variables\n",
        "# IMPORTANT: Replace \"YOUR_GOOGLE_API_KEY_HERE\" with your actual API key.\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"YOUR_GOOGLE_API_KEY_HERE\" # Placeholder, user needs to replace this\n",
        "GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')\n",
        "if not GOOGLE_API_KEY or GOOGLE_API_KEY == \"YOUR_GOOGLE_API_KEY_HERE\":\n",
        "    raise ValueError(\"GOOGLE_API_KEY environment variable not set or is a placeholder. Please set it with your actual key.\")\n",
        "gen.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "# Load the Gemini 1.5 Flash model\n",
        "llm_model = gen.get_model('gemini-1.5-flash')\n",
        "\n",
        "print(\"Imports and Google Generative AI client configured successfully.\")\n",
        "print(f\"Loaded LLM model: {llm_model.name}\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "GOOGLE_API_KEY environment variable not set or is a placeholder. Please set it with your actual key.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-58931964.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mGOOGLE_API_KEY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GOOGLE_API_KEY'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mGOOGLE_API_KEY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mGOOGLE_API_KEY\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"YOUR_GOOGLE_API_KEY_HERE\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GOOGLE_API_KEY environment variable not set or is a placeholder. Please set it with your actual key.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mGOOGLE_API_KEY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: GOOGLE_API_KEY environment variable not set or is a placeholder. Please set it with your actual key."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2a8876f"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code block failed because the `GOOGLE_API_KEY` environment variable was either not set or was still the placeholder value, resulting in a `ValueError`. To fix this and allow the code to proceed, I will modify the code to explicitly prompt the user for their API key if it's not already set as a valid value, making it easier for them to provide it interactively.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "04e52186",
        "outputId": "e41ed979-abb9-442a-b65e-846059231ab7"
      },
      "source": [
        "import google.generativeai as gen\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import PyPDF2\n",
        "import os\n",
        "\n",
        "# Configure the Google Generative AI client\n",
        "# Make sure to set your GOOGLE_API_KEY in your environment variables or provide it when prompted.\n",
        "GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')\n",
        "\n",
        "if not GOOGLE_API_KEY or GOOGLE_API_KEY == \"YOUR_GOOGLE_API_KEY_HERE\":\n",
        "    print(\"GOOGLE_API_KEY environment variable not set or is a placeholder.\")\n",
        "    print(\"Please provide your Google API Key to continue.\")\n",
        "    GOOGLE_API_KEY = input(\"Enter your Google API Key: \").strip()\n",
        "\n",
        "    # Optionally, set it as an environment variable for subsequent runs in this session\n",
        "    if GOOGLE_API_KEY:\n",
        "        os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
        "    else:\n",
        "        raise ValueError(\"Google API Key is required but was not provided.\")\n",
        "\n",
        "gen.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "# Load the Gemini 1.5 Flash model\n",
        "llm_model = gen.get_model('gemini-1.5-flash')\n",
        "\n",
        "print(\"Imports and Google Generative AI client configured successfully.\")\n",
        "print(f\"Loaded LLM model: {llm_model.name}\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GOOGLE_API_KEY environment variable not set or is a placeholder.\n",
            "Please provide your Google API Key to continue.\n",
            "Enter your Google API Key: AIzaSyDBCTU5EywlpGeSgur0oip6LtaJVWmPkaE\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:404 GET /v1beta/models/gemini-1.5-flash?%24alt=json%3Benum-encoding%3Dint (::1) 306.32ms\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NotFound",
          "evalue": "404 GET https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash?%24alt=json%3Benum-encoding%3Dint: Model is not found: models/gemini-1.5-flash for api version v1beta",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotFound\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3520868455.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Load the Gemini 1.5 Flash model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mllm_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gemini-1.5-flash'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Imports and Google Generative AI client configured successfully.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/generativeai/models.py\u001b[0m in \u001b[0;36mget_model\u001b[0;34m(name, client, request_options)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_types\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_model_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"models/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mget_base_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tunedModels/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mget_tuned_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/generativeai/models.py\u001b[0m in \u001b[0;36mget_base_model\u001b[0;34m(name, client, request_options)\u001b[0m\n\u001b[1;32m     98\u001b[0m         )\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mrequest_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_types\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/ai/generativelanguage_v1beta/services/model_service/client.py\u001b[0m in \u001b[0;36mget_model\u001b[0;34m(self, request, name, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m         \u001b[0;31m# Send the request.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 800\u001b[0;31m         response = rpc(\n\u001b[0m\u001b[1;32m    801\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m             \u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/gapic_v1/method.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maximum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiplier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_multiplier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m             )\n\u001b[0;32m--> 294\u001b[0;31m             return retry_target(\n\u001b[0m\u001b[1;32m    295\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predicate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;31m# defer to shared logic for handling errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             next_sleep = _retry_error_helper(\n\u001b[0m\u001b[1;32m    157\u001b[0m                 \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m                 \u001b[0mdeadline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_base.py\u001b[0m in \u001b[0;36m_retry_error_helper\u001b[0;34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0moriginal_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         )\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfinal_exc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msource_exc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mon_error_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0mon_error_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misawaitable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ASYNC_RETRY_WARNING\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/timeout.py\u001b[0m in \u001b[0;36mfunc_with_timeout\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"timeout\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremaining_timeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc_with_timeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0merror_remapped_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallable_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_grpc_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/ai/generativelanguage_v1beta/services/model_service/transports/rest.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, request, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    852\u001b[0m             \u001b[0;31m# subclass.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m400\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 854\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mcore_exceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_http_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    856\u001b[0m             \u001b[0;31m# Return the response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFound\u001b[0m: 404 GET https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash?%24alt=json%3Benum-encoding%3Dint: Model is not found: models/gemini-1.5-flash for api version v1beta"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b6b385c"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code block failed with a `NotFound` error for the `gemini-1.5-flash` model. This indicates that the model name used in `gen.get_model()` might be incorrect or not available in the current context. I will update the model name to `gemini-1.5-flash-001`, which is a common and often more stable identifier for this model, to resolve the `NotFound` error.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "5242c39d",
        "outputId": "efe27538-0c05-4349-9d37-aa3cc343dcb0"
      },
      "source": [
        "import google.generativeai as gen\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import PyPDF2\n",
        "import os\n",
        "\n",
        "# Configure the Google Generative AI client\n",
        "# Make sure to set your GOOGLE_API_KEY in your environment variables or provide it when prompted.\n",
        "GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')\n",
        "\n",
        "if not GOOGLE_API_KEY or GOOGLE_API_KEY == \"YOUR_GOOGLE_API_KEY_HERE\":\n",
        "    print(\"GOOGLE_API_KEY environment variable not set or is a placeholder.\")\n",
        "    print(\"Please provide your Google API Key to continue.\")\n",
        "    GOOGLE_API_KEY = input(\"Enter your Google API Key: \").strip()\n",
        "\n",
        "    # Optionally, set it as an environment variable for subsequent runs in this session\n",
        "    if GOOGLE_API_KEY:\n",
        "        os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
        "    else:\n",
        "        raise ValueError(\"Google API Key is required but was not provided.\")\n",
        "\n",
        "gen.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "# Load the Gemini 1.5 Flash model\n",
        "llm_model = gen.get_model('gemini-1.5-flash-001')\n",
        "\n",
        "print(\"Imports and Google Generative AI client configured successfully.\")\n",
        "print(f\"Loaded LLM model: {llm_model.name}\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:404 GET /v1beta/models/gemini-1.5-flash-001?%24alt=json%3Benum-encoding%3Dint (::1) 283.07ms\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NotFound",
          "evalue": "404 GET https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-001?%24alt=json%3Benum-encoding%3Dint: Model is not found: models/gemini-1.5-flash-001 for api version v1beta",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotFound\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2936944301.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Load the Gemini 1.5 Flash model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mllm_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gemini-1.5-flash-001'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Imports and Google Generative AI client configured successfully.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/generativeai/models.py\u001b[0m in \u001b[0;36mget_model\u001b[0;34m(name, client, request_options)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_types\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_model_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"models/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mget_base_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tunedModels/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mget_tuned_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/generativeai/models.py\u001b[0m in \u001b[0;36mget_base_model\u001b[0;34m(name, client, request_options)\u001b[0m\n\u001b[1;32m     98\u001b[0m         )\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mrequest_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_types\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/ai/generativelanguage_v1beta/services/model_service/client.py\u001b[0m in \u001b[0;36mget_model\u001b[0;34m(self, request, name, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m         \u001b[0;31m# Send the request.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 800\u001b[0;31m         response = rpc(\n\u001b[0m\u001b[1;32m    801\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m             \u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/gapic_v1/method.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maximum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiplier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_multiplier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m             )\n\u001b[0;32m--> 294\u001b[0;31m             return retry_target(\n\u001b[0m\u001b[1;32m    295\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predicate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;31m# defer to shared logic for handling errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             next_sleep = _retry_error_helper(\n\u001b[0m\u001b[1;32m    157\u001b[0m                 \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m                 \u001b[0mdeadline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_base.py\u001b[0m in \u001b[0;36m_retry_error_helper\u001b[0;34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0moriginal_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         )\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfinal_exc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msource_exc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mon_error_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0mon_error_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misawaitable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ASYNC_RETRY_WARNING\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/timeout.py\u001b[0m in \u001b[0;36mfunc_with_timeout\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"timeout\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremaining_timeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc_with_timeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0merror_remapped_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallable_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_grpc_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/ai/generativelanguage_v1beta/services/model_service/transports/rest.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, request, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    852\u001b[0m             \u001b[0;31m# subclass.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m400\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 854\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mcore_exceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_http_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    856\u001b[0m             \u001b[0;31m# Return the response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFound\u001b[0m: 404 GET https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-001?%24alt=json%3Benum-encoding%3Dint: Model is not found: models/gemini-1.5-flash-001 for api version v1beta"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cac7f68a"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code block failed with a `NotFound` error for `gemini-1.5-flash-001`, and previous attempts with `gemini-1.5-flash` also failed. This suggests that the model might not be available or the name is incorrect for the current API setup. To address this and allow the code to proceed, I will revert the model name to `gemini-1.5-flash` (as per the subtask instruction) but add a comment recommending `gemini-pro` as a widely available alternative if `gemini-1.5-flash` continues to cause a `NotFound` error.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "840b56be",
        "outputId": "ec556b2d-6ae8-4d6a-9018-72a4ad03ec8e"
      },
      "source": [
        "import google.generativeai as gen\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import PyPDF2\n",
        "import os\n",
        "\n",
        "# Configure the Google Generative AI client\n",
        "# Make sure to set your GOOGLE_API_KEY in your environment variables or provide it when prompted.\n",
        "GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')\n",
        "\n",
        "if not GOOGLE_API_KEY or GOOGLE_API_KEY == \"YOUR_GOOGLE_API_KEY_HERE\":\n",
        "    print(\"GOOGLE_API_KEY environment variable not set or is a placeholder.\")\n",
        "    print(\"Please provide your Google API Key to continue.\")\n",
        "    GOOGLE_API_KEY = input(\"Enter your Google API Key: \").strip()\n",
        "\n",
        "    # Optionally, set it as an environment variable for subsequent runs in this session\n",
        "    if GOOGLE_API_KEY:\n",
        "        os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
        "    else:\n",
        "        raise ValueError(\"Google API Key is required but was not provided.\")\n",
        "\n",
        "gen.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "# Load the Gemini 1.5 Flash model\n",
        "# If 'gemini-1.5-flash' is not found, try 'gemini-pro' as a widely available alternative.\n",
        "llm_model = gen.get_model('gemini-1.5-flash')\n",
        "\n",
        "print(\"Imports and Google Generative AI client configured successfully.\")\n",
        "print(f\"Loaded LLM model: {llm_model.name}\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:404 GET /v1beta/models/gemini-1.5-flash?%24alt=json%3Benum-encoding%3Dint (::1) 291.80ms\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NotFound",
          "evalue": "404 GET https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash?%24alt=json%3Benum-encoding%3Dint: Model is not found: models/gemini-1.5-flash for api version v1beta",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotFound\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1070288905.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Load the Gemini 1.5 Flash model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# If 'gemini-1.5-flash' is not found, try 'gemini-pro' as a widely available alternative.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mllm_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gemini-1.5-flash'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Imports and Google Generative AI client configured successfully.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/generativeai/models.py\u001b[0m in \u001b[0;36mget_model\u001b[0;34m(name, client, request_options)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_types\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_model_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"models/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mget_base_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tunedModels/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mget_tuned_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/generativeai/models.py\u001b[0m in \u001b[0;36mget_base_model\u001b[0;34m(name, client, request_options)\u001b[0m\n\u001b[1;32m     98\u001b[0m         )\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mrequest_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_types\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/ai/generativelanguage_v1beta/services/model_service/client.py\u001b[0m in \u001b[0;36mget_model\u001b[0;34m(self, request, name, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m         \u001b[0;31m# Send the request.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 800\u001b[0;31m         response = rpc(\n\u001b[0m\u001b[1;32m    801\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m             \u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/gapic_v1/method.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maximum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiplier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_multiplier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m             )\n\u001b[0;32m--> 294\u001b[0;31m             return retry_target(\n\u001b[0m\u001b[1;32m    295\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predicate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;31m# defer to shared logic for handling errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             next_sleep = _retry_error_helper(\n\u001b[0m\u001b[1;32m    157\u001b[0m                 \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m                 \u001b[0mdeadline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_base.py\u001b[0m in \u001b[0;36m_retry_error_helper\u001b[0;34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0moriginal_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         )\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfinal_exc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msource_exc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mon_error_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0mon_error_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misawaitable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ASYNC_RETRY_WARNING\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/timeout.py\u001b[0m in \u001b[0;36mfunc_with_timeout\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"timeout\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremaining_timeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc_with_timeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0merror_remapped_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallable_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_grpc_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/ai/generativelanguage_v1beta/services/model_service/transports/rest.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, request, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    852\u001b[0m             \u001b[0;31m# subclass.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m400\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 854\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mcore_exceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_http_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    856\u001b[0m             \u001b[0;31m# Return the response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFound\u001b[0m: 404 GET https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash?%24alt=json%3Benum-encoding%3Dint: Model is not found: models/gemini-1.5-flash for api version v1beta"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3475304d"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempts to load `gemini-1.5-flash` and `gemini-1.5-flash-001` resulted in `NotFound` errors. To unblock the execution and proceed with the task, I will use `gemini-pro`, which is a widely available and stable model, and is likely to resolve the `NotFound` error.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "ddb9d09e",
        "outputId": "046c6a9f-cc6d-42d1-d359-502f42fc94b3"
      },
      "source": [
        "import google.generativeai as gen\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import PyPDF2\n",
        "import os\n",
        "\n",
        "# Configure the Google Generative AI client\n",
        "# Make sure to set your GOOGLE_API_KEY in your environment variables or provide it when prompted.\n",
        "GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')\n",
        "\n",
        "if not GOOGLE_API_KEY or GOOGLE_API_KEY == \"YOUR_GOOGLE_API_KEY_HERE\":\n",
        "    print(\"GOOGLE_API_KEY environment variable not set or is a placeholder.\")\n",
        "    print(\"Please provide your Google API Key to continue.\")\n",
        "    GOOGLE_API_KEY = input(\"Enter your Google API Key: \").strip()\n",
        "\n",
        "    # Optionally, set it as an environment variable for subsequent runs in this session\n",
        "    if GOOGLE_API_KEY:\n",
        "        os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
        "    else:\n",
        "        raise ValueError(\"Google API Key is required but was not provided.\")\n",
        "\n",
        "gen.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "# Load a widely available Gemini model as 'gemini-1.5-flash' is not found.\n",
        "# If 'gemini-1.5-flash' or 'gemini-1.5-flash-001' are not available, 'gemini-pro' is a robust alternative.\n",
        "llm_model = gen.get_model('gemini-pro')\n",
        "\n",
        "print(\"Imports and Google Generative AI client configured successfully.\")\n",
        "print(f\"Loaded LLM model: {llm_model.name}\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:404 GET /v1beta/models/gemini-pro?%24alt=json%3Benum-encoding%3Dint (::1) 299.27ms\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NotFound",
          "evalue": "404 GET https://generativelanguage.googleapis.com/v1beta/models/gemini-pro?%24alt=json%3Benum-encoding%3Dint: Model is not found: models/gemini-pro for api version v1beta",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotFound\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-648775496.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Load a widely available Gemini model as 'gemini-1.5-flash' is not found.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# If 'gemini-1.5-flash' or 'gemini-1.5-flash-001' are not available, 'gemini-pro' is a robust alternative.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mllm_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gemini-pro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Imports and Google Generative AI client configured successfully.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/generativeai/models.py\u001b[0m in \u001b[0;36mget_model\u001b[0;34m(name, client, request_options)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_types\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_model_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"models/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mget_base_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tunedModels/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mget_tuned_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/generativeai/models.py\u001b[0m in \u001b[0;36mget_base_model\u001b[0;34m(name, client, request_options)\u001b[0m\n\u001b[1;32m     98\u001b[0m         )\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mrequest_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_types\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/ai/generativelanguage_v1beta/services/model_service/client.py\u001b[0m in \u001b[0;36mget_model\u001b[0;34m(self, request, name, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m         \u001b[0;31m# Send the request.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 800\u001b[0;31m         response = rpc(\n\u001b[0m\u001b[1;32m    801\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m             \u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/gapic_v1/method.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maximum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiplier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_multiplier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m             )\n\u001b[0;32m--> 294\u001b[0;31m             return retry_target(\n\u001b[0m\u001b[1;32m    295\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predicate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;31m# defer to shared logic for handling errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             next_sleep = _retry_error_helper(\n\u001b[0m\u001b[1;32m    157\u001b[0m                 \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m                 \u001b[0mdeadline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_base.py\u001b[0m in \u001b[0;36m_retry_error_helper\u001b[0;34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0moriginal_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         )\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfinal_exc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msource_exc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mon_error_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0mon_error_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misawaitable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ASYNC_RETRY_WARNING\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/timeout.py\u001b[0m in \u001b[0;36mfunc_with_timeout\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"timeout\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremaining_timeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc_with_timeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0merror_remapped_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallable_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_grpc_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/ai/generativelanguage_v1beta/services/model_service/transports/rest.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, request, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    852\u001b[0m             \u001b[0;31m# subclass.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m400\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 854\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mcore_exceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_http_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    856\u001b[0m             \u001b[0;31m# Return the response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFound\u001b[0m: 404 GET https://generativelanguage.googleapis.com/v1beta/models/gemini-pro?%24alt=json%3Benum-encoding%3Dint: Model is not found: models/gemini-pro for api version v1beta"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "131ad191"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempts to load `gemini-1.5-flash`, `gemini-1.5-flash-001`, and `gemini-pro` all resulted in `NotFound` errors. To unblock the execution and proceed, I will try using `gemini-1.0-pro`, which is a widely available and stable model, and is highly likely to resolve the `NotFound` error.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "b95faf67",
        "outputId": "e34d3f5e-6472-4998-896d-8818ec2aa363"
      },
      "source": [
        "import google.generativeai as gen\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import PyPDF2\n",
        "import os\n",
        "\n",
        "# Configure the Google Generative AI client\n",
        "# Make sure to set your GOOGLE_API_KEY in your environment variables or provide it when prompted.\n",
        "GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')\n",
        "\n",
        "if not GOOGLE_API_KEY or GOOGLE_API_KEY == \"YOUR_GOOGLE_API_KEY_HERE\":\n",
        "    print(\"GOOGLE_API_KEY environment variable not set or is a placeholder.\")\n",
        "    print(\"Please provide your Google API Key to continue.\")\n",
        "    GOOGLE_API_KEY = input(\"Enter your Google API Key: \").strip()\n",
        "\n",
        "    # Optionally, set it as an environment variable for subsequent runs in this session\n",
        "    if GOOGLE_API_KEY:\n",
        "        os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
        "    else:\n",
        "        raise ValueError(\"Google API Key is required but was not provided.\")\n",
        "\n",
        "gen.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "# Load a widely available Gemini model as 'gemini-1.5-flash' is not found.\n",
        "# Using 'gemini-1.0-pro' as a robust alternative if previous attempts ('gemini-1.5-flash', 'gemini-pro') failed.\n",
        "llm_model = gen.get_model('gemini-1.0-pro')\n",
        "\n",
        "print(\"Imports and Google Generative AI client configured successfully.\")\n",
        "print(f\"Loaded LLM model: {llm_model.name}\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:404 GET /v1beta/models/gemini-1.0-pro?%24alt=json%3Benum-encoding%3Dint (::1) 365.40ms\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NotFound",
          "evalue": "404 GET https://generativelanguage.googleapis.com/v1beta/models/gemini-1.0-pro?%24alt=json%3Benum-encoding%3Dint: Model is not found: models/gemini-1.0-pro for api version v1beta",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotFound\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2957169575.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Load a widely available Gemini model as 'gemini-1.5-flash' is not found.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Using 'gemini-1.0-pro' as a robust alternative if previous attempts ('gemini-1.5-flash', 'gemini-pro') failed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mllm_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gemini-1.0-pro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Imports and Google Generative AI client configured successfully.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/generativeai/models.py\u001b[0m in \u001b[0;36mget_model\u001b[0;34m(name, client, request_options)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_types\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_model_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"models/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mget_base_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tunedModels/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mget_tuned_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/generativeai/models.py\u001b[0m in \u001b[0;36mget_base_model\u001b[0;34m(name, client, request_options)\u001b[0m\n\u001b[1;32m     98\u001b[0m         )\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mrequest_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_types\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/ai/generativelanguage_v1beta/services/model_service/client.py\u001b[0m in \u001b[0;36mget_model\u001b[0;34m(self, request, name, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m         \u001b[0;31m# Send the request.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 800\u001b[0;31m         response = rpc(\n\u001b[0m\u001b[1;32m    801\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m             \u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/gapic_v1/method.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maximum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiplier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_multiplier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m             )\n\u001b[0;32m--> 294\u001b[0;31m             return retry_target(\n\u001b[0m\u001b[1;32m    295\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predicate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;31m# defer to shared logic for handling errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             next_sleep = _retry_error_helper(\n\u001b[0m\u001b[1;32m    157\u001b[0m                 \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m                 \u001b[0mdeadline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_base.py\u001b[0m in \u001b[0;36m_retry_error_helper\u001b[0;34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0moriginal_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         )\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfinal_exc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msource_exc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mon_error_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0mon_error_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misawaitable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ASYNC_RETRY_WARNING\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/timeout.py\u001b[0m in \u001b[0;36mfunc_with_timeout\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"timeout\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremaining_timeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc_with_timeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0merror_remapped_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallable_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_grpc_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/ai/generativelanguage_v1beta/services/model_service/transports/rest.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, request, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    852\u001b[0m             \u001b[0;31m# subclass.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m400\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 854\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mcore_exceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_http_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    856\u001b[0m             \u001b[0;31m# Return the response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFound\u001b[0m: 404 GET https://generativelanguage.googleapis.com/v1beta/models/gemini-1.0-pro?%24alt=json%3Benum-encoding%3Dint: Model is not found: models/gemini-1.0-pro for api version v1beta"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24b65e21"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempts to load several Gemini models, including 'gemini-1.5-flash', 'gemini-1.5-flash-001', 'gemini-pro', and 'gemini-1.0-pro', have all resulted in `NotFound` errors. This strongly suggests an underlying issue with the API key's access to generative models or a regional availability problem, rather than just an incorrect model name. To robustly handle this, I will modify the code to first try loading the intended 'gemini-1.5-flash' model. If that fails, it will attempt to list all available generative models and then try to load the first available one as a fallback. If no models can be loaded, it will provide a clear error message suggesting the user verify their API key and region.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "cd195be6",
        "outputId": "f9913607-97e5-46dd-fda1-42d060d4d644"
      },
      "source": [
        "import google.generativeai as gen\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import PyPDF2\n",
        "import os\n",
        "\n",
        "# Configure the Google Generative AI client\n",
        "# Make sure to set your GOOGLE_API_KEY in your environment variables or provide it when prompted.\n",
        "GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')\n",
        "\n",
        "if not GOOGLE_API_KEY or GOOGLE_API_KEY == \"YOUR_GOOGLE_API_KEY_HERE\":\n",
        "    print(\"GOOGLE_API_KEY environment variable not set or is a placeholder.\")\n",
        "    print(\"Please provide your Google API Key to continue.\")\n",
        "    GOOGLE_API_KEY = input(\"Enter your Google API Key: \").strip()\n",
        "\n",
        "    # Optionally, set it as an environment variable for subsequent runs in this session\n",
        "    if GOOGLE_API_KEY:\n",
        "        os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
        "    else:\n",
        "        raise ValueError(\"Google API Key is required but was not provided.\")\n",
        "\n",
        "gen.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "llm_model = None\n",
        "tried_model_names = ['gemini-1.5-flash', 'gemini-pro', 'gemini-1.0-pro']\n",
        "\n",
        "for model_name in tried_model_names:\n",
        "    try:\n",
        "        llm_model = gen.get_model(model_name)\n",
        "        print(f\"Successfully loaded LLM model: {llm_model.name}\")\n",
        "        break # Exit loop if a model is successfully loaded\n",
        "    except Exception as e:\n",
        "        print(f\"WARNING: Could not load model '{model_name}': {e}\")\n",
        "\n",
        "if not llm_model:\n",
        "    print(\"Attempting to list available models to find an alternative...\")\n",
        "    try:\n",
        "        # Filter for models that support text generation\n",
        "        available_generative_models = [m.name for m in gen.list_models() if \"generateContent\" in m.supported_generation_methods]\n",
        "        if available_generative_models:\n",
        "            print(f\"Available generative models: {', '.join(available_generative_models)}\")\n",
        "            # Try to pick the first available generative model as a fallback\n",
        "            fallback_model_name = available_generative_models[0]\n",
        "            print(f\"Attempting to load fallback model: {fallback_model_name}\")\n",
        "            llm_model = gen.get_model(fallback_model_name)\n",
        "            print(f\"Successfully loaded fallback LLM model: {llm_model.name}\")\n",
        "        else:\n",
        "            raise ValueError(\"No generative models found with the provided API key that support 'generateContent'.\")\n",
        "    except Exception as list_e:\n",
        "        raise ValueError(f\"Error listing models or no fallback model found: {list_e}. Please check your API key and region settings for Google Generative AI access.\")\n",
        "\n",
        "if llm_model:\n",
        "    print(\"Imports and Google Generative AI client configured successfully.\")\n",
        "else:\n",
        "    raise ValueError(\"Failed to load any LLM model after multiple attempts. Please ensure your GOOGLE_API_KEY is valid and has access to Google Generative AI models in your region.\")\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:404 GET /v1beta/models/gemini-1.5-flash?%24alt=json%3Benum-encoding%3Dint (::1) 314.50ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: Could not load model 'gemini-1.5-flash': 404 GET https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash?%24alt=json%3Benum-encoding%3Dint: Model is not found: models/gemini-1.5-flash for api version v1beta\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:404 GET /v1beta/models/gemini-pro?%24alt=json%3Benum-encoding%3Dint (::1) 243.07ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: Could not load model 'gemini-pro': 404 GET https://generativelanguage.googleapis.com/v1beta/models/gemini-pro?%24alt=json%3Benum-encoding%3Dint: Model is not found: models/gemini-pro for api version v1beta\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:404 GET /v1beta/models/gemini-1.0-pro?%24alt=json%3Benum-encoding%3Dint (::1) 266.28ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: Could not load model 'gemini-1.0-pro': 404 GET https://generativelanguage.googleapis.com/v1beta/models/gemini-1.0-pro?%24alt=json%3Benum-encoding%3Dint: Model is not found: models/gemini-1.0-pro for api version v1beta\n",
            "Attempting to list available models to find an alternative...\n",
            "Available generative models: models/gemini-2.5-pro-preview-03-25, models/gemini-2.5-flash-preview-05-20, models/gemini-2.5-flash, models/gemini-2.5-flash-lite-preview-06-17, models/gemini-2.5-pro-preview-05-06, models/gemini-2.5-pro-preview-06-05, models/gemini-2.5-pro, models/gemini-2.0-flash-exp, models/gemini-2.0-flash, models/gemini-2.0-flash-001, models/gemini-2.0-flash-exp-image-generation, models/gemini-2.0-flash-lite-001, models/gemini-2.0-flash-lite, models/gemini-2.0-flash-preview-image-generation, models/gemini-2.0-flash-lite-preview-02-05, models/gemini-2.0-flash-lite-preview, models/gemini-2.0-pro-exp, models/gemini-2.0-pro-exp-02-05, models/gemini-exp-1206, models/gemini-2.0-flash-thinking-exp-01-21, models/gemini-2.0-flash-thinking-exp, models/gemini-2.0-flash-thinking-exp-1219, models/gemini-2.5-flash-preview-tts, models/gemini-2.5-pro-preview-tts, models/learnlm-2.0-flash-experimental, models/gemma-3-1b-it, models/gemma-3-4b-it, models/gemma-3-12b-it, models/gemma-3-27b-it, models/gemma-3n-e4b-it, models/gemma-3n-e2b-it, models/gemini-flash-latest, models/gemini-flash-lite-latest, models/gemini-pro-latest, models/gemini-2.5-flash-lite, models/gemini-2.5-flash-image-preview, models/gemini-2.5-flash-image, models/gemini-2.5-flash-preview-09-2025, models/gemini-2.5-flash-lite-preview-09-2025, models/gemini-robotics-er-1.5-preview, models/gemini-2.5-computer-use-preview-10-2025\n",
            "Attempting to load fallback model: models/gemini-2.5-pro-preview-03-25\n",
            "Successfully loaded fallback LLM model: models/gemini-2.5-pro-preview-03-25\n",
            "Imports and Google Generative AI client configured successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab77824c"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous steps successfully loaded an LLM model, completing the initial setup part of the subtask. Now, I need to define a function to read the text content from a PDF resume using the `PyPDF2` library that was previously imported and installed. This is a prerequisite for analyzing the resume against a job description, which is the core of the new workflow.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82f6aad1",
        "outputId": "1dadae65-6745-4c3e-90c9-6bd21b225b65"
      },
      "source": [
        "def extract_text_from_pdf(pdf_path: str) -> str:\n",
        "    \"\"\"Extracts text from a local PDF file.\"\"\"\n",
        "    try:\n",
        "        with open(pdf_path, 'rb') as file:\n",
        "            reader = PyPDF2.PdfReader(file)\n",
        "            text = ''\n",
        "            for page in reader.pages:\n",
        "                text += page.extract_text() + \"\\n\"\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading PDF: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "print(\"PDF text extraction function defined.\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PDF text extraction function defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1bf1ca4"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous step defined a function to extract text from a PDF. Now, to analyze a resume against a job description from a URL, I need to define a function that can scrape text content from a given web page. This function will utilize `requests` for fetching the page and `BeautifulSoup` for parsing the HTML and extracting the relevant text, aligning with the subtask's requirement for web scraping.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ae39e48c",
        "outputId": "16b299c2-4d58-45ea-d6e7-7ed19426e471"
      },
      "source": [
        "def extract_text_from_url(url: str) -> str:\n",
        "    \"\"\"Extracts text content from a given URL, typically for a job description.\"\"\"\n",
        "    try:\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "        }\n",
        "        response = requests.get(url, headers=headers, timeout=10)\n",
        "        response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Remove script and style elements\n",
        "        for script_or_style in soup(['script', 'style']):\n",
        "            script_or_style.extract()\n",
        "\n",
        "        # Get text and clean it\n",
        "        text = soup.get_text()\n",
        "        lines = (line.strip() for line in text.splitlines())\n",
        "        # Break multi-headlines into a line each\n",
        "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
        "        # Drop blank lines\n",
        "        text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
        "        return text\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching URL {url}: {e}\")\n",
        "        return \"\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing URL {url}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "print(\"Web scraping function for job descriptions defined.\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Web scraping function for job descriptions defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3e04eb2d"
      },
      "source": [
        "## Define placeholder for user inputs\n",
        "\n",
        "### Subtask:\n",
        "Create variables for `resume_path` and `job_url` to simulate user input for the new workflow.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91d63539"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to define two placeholder variables, `resume_path` and `job_url`, with dummy values and add comments as instructed to simulate user input for the new workflow.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "be920017",
        "outputId": "8c655a28-47f0-45ee-de7f-0442df581789"
      },
      "source": [
        "resume_path = \"dummy_resume.pdf\"  # Placeholder: Replace with the actual path to your PDF resume file.\n",
        "job_url = \"https://example.com/job_description\"  # Placeholder: Replace with the actual URL of the job description.\n",
        "\n",
        "print(f\"Dummy resume_path set to: {resume_path}\")\n",
        "print(f\"Dummy job_url set to: {job_url}\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dummy resume_path set to: dummy_resume.pdf\n",
            "Dummy job_url set to: https://example.com/job_description\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fd6e5f2"
      },
      "source": [
        "## Update Agent and Tool Definitions\n",
        "\n",
        "### Subtask:\n",
        "Modify the `Agent` and `Tool` classes if necessary for the new workflow. Remove the `job_search_tool`, `job_search_agent`, and `submit_application_tool` as they are no longer needed. Introduce a new tool for `candidate_agent` for resume and job description analysis.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80252e89"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to update the agent and tool definitions by removing the old job search and application submission tools and agents, then creating a new analysis tool and re-instantiating the candidate agent with this new tool.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ec20f91",
        "outputId": "443d75db-fccb-4cf4-ad4a-183f4b4bd47b"
      },
      "source": [
        "import asyncio\n",
        "import nest_asyncio\n",
        "import random\n",
        "import re # Import regex for salary parsing\n",
        "\n",
        "# Ensure nest_asyncio is applied if not already done in the session\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Assuming Tool, Agent are defined in previous cells and are available in scope.\n",
        "# Re-defining them here for clarity in this single block if notebook state is reset\n",
        "class Tool:\n",
        "    def __init__(self, func, name, description):\n",
        "        self.func = func\n",
        "        self.name = name\n",
        "        self.description = description\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, name, instruction, tools: list):\n",
        "        self.name = name\n",
        "        self.instruction = instruction\n",
        "        self.tools = tools\n",
        "\n",
        "# Helper function to parse salary from string\n",
        "def parse_salary(salary_str: str) -> int:\n",
        "    if not salary_str: return 0\n",
        "    # Remove non-numeric characters except comma, then remove comma, then convert to int\n",
        "    numeric_str = re.sub(r'[^\\\\d,]', '', salary_str)\n",
        "    numeric_str = numeric_str.replace(',', '')\n",
        "    try:\n",
        "        return int(numeric_str)\n",
        "    except ValueError:\n",
        "        return 0\n",
        "\n",
        "# Placeholder for the new analysis function\n",
        "def analyze_resume_job_description_placeholder(resume_text: str, job_description_text: str) -> dict:\n",
        "    \"\"\"Placeholder function for resume and job description analysis.\"\"\"\n",
        "    print(f\"Analyzing resume (length: {len(resume_text)}) and job description (length: {len(job_description_text)}).\")\n",
        "    return {\"analysis_status\": \"success\", \"message\": \"Analysis not yet implemented, but placeholder executed.\", \"skills_match\": \"N/A\", \"gaps\": \"N/A\"}\n",
        "\n",
        "# 1. No modifications to Tool and Agent class structures are needed at this stage.\n",
        "\n",
        "# 2. Remove instantiation of old tools and agents.\n",
        "# job_search_tool, submit_application_tool, job_search_agent, and the original candidate_agent are removed.\n",
        "\n",
        "# 3. Define a new Tool instance for analysis.\n",
        "analysis_tool = Tool(\n",
        "    func=analyze_resume_job_description_placeholder,\n",
        "    name=\"analyze_resume_job_description\",\n",
        "    description=\"Analyzes a candidate's resume against a job description to identify skills and gaps.\"\n",
        ")\n",
        "\n",
        "# 4. Re-instantiate the candidate_agent with the new analysis_tool.\n",
        "candidate_agent = Agent(\n",
        "    name=\"candidate_agent\",\n",
        "    instruction=\"I manage candidate profiles and analyze resumes against job descriptions.\",\n",
        "    tools=[analysis_tool]\n",
        ")\n",
        "\n",
        "# Redefine CoordinatorAgent to reflect new workflow\n",
        "class CoordinatorAgent(Agent):\n",
        "    def __init__(self, name: str, instruction: str, tools: list = None, sub_agents: list = None):\n",
        "        super().__init__(name, instruction, tools if tools is not None else [])\n",
        "        self.sub_agents = sub_agents if sub_agents is not None else []\n",
        "\n",
        "    async def run_live(self, resume_text: str, job_description_text: str):\n",
        "        yield f\"üöÄ CoordinatorAgent '{self.name}' initiating resume and job description analysis...\"\n",
        "\n",
        "        candidate_agent_found = next((agent for agent in self.sub_agents if agent.name == \"candidate_agent\"), None)\n",
        "        if not candidate_agent_found:\n",
        "            yield \"‚ùå Error: candidate_agent not found.\"\n",
        "            return\n",
        "\n",
        "        analysis_tool_instance = next((tool for tool in candidate_agent_found.tools if tool.name == \"analyze_resume_job_description\"), None)\n",
        "        if not analysis_tool_instance:\n",
        "            yield \"‚ùå Error: analyze_resume_job_description tool not found for candidate_agent.\"\n",
        "            return\n",
        "\n",
        "        yield f\"‚öôÔ∏è Delegating analysis to {candidate_agent_found.name} using {analysis_tool_instance.name} tool...\"\n",
        "        try:\n",
        "            analysis_result = analysis_tool_instance.func(resume_text, job_description_text)\n",
        "            yield f\"‚úÖ Analysis complete: {analysis_result.get('message', 'No message provided.')} Status: {analysis_result.get('analysis_status', 'unknown')}\"\n",
        "        except Exception as e:\n",
        "            yield f\"‚ùå Error during analysis: {e}\"\n",
        "            return\n",
        "\n",
        "# Re-instantiate the CoordinatorAgent with the new class definition and updated sub-agents\n",
        "root_agent = CoordinatorAgent(\n",
        "    name=\"root_agent\",\n",
        "    instruction=\"I orchestrate the resume and job description analysis process.\",\n",
        "    sub_agents=[candidate_agent]\n",
        ")\n",
        "\n",
        "print(\"Agent and Tool definitions updated for new workflow.\")\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Agent and Tool definitions updated for new workflow.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a493a18e"
      },
      "source": [
        "## Implement Resume Processing and Job Description Fetching\n",
        "\n",
        "### Subtask:\n",
        "Extract text from the dummy PDF resume and fetch text from the dummy job description URL using the previously defined helper functions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcd0728e"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code blocks defined helper functions for PDF and URL text extraction. Now, I need to implement the first instruction of the subtask, which is to create a dummy PDF file. Since direct file creation with content is not directly supported in the notebook environment without specific libraries, I will install `reportlab` to programmatically create a PDF with dummy content. This library allows generating PDF files from Python code.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "56232e44",
        "outputId": "1a5d4685-ae3d-4578-ee43-6c8add4196a7"
      },
      "source": [
        "import os\n",
        "!pip install reportlab\n",
        "\n",
        "print(\"reportlab installed successfully.\")\n",
        "\n",
        "# Create a dummy PDF file named dummy_resume.pdf\n",
        "# This is necessary because the previous `resume_path` pointed to a non-existent file.\n",
        "from reportlab.pdfgen import canvas\n",
        "from reportlab.lib.pagesizes import letter\n",
        "\n",
        "def create_dummy_pdf(filename=\"dummy_resume.pdf\", content=\"\"):\n",
        "    c = canvas.Canvas(filename, pagesize=letter)\n",
        "    textobject = c.beginText()\n",
        "    textobject.setTextOrigin(10, 750) # Set starting position\n",
        "    textobject.setFont(\"Helvetica\", 12) # Set font and size\n",
        "\n",
        "    # Split content by lines to fit on PDF\n",
        "    lines = content.split('\\n')\n",
        "    for line in lines:\n",
        "        textobject.textLine(line)\n",
        "\n",
        "    c.drawText(textobject)\n",
        "    c.save()\n",
        "    print(f\"Dummy PDF '{filename}' created successfully.\")\n",
        "\n",
        "dummy_resume_content = (\n",
        "    \"John Doe\\n\" \\\n",
        "    \"john.doe@example.com | (123) 456-7890 | LinkedIn: linkedin.com/in/johndoe\\n\\n\" \\\n",
        "    \"Summary\\n\" \\\n",
        "    \"Highly skilled and results-driven Software Engineer with 10 years of experience \" \\\n",
        "    \"in developing robust and scalable web applications. Proficient in Python, Java, \" \\\n",
        "    \"and JavaScript, with extensive experience in cloud platforms (AWS, Azure) and \" \\\n",
        "    \"database management (SQL, NoSQL). Seeking to leverage strong technical expertise \" \\\n",
        "    \"and problem-solving skills to contribute to innovative projects.\\n\\n\" \\\n",
        "    \"Experience\\n\" \\\n",
        "    \"Senior Software Engineer | Tech Solutions Inc. | 2018 - Present\\n\" \\\n",
        "    \"- Led the design and development of a microservices-based e-commerce platform, \" \\\n",
        "    \"  resulting in a 30% improvement in system performance.\\n\" \\\n",
        "    \"- Implemented CI/CD pipelines using Jenkins and Docker, reducing deployment time by 50%.\\n\" \\\n",
        "    \"- Mentored junior engineers and conducted code reviews to ensure high code quality.\\n\\n\" \\\n",
        "    \"Software Engineer | Web Innovators LLC | 2014 - 2018\\n\" \\\n",
        "    \"- Developed and maintained full-stack web applications using Python/Django and React.\\n\" \\\n",
        "    \"- Collaborated with product managers and UX designers to translate requirements into technical specifications.\\n\" \\\n",
        "    \"- Optimized database queries, leading to a 20% reduction in data retrieval times.\\n\\n\" \\\n",
        "    \"Education\\n\" \\\n",
        "    \"M.S. Computer Science | University of Example | 2014\\n\" \\\n",
        "    \"B.S. Computer Science | State University | 2012\\n\\n\" \\\n",
        "    \"Skills\\n\" \\\n",
        "    \"Programming Languages: Python, Java, JavaScript, C++\\n\" \\\n",
        "    \"Web Frameworks: Django, Flask, React, Angular\\n\" \\\n",
        "    \"Cloud Platforms: AWS, Azure, Google Cloud Platform (GCP)\\n\" \\\n",
        "    \"Databases: PostgreSQL, MongoDB, MySQL\\n\" \\\n",
        "    \"Tools & Technologies: Docker, Kubernetes, Jenkins, Git, JIRA\\n\" \\\n",
        "    \"Methodologies: Agile, Scrum\n",
        "\""
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "unterminated string literal (detected at line 54) (ipython-input-451197547.py, line 54)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-451197547.py\"\u001b[0;36m, line \u001b[0;32m54\u001b[0m\n\u001b[0;31m    \"Methodologies: Agile, Scrum\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 54)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6555c47d"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code block failed due to an unterminated string literal in `dummy_resume_content`. I will fix the syntax error by closing the string literal and ensure the `create_dummy_pdf` function is called to generate the dummy PDF.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ab95ccb",
        "outputId": "6163e11f-d7be-4acd-85ce-f1e56d32179f"
      },
      "source": [
        "import os\n",
        "!pip install reportlab\n",
        "\n",
        "print(\"reportlab installed successfully.\")\n",
        "\n",
        "# Create a dummy PDF file named dummy_resume.pdf\n",
        "# This is necessary because the previous `resume_path` pointed to a non-existent file.\n",
        "from reportlab.pdfgen import canvas\n",
        "from reportlab.lib.pagesizes import letter\n",
        "\n",
        "def create_dummy_pdf(filename=\"dummy_resume.pdf\", content=\"\"):\n",
        "    c = canvas.Canvas(filename, pagesize=letter)\n",
        "    textobject = c.beginText()\n",
        "    textobject.setTextOrigin(10, 750) # Set starting position\n",
        "    textobject.setFont(\"Helvetica\", 12) # Set font and size\n",
        "\n",
        "    # Split content by lines to fit on PDF\n",
        "    lines = content.split('\\n')\n",
        "    for line in lines:\n",
        "        textobject.textLine(line)\n",
        "\n",
        "    c.drawText(textobject)\n",
        "    c.save()\n",
        "    print(f\"Dummy PDF '{filename}' created successfully.\")\n",
        "\n",
        "dummy_resume_content = (\n",
        "    \"John Doe\\n\" \\\n",
        "    \"john.doe@example.com | (123) 456-7890 | LinkedIn: linkedin.com/in/johndoe\\n\\n\" \\\n",
        "    \"Summary\\n\" \\\n",
        "    \"Highly skilled and results-driven Software Engineer with 10 years of experience \" \\\n",
        "    \"in developing robust and scalable web applications. Proficient in Python, Java, \" \\\n",
        "    \"and JavaScript, with extensive experience in cloud platforms (AWS, Azure) and \" \\\n",
        "    \"database management (SQL, NoSQL). Seeking to leverage strong technical expertise \" \\\n",
        "    \"and problem-solving skills to contribute to innovative projects.\\n\\n\" \\\n",
        "    \"Experience\\n\" \\\n",
        "    \"Senior Software Engineer | Tech Solutions Inc. | 2018 - Present\\n\" \\\n",
        "    \"- Led the design and development of a microservices-based e-commerce platform, \" \\\n",
        "    \"  resulting in a 30% improvement in system performance.\\n\" \\\n",
        "    \"- Implemented CI/CD pipelines using Jenkins and Docker, reducing deployment time by 50%.\\n\" \\\n",
        "    \"- Mentored junior engineers and conducted code reviews to ensure high code quality.\\n\\n\" \\\n",
        "    \"Software Engineer | Web Innovators LLC | 2014 - 2018\\n\" \\\n",
        "    \"- Developed and maintained full-stack web applications using Python/Django and React.\\n\" \\\n",
        "    \"- Collaborated with product managers and UX designers to translate requirements into technical specifications.\\n\" \\\n",
        "    \"- Optimized database queries, leading to a 20% reduction in data retrieval times.\\n\\n\" \\\n",
        "    \"Education\\n\" \\\n",
        "    \"M.S. Computer Science | University of Example | 2014\\n\" \\\n",
        "    \"B.S. Computer Science | State University | 2012\\n\\n\" \\\n",
        "    \"Skills\\n\" \\\n",
        "    \"Programming Languages: Python, Java, JavaScript, C++\\n\" \\\n",
        "    \"Web Frameworks: Django, Flask, React, Angular\\n\" \\\n",
        "    \"Cloud Platforms: AWS, Azure, Google Cloud Platform (GCP)\\n\" \\\n",
        "    \"Databases: PostgreSQL, MongoDB, MySQL\\n\" \\\n",
        "    \"Tools & Technologies: Docker, Kubernetes, Jenkins, Git, JIRA\\n\" \\\n",
        "    \"Methodologies: Agile, Scrum\"\n",
        ")\n",
        "\n",
        "create_dummy_pdf(filename=\"dummy_resume.pdf\", content=dummy_resume_content)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting reportlab\n",
            "  Downloading reportlab-4.4.4-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.12/dist-packages (from reportlab) (11.3.0)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from reportlab) (3.4.4)\n",
            "Downloading reportlab-4.4.4-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: reportlab\n",
            "Successfully installed reportlab-4.4.4\n",
            "reportlab installed successfully.\n",
            "Dummy PDF 'dummy_resume.pdf' created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53445e3f"
      },
      "source": [
        "**Reasoning**:\n",
        "The dummy PDF has been created and `reportlab` is installed. Now I will use the previously defined `extract_text_from_pdf` function to read the content of the `dummy_resume.pdf` file into `resume_text`. I will then define a dummy job description HTML content and save it to a temporary file named `dummy_job_description.html` to simulate a web page, since the provided `job_url` is a placeholder. After creating the dummy HTML file, I'll call the `extract_text_from_url` function to process this local file. Finally, I will print the first 500 characters of both `resume_text` and `job_description_text` to verify the extraction, which fulfills the remaining instructions of the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98d22bd9",
        "outputId": "d7cabfd3-47f4-46c3-d234-95bfb67fbefc"
      },
      "source": [
        "import os\n",
        "\n",
        "# Create a dummy job description HTML file since the job_url is a placeholder.\n",
        "dummy_job_description_html_content = (\n",
        "    \"<html><body><h1>Senior Software Engineer</h1>\"\n",
        "    \"<p>We are looking for a highly motivated Senior Software Engineer to join our team. \"\n",
        "    \"The ideal candidate will have extensive experience in <b>Python, AWS, and Kubernetes</b>. \"\n",
        "    \"Responsibilities include designing and implementing scalable software solutions, \"\n",
        "    \"mentoring junior developers, and contributing to architectural decisions. \"\n",
        "    \"Familiarity with Agile methodologies and CI/CD pipelines is a plus.</p>\"\n",
        "    \"<h2>Requirements</h2>\"\n",
        "    \"<ul>\"\n",
        "    \"<li>10+ years of experience in software development.</li>\"\n",
        "    \"<li>Expertise in Python, Java, or Go.</li>\"\n",
        "    \"<li>Strong knowledge of cloud platforms (AWS, Azure, GCP).</li>\"\n",
        "    \"<li>Experience with containerization (Docker, Kubernetes).</li>\"\n",
        "    \"<li>Excellent problem-solving and communication skills.</li>\"\n",
        "    \"</ul>\"\n",
        "    \"</body></html>\"\n",
        ")\n",
        "\n",
        "dummy_job_html_file = \"dummy_job_description.html\"\n",
        "with open(dummy_job_html_file, \"w\") as f:\n",
        "    f.write(dummy_job_description_html_content)\n",
        "print(f\"Dummy job description HTML file '{dummy_job_html_file}' created.\")\n",
        "\n",
        "# Update job_url to point to the local dummy HTML file\n",
        "# This is a workaround since direct web scraping of a placeholder URL is not feasible.\n",
        "job_url_local = f\"file://{os.path.abspath(dummy_job_html_file)}\"\n",
        "\n",
        "\n",
        "# Extract text from the dummy PDF resume\n",
        "resume_text = extract_text_from_pdf(resume_path)\n",
        "print(f\"Extracted text from resume (first 500 chars):\\n{resume_text[:500]}...\")\n",
        "\n",
        "# Extract text from the dummy job description (using the local HTML file workaround)\n",
        "# The extract_text_from_url function is designed for http/https, but we can make a local path work by reading the file directly\n",
        "# For demonstration, we will read the local dummy HTML file directly\n",
        "with open(dummy_job_html_file, 'r') as f:\n",
        "    dummy_html_content = f.read()\n",
        "\n",
        "# Use BeautifulSoup to parse the local HTML content\n",
        "soup = BeautifulSoup(dummy_html_content, 'html.parser')\n",
        "for script_or_style in soup(['script', 'style']):\n",
        "    script_or_style.extract()\n",
        "text = soup.get_text()\n",
        "lines = (line.strip() for line in text.splitlines())\n",
        "chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
        "job_description_text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
        "\n",
        "print(f\"Extracted text from job description (first 500 chars):\\n{job_description_text[:500]}...\")\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dummy job description HTML file 'dummy_job_description.html' created.\n",
            "Extracted text from resume (first 500 chars):\n",
            "John Doe\n",
            "john.doe@example.com | (123) 456-7890 | LinkedIn: linkedin.com/in/johndoe\n",
            "Summary\n",
            "Highly skilled and results-driven Software Engineer with 10 years of experience in developing robust and scalable web applications. Proficient in Python, Java, and JavaScript, with extensive experience in cloud platforms (AWS, Azure) and database management (SQL, NoSQL). Seeking to leverage strong technical expertise and problem-solving skills to contribute to innovative projects.\n",
            "Experience\n",
            "Senior Softwar...\n",
            "Extracted text from job description (first 500 chars):\n",
            "Senior Software EngineerWe are looking for a highly motivated Senior Software Engineer to join our team. The ideal candidate will have extensive experience in Python, AWS, and Kubernetes. Responsibilities include designing and implementing scalable software solutions, mentoring junior developers, and contributing to architectural decisions. Familiarity with Agile methodologies and CI/CD pipelines is a plus.Requirements10+ years of experience in software development.Expertise in Python, Java, or ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "553beb35"
      },
      "source": [
        "## Develop `analyze_skills_and_gaps` function\n",
        "\n",
        "### Subtask:\n",
        "Create a new function that takes extracted resume text and job description text as input, uses the Gemini 2.5 Flash model to identify skills from both sources, compare them, and generate a skills gap analysis report. It should clearly outline missing skills.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f70df3a5"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to define the `analyze_skills_and_gaps` function as instructed, which will construct a prompt for the LLM, call the `llm_model.generate_content()` method, and return the generated analysis.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "981990ef",
        "outputId": "bd6bb001-0959-457f-dd45-470d52934697"
      },
      "source": [
        "def analyze_skills_and_gaps(resume_text: str, job_description_text: str) -> str:\n",
        "    \"\"\"Analyzes a candidate's resume against a job description using the LLM to identify skills and gaps.\"\"\"\n",
        "    prompt = f\"\"\"You are an expert HR analyst. Your task is to compare a candidate's resume with a job description.\n",
        "\n",
        "Here is the candidate's Resume:\n",
        "---\\n{resume_text}\\n---\n",
        "\n",
        "Here is the Job Description:\n",
        "---\\n{job_description_text}\\n---\n",
        "\n",
        "Please perform the following steps and provide your output in a structured, easy-to-read format:\n",
        "\n",
        "1.  **Extract Candidate Skills**: List all key technical and soft skills explicitly mentioned in the resume.\n",
        "2.  **Extract Required Job Skills**: List all essential technical and soft skills mentioned in the job description.\n",
        "3.  **Skills Comparison**: Compare the two lists and identify:\n",
        "    *   **Matched Skills**: Skills present in both the resume and the job description.\n",
        "    *   **Missing Skills (Gaps)**: Skills required by the job description but NOT found in the resume.\n",
        "    *   **Additional Skills**: Skills present in the resume but not explicitly required by the job description.\n",
        "4.  **Overall Fit Summary**: Provide a brief summary of how well the candidate's skills align with the job requirements, highlighting their strengths and areas for development based on the identified gaps.\n",
        "\n",
        "Ensure your output is clear, concise, and structured, using bullet points for lists where appropriate.\n",
        "\"\"\"\n",
        "\n",
        "    try:\n",
        "        # Assuming llm_model is already loaded from previous steps\n",
        "        if llm_model is None:\n",
        "            return \"Error: LLM model not initialized.\"\n",
        "\n",
        "        response = llm_model.generate_content(prompt)\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        return f\"Error during LLM analysis: {e}\"\n",
        "\n",
        "print(\"Function `analyze_skills_and_gaps` defined.\")\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function `analyze_skills_and_gaps` defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dca9857"
      },
      "source": [
        "**Reasoning**:\n",
        "The `analyze_skills_and_gaps` function has been defined. Now, I need to replace the placeholder implementation of `analyze_resume_job_description_placeholder` with a call to the newly defined `analyze_skills_and_gaps` function. This will integrate the LLM-based analysis into the `candidate_agent`'s tool, allowing the `root_agent` to orchestrate the full workflow.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ea486296",
        "outputId": "c3cac974-7109-43ab-81c0-c4082fc45c31"
      },
      "source": [
        "import asyncio\n",
        "import nest_asyncio\n",
        "import random\n",
        "import re # Import regex for salary parsing\n",
        "\n",
        "# Ensure nest_asyncio is applied if not already done in the session\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Assuming Tool, Agent are defined in previous cells and are available in scope.\n",
        "# Re-defining them here for clarity in this single block if notebook state is reset\n",
        "class Tool:\n",
        "    def __init__(self, func, name, description):\n",
        "        self.func = func\n",
        "        self.name = name\n",
        "        self.description = description\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, name, instruction, tools: list):\n",
        "        self.name = name\n",
        "        self.instruction = instruction\n",
        "        self.tools = tools\n",
        "\n",
        "# Helper function to parse salary from string\n",
        "def parse_salary(salary_str: str) -> int:\n",
        "    if not salary_str: return 0\n",
        "    # Remove non-numeric characters except comma, then remove comma, then convert to int\n",
        "    numeric_str = re.sub(r'[^\\\\d,]', '', salary_str)\n",
        "    numeric_str = numeric_str.replace(',', '')\n",
        "    try:\n",
        "        return int(numeric_str)\n",
        "    except ValueError:\n",
        "        return 0\n",
        "\n",
        "# NOTE: The analyze_skills_and_gaps function from the previous step is now fully implemented.\n",
        "# It will be called by the analysis tool below.\n",
        "\n",
        "def analyze_skills_and_gaps(resume_text: str, job_description_text: str) -> str:\n",
        "    \"\"\"Analyzes a candidate's resume against a job description using the LLM to identify skills and gaps.\"\"\"\n",
        "    prompt = f\"\"\"You are an expert HR analyst. Your task is to compare a candidate's resume with a job description.\n",
        "\n",
        "Here is the candidate's Resume:\n",
        "---\n",
        "{resume_text}\\n---\n",
        "\n",
        "Here is the Job Description:\n",
        "---\n",
        "{job_description_text}\\n---\n",
        "\n",
        "Please perform the following steps and provide your output in a structured, easy-to-read format:\n",
        "\n",
        "1.  **Extract Candidate Skills**: List all key technical and soft skills explicitly mentioned in the resume.\n",
        "2.  **Extract Required Job Skills**: List all essential technical and soft skills mentioned in the job description.\n",
        "3.  **Skills Comparison**: Compare the two lists and identify:\n",
        "    *   **Matched Skills**: Skills present in both the resume and the job description.\n",
        "    *   **Missing Skills (Gaps)**: Skills required by the job description but NOT found in the resume.\n",
        "    *   **Additional Skills**: Skills present in the resume but not explicitly required by the job description.\n",
        "4.  **Overall Fit Summary**: Provide a brief summary of how well the candidate's skills align with the job requirements, highlighting their strengths and areas for development based on the identified gaps.\n",
        "\n",
        "Ensure your output is clear, concise, and structured, using bullet points for lists where appropriate.\n",
        "\"\"\"\n",
        "\n",
        "    try:\n",
        "        # Assuming llm_model is already loaded from previous steps\n",
        "        global llm_model # Declare llm_model as global to access it\n",
        "        if llm_model is None:\n",
        "            return \"Error: LLM model not initialized.\"\n",
        "\n",
        "        response = llm_model.generate_content(prompt)\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        return f\"Error during LLM analysis: {e}\"\n",
        "\n",
        "# Update the analysis function to call the LLM-based logic\n",
        "def analyze_resume_job_description_full(resume_text: str, job_description_text: str) -> dict:\n",
        "    \"\"\"Performs a full resume and job description analysis using the LLM.\n",
        "    This function replaces the placeholder and calls analyze_skills_and_gaps.\n",
        "    \"\"\"\n",
        "    print(f\"Initiating LLM-based analysis for resume (length: {len(resume_text)}) and job description (length: {len(job_description_text)}).\")\n",
        "    analysis_report = analyze_skills_and_gaps(resume_text, job_description_text)\n",
        "\n",
        "    if \"Error during LLM analysis\" in analysis_report:\n",
        "        return {\"analysis_status\": \"failure\", \"message\": analysis_report}\n",
        "    else:\n",
        "        return {\"analysis_status\": \"success\", \"message\": \"LLM-based analysis completed.\", \"report\": analysis_report}\n",
        "\n",
        "# 1. No modifications to Tool and Agent class structures are needed at this stage.\n",
        "\n",
        "# 2. Remove instantiation of old tools and agents.\n",
        "# job_search_tool, submit_application_tool, job_search_agent, and the original candidate_agent are removed.\n",
        "\n",
        "# 3. Define a new Tool instance for analysis.\n",
        "analysis_tool = Tool(\n",
        "    func=analyze_resume_job_description_full, # Use the full analysis function\n",
        "    name=\"analyze_resume_job_description\",\n",
        "    description=\"Analyzes a candidate's resume against a job description to identify skills and gaps using an LLM.\"\n",
        ")\n",
        "\n",
        "# 4. Re-instantiate the candidate_agent with the new analysis_tool.\n",
        "candidate_agent = Agent(\n",
        "    name=\"candidate_agent\",\n",
        "    instruction=\"I manage candidate profiles and analyze resumes against job descriptions.\",\n",
        "    tools=[analysis_tool]\n",
        ")\n",
        "\n",
        "# Redefine CoordinatorAgent to reflect new workflow\n",
        "class CoordinatorAgent(Agent):\n",
        "    def __init__(self, name: str, instruction: str, tools: list = None, sub_agents: list = None):\n",
        "        super().__init__(name, instruction, tools if tools is not None else [])\n",
        "        self.sub_agents = sub_agents if sub_agents is not None else []\n",
        "\n",
        "    async def run_live(self, resume_text: str, job_description_text: str):\n",
        "        yield f\"üöÄ CoordinatorAgent '{self.name}' initiating resume and job description analysis...\"\n",
        "\n",
        "        candidate_agent_found = next((agent for agent in self.sub_agents if agent.name == \"candidate_agent\"), None)\n",
        "        if not candidate_agent_found:\n",
        "            yield \"‚ùå Error: candidate_agent not found.\"\n",
        "            return\n",
        "\n",
        "        analysis_tool_instance = next((tool for tool in candidate_agent_found.tools if tool.name == \"analyze_resume_job_description\"), None)\n",
        "        if not analysis_tool_instance:\n",
        "            yield \"‚ùå Error: analyze_resume_job_description tool not found for candidate_agent.\"\n",
        "            return\n",
        "\n",
        "        yield f\"‚öôÔ∏è Delegating analysis to {candidate_agent_found.name} using {analysis_tool_instance.name} tool...\"\n",
        "        try:\n",
        "            analysis_result = analysis_tool_instance.func(resume_text, job_description_text)\n",
        "            if analysis_result.get('analysis_status') == 'success':\n",
        "                yield f\"‚úÖ Analysis complete: {analysis_result.get('message', 'No message provided.')}\"\n",
        "                yield \"--- LLM Analysis Report ---\"\n",
        "                yield analysis_result.get('report', 'No report content available.')\n",
        "                yield \"---------------------------\"\n",
        "            else:\n",
        "                yield f\"‚ùå Analysis failed: {analysis_result.get('message', 'Unknown error.')}\"\n",
        "        except Exception as e:\n",
        "            yield f\"‚ùå Error during analysis: {e}\"\n",
        "            return\n",
        "\n",
        "# Re-instantiate the CoordinatorAgent with the new class definition and updated sub-agents\n",
        "root_agent = CoordinatorAgent(\n",
        "    name=\"root_agent\",\n",
        "    instruction=\"I orchestrate the resume and job description analysis process.\",\n",
        "    sub_agents=[candidate_agent]\n",
        ")\n",
        "\n",
        "print(\"Agent and Tool definitions updated for new workflow and analysis logic integrated.\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Agent and Tool definitions updated for new workflow and analysis logic integrated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f04cd089"
      },
      "source": [
        "## Run the new workflow\n",
        "\n",
        "### Subtask:\n",
        "Call the updated `CoordinatorAgent.run_live` method with the sample `resume_path` and `job_url` to demonstrate the new workflow and display the output.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3cfb477"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to define an asynchronous function to run the `CoordinatorAgent.run_live` method, iterate through its yielded outputs, and then execute this function using `asyncio.run` with the extracted resume and job description texts to demonstrate the new workflow.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95dd04fa",
        "outputId": "e31faf0a-a8df-4a57-dd73-609fa3ad660a"
      },
      "source": [
        "async def run_analysis_workflow(resume_text: str, job_description_text: str):\n",
        "    \"\"\"Runs the resume and job description analysis workflow.\"\"\"\n",
        "    print(\"\\nStarting skills gap analysis workflow...\")\n",
        "    async for step_output in root_agent.run_live(resume_text, job_description_text):\n",
        "        print(step_output)\n",
        "    print(\"Skills gap analysis workflow completed.\")\n",
        "\n",
        "# Run the workflow with the extracted texts\n",
        "if __name__ == '__main__':\n",
        "    # Ensure an event loop is running. nest_asyncio.apply() usually handles this for Colab.\n",
        "    asyncio.run(run_analysis_workflow(resume_text, job_description_text))\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting skills gap analysis workflow...\n",
            "üöÄ CoordinatorAgent 'root_agent' initiating resume and job description analysis...\n",
            "‚öôÔ∏è Delegating analysis to candidate_agent using analyze_resume_job_description tool...\n",
            "Initiating LLM-based analysis for resume (length: 1592) and job description (length: 662).\n",
            "‚ùå Analysis failed: Error during LLM analysis: 'Model' object has no attribute 'generate_content'\n",
            "Skills gap analysis workflow completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "580b3cd8"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous execution failed because the `llm_model` object did not have a `generate_content` method. This indicates that `gen.get_model()` was used instead of `gen.GenerativeModel()` to instantiate the LLM. I need to re-initialize the LLM model using `gen.GenerativeModel()` so that it can correctly generate content. This change will be applied to the relevant code block where `llm_model` was originally set up.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fd79ffc",
        "outputId": "b2b1c1ff-ae9e-469d-87e6-dc0698bde944"
      },
      "source": [
        "import google.generativeai as gen\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import PyPDF2\n",
        "import os\n",
        "\n",
        "# Configure the Google Generative AI client\n",
        "# Make sure to set your GOOGLE_API_KEY in your environment variables or provide it when prompted.\n",
        "GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')\n",
        "\n",
        "if not GOOGLE_API_KEY or GOOGLE_API_KEY == \"YOUR_GOOGLE_API_KEY_HERE\":\n",
        "    print(\"GOOGLE_API_KEY environment variable not set or is a placeholder.\")\n",
        "    print(\"Please provide your Google API Key to continue.\")\n",
        "    GOOGLE_API_KEY = input(\"Enter your Google API Key: \").strip()\n",
        "\n",
        "    # Optionally, set it as an environment variable for subsequent runs in this session\n",
        "    if GOOGLE_API_KEY:\n",
        "        os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
        "    else:\n",
        "        raise ValueError(\"Google API Key is required but was not provided.\")\n",
        "\n",
        "gen.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "llm_model = None\n",
        "tried_model_names = ['gemini-1.5-flash', 'gemini-pro', 'gemini-1.0-pro']\n",
        "\n",
        "for model_name in tried_model_names:\n",
        "    try:\n",
        "        # Correctly instantiate GenerativeModel\n",
        "        llm_model = gen.GenerativeModel(model_name)\n",
        "        print(f\"Successfully loaded LLM model: {llm_model.model_name}\")\n",
        "        break # Exit loop if a model is successfully loaded\n",
        "    except Exception as e:\n",
        "        print(f\"WARNING: Could not load model '{model_name}': {e}\")\n",
        "\n",
        "if not llm_model:\n",
        "    print(\"Attempting to list available models to find an alternative...\")\n",
        "    try:\n",
        "        # Filter for models that support text generation\n",
        "        available_generative_models = [m.name for m in gen.list_models() if \"generateContent\" in m.supported_generation_methods]\n",
        "        if available_generative_models:\n",
        "            print(f\"Available generative models: {', '.join(available_generative_models)}\")\n",
        "            # Try to pick the first available generative model as a fallback\n",
        "            fallback_model_name = available_generative_models[0]\n",
        "            print(f\"Attempting to load fallback model: {fallback_model_name}\")\n",
        "            # Correctly instantiate GenerativeModel for fallback\n",
        "            llm_model = gen.GenerativeModel(fallback_model_name)\n",
        "            print(f\"Successfully loaded fallback LLM model: {llm_model.model_name}\")\n",
        "        else:\n",
        "            raise ValueError(\"No generative models found with the provided API key that support 'generateContent'.\")\n",
        "    except Exception as list_e:\n",
        "        raise ValueError(f\"Error listing models or no fallback model found: {list_e}. Please check your API key and region settings for Google Generative AI access.\")\n",
        "\n",
        "if llm_model:\n",
        "    print(\"Imports and Google Generative AI client configured successfully.\")\n",
        "else:\n",
        "    raise ValueError(\"Failed to load any LLM model after multiple attempts. Please ensure your GOOGLE_API_KEY is valid and has access to Google Generative AI models in your region.\")\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded LLM model: models/gemini-1.5-flash\n",
            "Imports and Google Generative AI client configured successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "012b5b18"
      },
      "source": [
        "**Reasoning**:\n",
        "The LLM model has been successfully re-initialized with `gen.GenerativeModel()`. Now, I need to re-run the `run_analysis_workflow` to ensure the `analyze_skills_and_gaps` function correctly calls the LLM's `generate_content` method and produces the expected analysis report.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "f06ad225",
        "outputId": "25e37655-7011-48e9-9cd2-ea0e96e0524a"
      },
      "source": [
        "async def run_analysis_workflow(resume_text: str, job_description_text: str):\n",
        "    \"\"\"Runs the resume and job description analysis workflow.\"\"\"\n",
        "    print(\"\\nStarting skills gap analysis workflow...\")\n",
        "    async for step_output in root_agent.run_live(resume_text, job_description_text):\n",
        "        print(step_output)\n",
        "    print(\"Skills gap analysis workflow completed.\")\n",
        "\n",
        "# Run the workflow with the extracted texts\n",
        "if __name__ == '__main__':\n",
        "    # Ensure an event loop is running. nest_asyncio.apply() usually handles this for Colab.\n",
        "    asyncio.run(run_analysis_workflow(resume_text, job_description_text))\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting skills gap analysis workflow...\n",
            "üöÄ CoordinatorAgent 'root_agent' initiating resume and job description analysis...\n",
            "‚öôÔ∏è Delegating analysis to candidate_agent using analyze_resume_job_description tool...\n",
            "Initiating LLM-based analysis for resume (length: 1592) and job description (length: 662).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 339.87ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ùå Analysis failed: Error during LLM analysis: 404 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: models/gemini-1.5-flash is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
            "Skills gap analysis workflow completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8303d94d"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt to run the analysis workflow failed because the model 'gemini-1.5-flash' was not found or not supported for `generateContent` despite being 'successfully loaded' in an earlier step. This indicates that the initial model loading logic was not robust enough to ensure the chosen model is truly functional for content generation. To fix this, I will update the LLM initialization to explicitly select 'models/gemini-2.5-flash' (as identified in the `available_generative_models` from the kernel state and requested in the original task as 'Gemini 2.5 Flash'). If 'models/gemini-2.5-flash' is not available for content generation, it will fall back to the first model returned by `gen.list_models()` that supports `generateContent`, ensuring a functional model is always loaded.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "8b00ce85",
        "outputId": "6aae6cde-b4da-4a2a-9e36-8a565cefed31"
      },
      "source": [
        "import google.generativeai as gen\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import PyPDF2\n",
        "import os\n",
        "\n",
        "# Configure the Google Generative AI client\n",
        "# Make sure to set your GOOGLE_API_KEY in your environment variables or provide it when prompted.\n",
        "GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')\n",
        "\n",
        "if not GOOGLE_API_KEY or GOOGLE_API_KEY == \"YOUR_GOOGLE_API_KEY_HERE\":\n",
        "    print(\"GOOGLE_API_KEY environment variable not set or is a placeholder.\")\n",
        "    print(\"Please provide your Google API Key to continue.\")\n",
        "    GOOGLE_API_KEY = input(\"Enter your Google API Key: \").strip()\n",
        "\n",
        "    # Optionally, set it as an environment variable for subsequent runs in this session\n",
        "    if GOOGLE_API_KEY:\n",
        "        os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
        "    else:\n",
        "        raise ValueError(\"Google API Key is required but was not provided.\")\n",
        "\n",
        "gen.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "llm_model = None\n",
        "\n",
        "print(\"Attempting to find and load a suitable generative model...\")\n",
        "try:\n",
        "    # Get all models that support text generation\n",
        "    available_generative_models = [m.name for m in gen.list_models() if \"generateContent\" in m.supported_generation_methods]\n",
        "    if not available_generative_models:\n",
        "        raise ValueError(\"No generative models found with the provided API key that support 'generateContent'.\")\n",
        "\n",
        "    chosen_model_name = None\n",
        "    # Prioritize 'gemini-2.5-flash' as per the subtask instruction if available\n",
        "    if 'models/gemini-2.5-flash' in available_generative_models:\n",
        "        chosen_model_name = 'models/gemini-2.5-flash'\n",
        "    # Fallback to 'gemini-1.5-flash' if 'gemini-2.5-flash' is not available (though it failed previously, it's worth re-checking availability)\n",
        "    elif 'models/gemini-1.5-flash' in available_generative_models:\n",
        "        chosen_model_name = 'models/gemini-1.5-flash'\n",
        "    else:\n",
        "        # Fallback to the first available generative model if neither specific flash model is found\n",
        "        chosen_model_name = available_generative_models[0]\n",
        "\n",
        "    print(f\"Loading LLM model: {chosen_model_name}\")\n",
        "    llm_model = gen.GenerativeModel(chosen_model_name)\n",
        "    print(f\"Successfully loaded LLM model: {llm_model.model_name}\")\n",
        "\n",
        "except Exception as e:\n",
        "    raise ValueError(f\"Error during LLM model loading: {e}. Please check your API key and region settings for Google Generative AI access.\")\n",
        "\n",
        "if llm_model:\n",
        "    print(\"Imports and Google Generative AI client configured successfully.\")\n",
        "else:\n",
        "    raise ValueError(\"Failed to load any LLM model. This should not happen if the previous block passed.\")\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to find and load a suitable generative model...\n",
            "Loading LLM model: models/gemini-2.5-flash\n",
            "Successfully loaded LLM model: models/gemini-2.5-flash\n",
            "Imports and Google Generative AI client configured successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaaedafc"
      },
      "source": [
        "**Reasoning**:\n",
        "The LLM model has now been successfully loaded with `models/gemini-2.5-flash`, which is a functional generative model. I need to re-run the `run_analysis_workflow` to verify that the `analyze_skills_and_gaps` function correctly calls the LLM's `generate_content` method and produces the expected analysis report, now that the model issue is resolved.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "556bdf7b",
        "outputId": "2e8b8bc3-49cb-40d3-d2d1-4c20ebb8a8fc"
      },
      "source": [
        "async def run_analysis_workflow(resume_text: str, job_description_text: str):\n",
        "    \"\"\"Runs the resume and job description analysis workflow.\"\"\"\n",
        "    print(\"\\nStarting skills gap analysis workflow...\")\n",
        "    async for step_output in root_agent.run_live(resume_text, job_description_text):\n",
        "        print(step_output)\n",
        "    print(\"Skills gap analysis workflow completed.\")\n",
        "\n",
        "# Run the workflow with the extracted texts\n",
        "if __name__ == '__main__':\n",
        "    # Ensure an event loop is running. nest_asyncio.apply() usually handles this for Colab.\n",
        "    asyncio.run(run_analysis_workflow(resume_text, job_description_text))\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting skills gap analysis workflow...\n",
            "üöÄ CoordinatorAgent 'root_agent' initiating resume and job description analysis...\n",
            "‚öôÔ∏è Delegating analysis to candidate_agent using analyze_resume_job_description tool...\n",
            "Initiating LLM-based analysis for resume (length: 1592) and job description (length: 662).\n",
            "‚úÖ Analysis complete: LLM-based analysis completed.\n",
            "--- LLM Analysis Report ---\n",
            "Here's an HR analyst's comparison of John Doe's resume against the Senior Software Engineer job description:\n",
            "\n",
            "---\n",
            "\n",
            "### 1. Extract Candidate Skills\n",
            "\n",
            "**Technical Skills:**\n",
            "*   **Programming Languages:** Python, Java, JavaScript, C++\n",
            "*   **Web Frameworks:** Django, Flask, React, Angular\n",
            "*   **Cloud Platforms:** AWS, Azure, Google Cloud Platform (GCP)\n",
            "*   **Databases:** PostgreSQL, MongoDB, MySQL, SQL, NoSQL\n",
            "*   **Tools & Technologies:** Docker, Kubernetes, Jenkins, Git, JIRA\n",
            "*   **Architectural/System Design:** Microservices, Full-stack web applications, CI/CD pipelines, Database optimization, Scalable web application development\n",
            "\n",
            "**Methodologies:**\n",
            "*   Agile, Scrum\n",
            "\n",
            "**Experience & Soft Skills:**\n",
            "*   10 years of software development experience\n",
            "*   Problem-solving\n",
            "*   Mentoring junior engineers\n",
            "*   Leading design and development\n",
            "*   Conducting code reviews\n",
            "*   Collaborating with product managers and UX designers\n",
            "*   Translating requirements into technical specifications\n",
            "\n",
            "### 2. Extract Required Job Skills\n",
            "\n",
            "**Experience & Core Requirements:**\n",
            "*   10+ years of experience in software development\n",
            "*   Expertise in Python, Java, or Go (proficiency in at least one)\n",
            "*   Strong knowledge of cloud platforms (AWS, Azure, GCP)\n",
            "*   Experience with containerization (Docker, Kubernetes)\n",
            "*   Excellent problem-solving skills\n",
            "*   Excellent communication skills\n",
            "\n",
            "**Responsibilities (implying skills):**\n",
            "*   Designing and implementing scalable software solutions\n",
            "*   Mentoring junior developers\n",
            "*   Contributing to architectural decisions\n",
            "\n",
            "**\"Plus\" Skills:**\n",
            "*   Familiarity with Agile methodologies\n",
            "*   Familiarity with CI/CD pipelines\n",
            "\n",
            "### 3. Skills Comparison\n",
            "\n",
            "**Matched Skills:**\n",
            "*   **Experience:** 10+ years of software development (Candidate has 10 years)\n",
            "*   **Programming Languages:** Python, Java (Job requires expertise in Python, Java, *or* Go; candidate has both Python and Java expertise)\n",
            "*   **Cloud Platforms:** AWS, Azure, GCP (Candidate has extensive experience in all listed)\n",
            "*   **Containerization:** Docker, Kubernetes\n",
            "*   **Scalable Solutions:** Designing and implementing scalable software solutions (Candidate led development of a microservices platform)\n",
            "*   **Mentoring:** Mentoring junior developers (Candidate has direct experience)\n",
            "*   **Architectural Contributions:** Contributing to architectural decisions (Candidate led design efforts)\n",
            "*   **Problem-solving:** Excellent problem-solving skills\n",
            "*   **Methodologies (Plus):** Agile\n",
            "*   **CI/CD (Plus):** CI/CD pipelines (Candidate implemented)\n",
            "\n",
            "**Missing Skills (Gaps):**\n",
            "*   **Go:** While the job offers it as an alternative to Python/Java, the candidate does not list Go. However, since Python and Java are covered, this is not a critical gap.\n",
            "*   **Communication skills:** Explicitly mentioned as \"excellent communication skills\" in the job description, but not directly stated as a skill on the resume (though implied by collaboration and mentoring roles).\n",
            "\n",
            "**Additional Skills (Present in Resume, Not Explicitly Required by Job):**\n",
            "*   JavaScript, C++ (Programming Languages)\n",
            "*   Django, Flask, React, Angular (Web Frameworks)\n",
            "*   PostgreSQL, MongoDB, MySQL, SQL, NoSQL (Specific Databases)\n",
            "*   Jenkins, Git, JIRA (Tools & Technologies)\n",
            "*   Scrum (Methodology)\n",
            "*   Microservices (Specific architectural approach/experience)\n",
            "*   Code reviews, Collaboration, Technical specifications, Database optimization (Specific experience/tasks)\n",
            "\n",
            "### 4. Overall Fit Summary\n",
            "\n",
            "John Doe presents an **exceptionally strong fit** for the Senior Software Engineer position. He possesses the required 10 years of experience and demonstrates deep expertise in all the core technical skills specified: Python, AWS, Kubernetes, Docker, and other major cloud platforms. His experience in designing and leading the development of scalable solutions, implementing CI/CD pipelines, and mentoring junior engineers directly aligns with the job's responsibilities and \"plus\" requirements.\n",
            "\n",
            "The only minor potential area for development or clarification is the explicit mention of \"excellent communication skills,\" which is not directly listed but is strongly implied by his collaborative work and mentoring roles. His resume showcases a wealth of additional relevant technical skills and experiences that would be highly beneficial to the team.\n",
            "\n",
            "Overall, John Doe is a **highly qualified candidate** who meets or exceeds nearly all the job's requirements, making him an excellent prospect for this role.\n",
            "---------------------------\n",
            "Skills gap analysis workflow completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04d55593"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Summarize the successful implementation of the new workflow, including resume analysis, job description comparison, and skills gap identification using Gemini 2.5 Flash.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eec3d87a"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   The workflow successfully replaced the previous job search and application submission functionality with a resume analysis and skills gap identification process.\n",
        "*   Initial setup required the installation of `PyPDF2` for PDF text extraction.\n",
        "*   Configuring the Google Generative AI client faced challenges, including missing API key environment variables and `NotFound` errors for specific Gemini models (e.g., `gemini-1.5-flash`, `gemini-pro`). This was resolved by implementing a robust model loading strategy that lists available models and prioritizes `models/gemini-2.5-flash`.\n",
        "*   Dummy `resume_path` (PDF) and `job_url` (HTML) inputs were successfully created and processed to extract text content, demonstrating the data ingestion capabilities.\n",
        "*   The `Agent` and `Tool` definitions were updated, removing old functionalities and introducing a new `analysis_tool` integrated into the `candidate_agent` for resume and job description comparison.\n",
        "*   A core `analyze_skills_and_gaps` function was developed, leveraging the Gemini 2.5 Flash model through a structured prompt to identify candidate skills, required job skills, matched skills, missing skills (gaps), and provide an overall fit summary.\n",
        "*   The complete workflow, orchestrated by the `CoordinatorAgent`, successfully executed the analysis, producing a detailed LLM-generated report highlighting skill matches and gaps.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   The implemented workflow provides a valuable automated solution for initial candidate screening by rapidly identifying critical skills gaps using an advanced LLM, thereby streamlining the recruitment process.\n",
        "*   To enhance the workflow, consider parsing the LLM's structured output (e.g., into JSON format) to enable programmatic access to identified skills and gaps. This would allow for more advanced filtering, visualization, and integration with other HR systems.\n"
      ]
    }
  ]
}