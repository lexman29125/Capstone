{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit PyPDF2 beautifulsoup4 requests google-generativeai nest-asyncio"
      ],
      "metadata": {
        "id": "ltu3SAojn68Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "KxKOZqX9l86E"
      },
      "outputs": [],
      "source": [
        "import streamlit as st\n",
        "import os\n",
        "import io\n",
        "import PyPDF2\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "import random\n",
        "import re\n",
        "import json\n",
        "from google import genai\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from dotenv import load_dotenv\n",
        "# Load .env file\n",
        "load_dotenv()\n",
        "\n",
        "# Check if Streamlit is running in a script context\n",
        "# This is a common way to check if the script is being executed by Streamlit or directly by Python\n",
        "IS_STREAMLIT_RUNNING = False\n",
        "if 'st' in globals() and hasattr(st, '_is_running_with_streamlit'): # Using the more common internal check\n",
        "    IS_STREAMLIT_RUNNING = st._is_running_with_streamlit\n",
        "    if IS_STREAMLIT_RUNNING:\n",
        "        print(\"Streamlit script run context detected.\")\n",
        "    else:\n",
        "        print(\"Streamlit module imported, but not running in a script run context.\")\n",
        "elif 'st' in globals() and hasattr(st, 'runtime') and hasattr(st.runtime, 'scriptrunner') and hasattr(st.runtime.scriptrunner, 'is_in_script_run_context'):\n",
        "    IS_STREAMLIT_RUNNING = st.runtime.scriptrunner.is_in_script_run_context()\n",
        "    if IS_STREAMLIT_RUNNING:\n",
        "        print(\"Streamlit script run context detected via runtime check.\")\n",
        "    else:\n",
        "        print(\"Streamlit module imported, but not running in a script run context via runtime check.\")\n",
        "else:\n",
        "    print(\"Streamlit module not fully initialized or relevant runtime attributes missing for context check.\")\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Helper function to parse salary from string (might not be used in current workflow but kept for consistency)\n",
        "def parse_salary(salary_str: str) -> int:\n",
        "    if not salary_str: return 0\n",
        "    # Remove non-numeric characters except comma, then remove comma, then convert to int\n",
        "    numeric_str = re.sub(r'[^\\\\d,]', '', salary_str)\n",
        "    numeric_str = numeric_str.replace(',', '')\n",
        "    try:\n",
        "        return int(numeric_str)\n",
        "    except ValueError:\n",
        "        return 0\n",
        "\n",
        "# Tool and Agent class definitions\n",
        "class Tool:\n",
        "    def __init__(self, func, name, description):\n",
        "        self.func = func\n",
        "        self.name = name\n",
        "        self.description = description\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, name, instruction, tools: list):\n",
        "        self.name = name\n",
        "        self.instruction = instruction\n",
        "        self.tools = tools\n",
        "\n",
        "# Helper function to extract text from URL\n",
        "def extract_text_from_url(url: str) -> str:\n",
        "    \"\"\"Extracts text content from a given URL, typically for a job description.\"\"\"\n",
        "    try:\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/50 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "        }\n",
        "        # Modified: Increased timeout from 10 to 30 seconds\n",
        "        response = requests.get(url, headers=headers, timeout=30)\n",
        "        response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Remove script and style elements\n",
        "        for script_or_style in soup(['script', 'style']):\n",
        "            script_or_style.extract()\n",
        "\n",
        "        # Get text and clean it\n",
        "        text = soup.get_text()\n",
        "        lines = (line.strip() for line in text.splitlines())\n",
        "        # Break multi-headlines into a line each\n",
        "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
        "        # Drop blank lines\n",
        "        text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
        "        return text\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        if IS_STREAMLIT_RUNNING:\n",
        "            st.error(f\"Error fetching URL {url}: {e}\")\n",
        "        else:\n",
        "            print(f\"Error fetching URL {url}: {e}\")\n",
        "        return \"\"\n",
        "    except Exception as e:\n",
        "        if IS_STREAMLIT_RUNNING:\n",
        "            st.error(f\"Error processing URL {url}: {e}\")\n",
        "        else:\n",
        "            print(f\"Error processing URL {url}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "llm_model_name = \"gemini-2.5-flash\" # Using a suitable Gemini model\n",
        "\n",
        "# Configure the Gemini client for google-genai 1.52.0+\n",
        "GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')\n",
        "gemini_model = genai.Client(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "def analyze_skills_and_gaps(resume_text: str, job_description_text: str) -> str:\n",
        "    \"\"\"Analyzes a candidate's resume against a job description using the LLM to identify skills and gaps.\"\"\"\n",
        "    # Modified: Combine system prompt into user prompt for Gemini's single-turn API\n",
        "    full_prompt = f\"\"\"\n",
        "    You are an expert HR analyst. Your task is to compare a candidate's resume with a job description.\n",
        "    Provide your output as a JSON object ONLY. Do not include any other text or explanation outside the JSON.\n",
        "\n",
        "    Here is the candidate's Resume:\n",
        "    ---\n",
        "    {resume_text}\n",
        "    ---\n",
        "\n",
        "    Here is the Job Description:\n",
        "    ---\n",
        "    {job_description_text}\n",
        "    ---\n",
        "\n",
        "    JSON Schema:\n",
        "    {{\n",
        "        \"candidate_skills\": [\"string\"], # List of key technical and soft skills explicitly mentioned in the resume.\n",
        "        \"required_job_skills\": [\"string\"], # List of essential technical and soft skills mentioned in the job description.\n",
        "        \"matched_skills\": [\"string\"], # Skills present in both the resume and the job description.\n",
        "        \"missing_skills\": [\"string\"], # Skills required by the job description but NOT found in the resume.\n",
        "        \"additional_skills\": [\"string\"], # Skills present in the resume but not explicitly required by the job description.\n",
        "        \"overall_fit_summary\": \"string\" # A brief summary of how well the candidate's skills align with the job requirements.\n",
        "    }}\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        response = gemini_model.models.generate_content(\n",
        "            model=llm_model_name,\n",
        "            contents=full_prompt\n",
        "        )\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        return f\"Error during LLM analysis: {e}\"\n",
        "\n",
        "def analyze_resume_job_description_full(resume_text: str, job_description_text: str) -> dict:\n",
        "    \"\"\"Performs a full resume and job description analysis using the LLM.\n",
        "    This function replaces the placeholder and calls analyze_skills_and_gaps.\n",
        "    \"\"\"\n",
        "    if IS_STREAMLIT_RUNNING:\n",
        "        st.info(f\"Initiating LLM-based analysis for resume (length: {len(resume_text)}) and job description (length: {len(job_description_text)}).\")\n",
        "    else:\n",
        "        print(f\"Initiating LLM-based analysis for resume (length: {len(resume_text)}) and job description (length: {len(job_description_text)}).\")\n",
        "    analysis_report = analyze_skills_and_gaps(resume_text, job_description_text)\n",
        "\n",
        "    if \"Error during LLM analysis\" in analysis_report:\n",
        "        return {\"analysis_status\": \"failure\", \"message\": analysis_report}\n",
        "    else:\n",
        "        try:\n",
        "            cleaned_report = analysis_report.strip()\n",
        "            if cleaned_report.startswith('```json') and cleaned_report.endswith('```'):\n",
        "                cleaned_report = cleaned_report[len('```json'):-len('```')].strip()\n",
        "\n",
        "            parsed_report = json.loads(cleaned_report)\n",
        "            return {\"analysis_status\": \"success\", \"message\": \"LLM-based analysis completed and parsed.\", \"parsed_report\": parsed_report}\n",
        "        except json.JSONDecodeError as e:\n",
        "            return {\"analysis_status\": \"failure\", \"message\": f\"Failed to parse LLM output as JSON: {e}\", \"raw_report\": analysis_report}\n",
        "        except Exception as e:\n",
        "            return {\"analysis_status\": \"failure\", \"message\": f\"An unexpected error occurred during JSON parsing: {e}\", \"raw_report\": analysis_report}\n",
        "\n",
        "# Re-define Tool instances\n",
        "analysis_tool = Tool(\n",
        "    func=analyze_resume_job_description_full,\n",
        "    name=\"analyze_resume_job_description\",\n",
        "    description=\"Analyzes a candidate's resume against a job description to identify skills and gaps using an LLM.\"\n",
        ")\n",
        "\n",
        "# Re-define Agent instances\n",
        "candidate_agent = Agent(\n",
        "    name=\"candidate_agent\",\n",
        "    instruction=\"I manage candidate profiles and analyze resumes against job descriptions.\",\n",
        "    tools=[analysis_tool]\n",
        ")\n",
        "\n",
        "# Redefine CoordinatorAgent to reflect new workflow\n",
        "class CoordinatorAgent(Agent):\n",
        "    def __init__(self, name: str, instruction: str, tools: list = None, sub_agents: list = None):\n",
        "        super().__init__(name, instruction, tools if tools is not None else [])\n",
        "        self.sub_agents = sub_agents if sub_agents is not None else []\n",
        "\n",
        "    async def run_live(self, resume_text: str, job_description_text: str):\n",
        "        yield f\"üöÄ CoordinatorAgent '{self.name}' initiating resume and job description analysis...\"\n",
        "\n",
        "        candidate_agent_found = next((agent for agent in self.sub_agents if agent.name == \"candidate_agent\"), None)\n",
        "        if not candidate_agent_found:\n",
        "            yield \"‚ùå Error: candidate_agent not found.\"\n",
        "            return\n",
        "\n",
        "        analysis_tool_instance = next((tool for tool in candidate_agent_found.tools if tool.name == \"analyze_resume_job_description\"), None)\n",
        "        if not analysis_tool_instance:\n",
        "            yield \"‚ùå Error: analyze_resume_job_description tool not found for candidate_agent.\"\n",
        "            return\n",
        "\n",
        "        yield f\"‚öôÔ∏è Delegating analysis to {candidate_agent_found.name} using {analysis_tool_instance.name} tool...\"\n",
        "        try:\n",
        "            analysis_result = analysis_tool_instance.func(resume_text, job_description_text)\n",
        "            if analysis_result.get('analysis_status') == 'success':\n",
        "                yield f\"‚úÖ Analysis complete: {analysis_result.get('message', 'No message provided.')}\"\n",
        "                yield \"<h2>Analysis Report</h2>\" # Moved here to display after initial messages\n",
        "\n",
        "                parsed_report = analysis_result.get('parsed_report', {})\n",
        "\n",
        "                report_html_parts = []\n",
        "\n",
        "                # Overall Fit Summary\n",
        "                overall_fit_summary = parsed_report.get('overall_fit_summary', 'N/A')\n",
        "                report_html_parts.append(f\"<p><b>Overall Fit Summary:</b> {overall_fit_summary}</p>\")\n",
        "\n",
        "                # Candidate Skills\n",
        "                candidate_skills = parsed_report.get('candidate_skills', [])\n",
        "                if candidate_skills:\n",
        "                    report_html_parts.append(\"<h4>Candidate Skills:</h4><ul>\")\n",
        "                    for skill in candidate_skills:\n",
        "                        report_html_parts.append(f\"<li>{skill}</li>\")\n",
        "                    report_html_parts.append(\"</ul>\")\n",
        "\n",
        "                # Required Job Skills\n",
        "                required_job_skills = parsed_report.get('required_job_skills', [])\n",
        "                if required_job_skills:\n",
        "                    report_html_parts.append(\"<h4>Required Job Skills:</h4><ul>\")\n",
        "                    for skill in required_job_skills:\n",
        "                        report_html_parts.append(f\"<li>{skill}</li>\")\n",
        "                    report_html_parts.append(\"</ul>\")\n",
        "\n",
        "                # Matched Skills\n",
        "                matched_skills = parsed_report.get('matched_skills', [])\n",
        "                if matched_skills:\n",
        "                    report_html_parts.append(\"<h4>Matched Skills:</h4><ul>\")\n",
        "                    for skill in matched_skills:\n",
        "                        report_html_parts.append(f\"<li>{skill}</li>\")\n",
        "                    report_html_parts.append(\"</ul>\")\n",
        "\n",
        "                # Missing Skills\n",
        "                missing_skills = parsed_report.get('missing_skills', [])\n",
        "                if missing_skills:\n",
        "                    report_html_parts.append(\"<h4 style=\\\"color:red;\\\">Missing Skills (Gaps):</h4><ul>\")\n",
        "                    for skill in missing_skills:\n",
        "                        report_html_parts.append(f\"<li style=\\\"color:red;\\\">{skill}</li>\")\n",
        "                    report_html_parts.append(\"</ul>\")\n",
        "\n",
        "                # Additional Skills\n",
        "                additional_skills = parsed_report.get('additional_skills', [])\n",
        "                if additional_skills:\n",
        "                    report_html_parts.append(\"<h4>Additional Skills:</h4><ul>\")\n",
        "                    for skill in additional_skills:\n",
        "                        report_html_parts.append(f\"<li>{skill}</li>\")\n",
        "                    report_html_parts.append(\"</ul>\")\n",
        "\n",
        "                yield \"\\n\".join(report_html_parts) # Yield the complete HTML string\n",
        "\n",
        "            else:\n",
        "                yield f\"‚ùå Analysis failed: {analysis_result.get('message', 'Unknown error.')}\"\n",
        "                if 'raw_report' in analysis_result:\n",
        "                    yield f\"Raw LLM output: {analysis_result['raw_report']}\"\n",
        "        except Exception as e:\n",
        "            yield f\"‚ùå Error during analysis: {e}\"\n",
        "            return\n",
        "\n",
        "# Re-instantiate the CoordinatorAgent with the new class definition and updated sub-agents\n",
        "root_agent = CoordinatorAgent(\n",
        "    name=\"root_agent\",\n",
        "    instruction=\"I orchestrate the resume and job description analysis process.\",\n",
        "    sub_agents=[candidate_agent]\n",
        ")\n",
        "\n",
        "\n",
        "# --- Streamlit UI and Workflow Integration ---\n",
        "\n",
        "st.set_page_config(\n",
        "    page_title=\"AI-Powered Resume and Job Description Analyzer\",\n",
        "    layout=\"wide\",\n",
        "    initial_sidebar_state=\"expanded\"\n",
        ")\n",
        "st.markdown(\"<h1 style='text-align: center; color: #4CAF50;'> üîç AI Job Search Assistant </h1> <p style='text-align:center; font-size:18px;'> Discover tailored job recommendations powered by Agentic AI. </p>\", unsafe_allow_html=True)\n",
        "st.sidebar.header(\"User Inputs\")\n",
        "\n",
        "job_url_input = st.sidebar.text_input(\n",
        "    \"Job Description URL\",\n",
        "    value=\"https://example.com/job_description\",\n",
        "    help=\"Enter the URL of the job description webpage.\"\n",
        ")\n",
        "\n",
        "uploaded_resume_file = st.sidebar.file_uploader(\n",
        "    \"Upload Your Resume (PDF)\",\n",
        "    type=[\"pdf\"],\n",
        "    help=\"Upload your resume in PDF format.\"\n",
        ")\n",
        "\n",
        "is_valid_job_url = False\n",
        "if job_url_input:\n",
        "    if job_url_input.startswith(\"http://\") or job_url_input.startswith(\"https://\"):\n",
        "        is_valid_job_url = True\n",
        "    else:\n",
        "        st.sidebar.error(\"Please enter a valid URL (starting with http:// or https://).\")\n",
        "\n",
        "is_resume_uploaded = False\n",
        "if uploaded_resume_file is not None:\n",
        "    is_resume_uploaded = True\n",
        "\n",
        "if st.sidebar.button(\"Run Analysis\", disabled=(not is_valid_job_url or not is_resume_uploaded)):\n",
        "    if is_valid_job_url and is_resume_uploaded:\n",
        "        with st.spinner(\"Processing resume and fetching job description...\"):\n",
        "            resume_text = \"\"\n",
        "            try:\n",
        "                pdf_reader = PyPDF2.PdfReader(io.BytesIO(uploaded_resume_file.getvalue()))\n",
        "                resume_text = \"\".join([page.extract_text() for page in pdf_reader.pages])\n",
        "                st.success(\"Resume extracted successfully.\")\n",
        "            except Exception as e:\n",
        "                st.error(f\"Error reading resume PDF: {e}\")\n",
        "                resume_text = \"\"\n",
        "\n",
        "            job_description_text = \"\"\n",
        "            try:\n",
        "                job_description_text = extract_text_from_url(job_url_input)\n",
        "                if job_description_text:\n",
        "                    st.success(\"Job description fetched successfully.\")\n",
        "                else:\n",
        "                    st.error(\"Failed to fetch job description. Please check the URL.\")\n",
        "            except Exception as e:\n",
        "                st.error(f\"Error fetching job description from URL: {e}\")\n",
        "                job_description_text = \"\"\n",
        "\n",
        "        if resume_text and job_description_text:\n",
        "            # st.subheader(\"Analysis Report\") # Removed this line\n",
        "            progress_bar = st.progress(0)\n",
        "            status_text = st.empty()\n",
        "            report_container = st.empty()\n",
        "\n",
        "            async def run_analysis_workflow_streamlit(res_text: str, jd_text: str):\n",
        "                output_lines = []\n",
        "                total_steps = 7\n",
        "                current_step = 0\n",
        "                async for step_output in root_agent.run_live(res_text, jd_text):\n",
        "                    output_lines.append(step_output)\n",
        "                    status_text.text(step_output)\n",
        "                    current_step += 1\n",
        "                    progress_bar.progress(min(current_step / total_steps, 1.0))\n",
        "                return output_lines\n",
        "\n",
        "            st.write(\"Starting AI analysis...\")\n",
        "            full_report_lines = []\n",
        "            # Use asyncio.run to execute the async generator\n",
        "            for step_output in asyncio.run(run_analysis_workflow_streamlit(resume_text, job_description_text)):\n",
        "                 full_report_lines.append(step_output)\n",
        "\n",
        "            progress_bar.empty()\n",
        "            status_text.empty()\n",
        "            report_container.markdown(\"\\n\".join(full_report_lines), unsafe_allow_html=True)\n",
        "\n",
        "        else:\n",
        "            st.error(\"Analysis cannot be performed due to missing resume text or job description text.\")\n",
        "    else:\n",
        "        st.error(\"Please fix the input errors before running analysis.\")\n",
        "else:\n",
        "    if not is_valid_job_url or not is_resume_uploaded:\n",
        "        st.warning(\"Please provide a valid Job URL and upload your resume to proceed.\")\n",
        "    else:\n",
        "        st.success(\"Job URL and Resume uploaded successfully. Ready for analysis!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f31f6091"
      },
      "source": [
        "# Task\n",
        "Refactor the existing monolithic Colab notebook into separate backend and frontend components. The backend should contain the core agentic AI logic, configured for secure API key handling and using `gemini-2.5-flash` as the LLM, and should be capable of independent deployment (e.g., to Google Cloud Run). The frontend, a Streamlit application, should be modified to interact with a hypothetical deployed backend API. Finally, demonstrate the functionality of the separated backend and provide instructions for deploying both components to Google Cloud Run, acknowledging the limitations of testing the interactive Streamlit frontend within Colab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0a5e2f8"
      },
      "source": [
        "## Update Dependencies\n",
        "\n",
        "### Subtask:\n",
        "Install the specified versions of required Python packages.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ae5201a"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask is to install specified Python packages. This command will install all required packages with their exact versions in a single pip command.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2585766f",
        "outputId": "b228f11a-0e98-4c2b-8781-dc86cbd92e4a"
      },
      "source": [
        "get_ipython().system('pip install streamlit==1.33.0 PyPDF2==3.0.1 beautifulsoup4==4.12.3 requests==2.32.3 nest-asyncio==1.6.0 openai==1.109.1 python-dotenv google-generativeai==1.52.0 pydeck==0.9.1')"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit==1.33.0\n",
            "  Downloading streamlit-1.33.0-py2.py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: PyPDF2==3.0.1 in /usr/local/lib/python3.12/dist-packages (3.0.1)\n",
            "Collecting beautifulsoup4==4.12.3\n",
            "  Downloading beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting requests==2.32.3\n",
            "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: nest-asyncio==1.6.0 in /usr/local/lib/python3.12/dist-packages (1.6.0)\n",
            "Requirement already satisfied: openai==1.109.1 in /usr/local/lib/python3.12/dist-packages (1.109.1)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.2.1)\n",
            "\u001b[31mERROR: Ignored the following yanked versions: 0.1.0rc2\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Ignored the following versions that require a different python version: 0.55.2 Requires-Python <3.5\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement google-generativeai==1.52.0 (from versions: 0.1.0rc1, 0.1.0rc3, 0.1.0, 0.2.0, 0.2.1, 0.2.2, 0.3.0, 0.3.1, 0.3.2, 0.4.0, 0.4.1, 0.5.0, 0.5.1, 0.5.2, 0.5.3, 0.5.4, 0.6.0, 0.7.0, 0.7.1, 0.7.2, 0.8.0, 0.8.1, 0.8.2, 0.8.3, 0.8.4, 0.8.5)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for google-generativeai==1.52.0\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4705ffc"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt to install `google-generativeai==1.52.0` failed because that version is not available. Based on the error message, the latest available version seems to be `0.8.5`. I will update the installation command to use `google-generativeai==0.8.5` and reinstall all packages to ensure consistency.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5420f7b3",
        "outputId": "21b52918-11fb-4ec4-f762-c6215a651da1"
      },
      "source": [
        "get_ipython().system('pip install streamlit==1.33.0 PyPDF2==3.0.1 beautifulsoup4==4.12.3 requests==2.32.3 nest-asyncio==1.6.0 openai==1.109.1 python-dotenv google-generativeai==0.8.5 pydeck==0.9.1')"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit==1.33.0\n",
            "  Using cached streamlit-1.33.0-py2.py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: PyPDF2==3.0.1 in /usr/local/lib/python3.12/dist-packages (3.0.1)\n",
            "Collecting beautifulsoup4==4.12.3\n",
            "  Using cached beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting requests==2.32.3\n",
            "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: nest-asyncio==1.6.0 in /usr/local/lib/python3.12/dist-packages (1.6.0)\n",
            "Requirement already satisfied: openai==1.109.1 in /usr/local/lib/python3.12/dist-packages (1.109.1)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.2.1)\n",
            "Requirement already satisfied: google-generativeai==0.8.5 in /usr/local/lib/python3.12/dist-packages (0.8.5)\n",
            "Requirement already satisfied: pydeck==0.9.1 in /usr/local/lib/python3.12/dist-packages (0.9.1)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit==1.33.0) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from streamlit==1.33.0) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit==1.33.0) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit==1.33.0) (8.3.0)\n",
            "Collecting numpy<2,>=1.19.3 (from streamlit==1.33.0)\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting packaging<25,>=16.8 (from streamlit==1.33.0)\n",
            "  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: pandas<3,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from streamlit==1.33.0) (2.2.2)\n",
            "Collecting pillow<11,>=7.1.0 (from streamlit==1.33.0)\n",
            "  Downloading pillow-10.4.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
            "Collecting protobuf<5,>=3.20 (from streamlit==1.33.0)\n",
            "  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit==1.33.0) (18.1.0)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.12/dist-packages (from streamlit==1.33.0) (13.9.4)\n",
            "Requirement already satisfied: tenacity<9,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit==1.33.0) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit==1.33.0) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from streamlit==1.33.0) (4.15.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit==1.33.0) (3.1.45)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit==1.33.0) (6.5.1)\n",
            "Requirement already satisfied: watchdog>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit==1.33.0) (6.0.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4==4.12.3) (2.8)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests==2.32.3) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests==2.32.3) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests==2.32.3) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests==2.32.3) (2025.10.5)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai==1.109.1) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai==1.109.1) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai==1.109.1) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai==1.109.1) (0.12.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai==1.109.1) (2.11.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai==1.109.1) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai==1.109.1) (4.67.1)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/lib/python3.12/dist-packages (from google-generativeai==0.8.5) (0.6.15)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.12/dist-packages (from google-generativeai==0.8.5) (2.28.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.12/dist-packages (from google-generativeai==0.8.5) (2.187.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.12/dist-packages (from google-generativeai==0.8.5) (2.38.0)\n",
            "Requirement already satisfied: jinja2>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from pydeck==0.9.1) (3.1.6)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage==0.6.15->google-generativeai==0.8.5) (1.26.1)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair<6,>=4.0->streamlit==1.33.0) (4.25.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair<6,>=4.0->streamlit==1.33.0) (2.11.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit==1.33.0) (4.0.12)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core->google-generativeai==0.8.5) (1.72.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai==0.8.5) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=2.15.0->google-generativeai==0.8.5) (4.9.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai==1.109.1) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.109.1) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2>=2.10.1->pydeck==0.9.1) (3.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.3.0->streamlit==1.33.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.3.0->streamlit==1.33.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.3.0->streamlit==1.33.0) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai==1.109.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai==1.109.1) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai==1.109.1) (0.4.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich<14,>=10.14.0->streamlit==1.33.0) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich<14,>=10.14.0->streamlit==1.33.0) (2.19.2)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai==0.8.5) (0.31.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai==0.8.5) (0.2.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from google-api-python-client->google-generativeai==0.8.5) (4.2.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit==1.33.0) (5.0.2)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai==0.8.5) (1.76.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai==0.8.5) (1.71.2)\n",
            "Requirement already satisfied: pyparsing<4,>=3.0.4 in /usr/local/lib/python3.12/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai==0.8.5) (3.2.5)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit==1.33.0) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit==1.33.0) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit==1.33.0) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit==1.33.0) (0.28.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit==1.33.0) (0.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai==0.8.5) (0.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.3.0->streamlit==1.33.0) (1.17.0)\n",
            "INFO: pip is looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting grpcio-status<2.0.0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai==0.8.5)\n",
            "  Downloading grpcio_status-1.76.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.75.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.75.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.74.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.73.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.73.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.72.2-py3-none-any.whl.metadata (1.1 kB)\n",
            "INFO: pip is still looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading grpcio_status-1.72.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.71.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.70.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.69.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.68.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading grpcio_status-1.68.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.67.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.67.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.66.2-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.66.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.66.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.65.5-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.65.4-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.65.2-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.65.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.64.3-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.64.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.64.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.63.2-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.63.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.62.3-py3-none-any.whl.metadata (1.3 kB)\n",
            "Downloading streamlit-1.33.0-py2.py3-none-any.whl (8.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m81.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m147.9/147.9 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m114.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading packaging-24.2-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow-10.4.0-cp312-cp312-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m111.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading grpcio_status-1.62.3-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: requests, protobuf, pillow, packaging, numpy, beautifulsoup4, grpcio-status, streamlit\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.4\n",
            "    Uninstalling requests-2.32.4:\n",
            "      Successfully uninstalled requests-2.32.4\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.5\n",
            "    Uninstalling protobuf-5.29.5:\n",
            "      Successfully uninstalled protobuf-5.29.5\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: pillow 11.3.0\n",
            "    Uninstalling pillow-11.3.0:\n",
            "      Successfully uninstalled pillow-11.3.0\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 25.0\n",
            "    Uninstalling packaging-25.0:\n",
            "      Successfully uninstalled packaging-25.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.13.5\n",
            "    Uninstalling beautifulsoup4-4.13.5:\n",
            "      Successfully uninstalled beautifulsoup4-4.13.5\n",
            "  Attempting uninstall: grpcio-status\n",
            "    Found existing installation: grpcio-status 1.71.2\n",
            "    Uninstalling grpcio-status-1.71.2:\n",
            "      Successfully uninstalled grpcio-status-1.71.2\n",
            "  Attempting uninstall: streamlit\n",
            "    Found existing installation: streamlit 1.51.0\n",
            "    Uninstalling streamlit-1.51.0:\n",
            "      Successfully uninstalled streamlit-1.51.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.3 which is incompatible.\n",
            "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "google-adk 1.17.0 requires requests<3.0.0,>=2.32.4, but you have requests 2.32.3 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 4.25.8 which is incompatible.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed beautifulsoup4-4.12.3 grpcio-status-1.62.3 numpy-1.26.4 packaging-24.2 pillow-10.4.0 protobuf-4.25.8 requests-2.32.3 streamlit-1.33.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "bs4",
                  "google",
                  "grpc_status",
                  "numpy",
                  "packaging",
                  "requests",
                  "streamlit"
                ]
              },
              "id": "b2aa33e554ae4dfe9000801596b87c1f"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ec35508"
      },
      "source": [
        "## Create Backend Agentic AI Logic (Colab Secret Ready)\n",
        "\n",
        "### Subtask:\n",
        "Generate a new code cell containing the core AI logic, stripped of Streamlit dependencies, configured for secure API key handling, and initialized for `gemini-2.5-flash` LLM.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "526365a7"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires creating a backend component with the core AI logic. This command will generate a new code cell containing all necessary imports, API key handling for Colab and local environments, LLM configuration, and refactored versions of the `extract_text_from_url`, `analyze_skills_and_gaps`, `analyze_resume_job_description_full` functions, along with the `Tool`, `Agent`, and `CoordinatorAgent` classes, ensuring all Streamlit-specific calls are replaced with print statements for backend compatibility.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "1802b495",
        "outputId": "d9dcc8c7-b322-457f-800d-3c6fdb3ebbce"
      },
      "source": [
        "import os\n",
        "import io\n",
        "import PyPDF2\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "import random\n",
        "import re\n",
        "import json\n",
        "from google import generativeai as genai\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Apply nest_asyncio for compatibility in environments like Colab\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# --- API Key Handling ---\n",
        "GOOGLE_API_KEY = None\n",
        "\n",
        "# Check if running in Colab and COLAB_ENVIRONMENT is set\n",
        "if 'COLAB_ENVIRONMENT' in os.environ and os.environ['COLAB_ENVIRONMENT'] == 'True':\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "        print(\"GOOGLE_API_KEY loaded from Colab secrets.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading GOOGLE_API_KEY from Colab secrets: {e}\")\n",
        "else:\n",
        "    # Load .env file for local development\n",
        "    load_dotenv()\n",
        "    GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')\n",
        "    print(\"Attempting to load GOOGLE_API_KEY from .env file or environment variables.\")\n",
        "\n",
        "if not GOOGLE_API_KEY:\n",
        "    error_message = \"GOOGLE_API_KEY not found. Please set it as an environment variable, in your .env file, or in Colab secrets.\"\n",
        "    print(f\"ERROR: {error_message}\")\n",
        "    raise ValueError(error_message)\n",
        "\n",
        "# Configure the Gemini client\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "llm_model_name = \"gemini-2.5-flash\" # Using a suitable Gemini model\n",
        "\n",
        "# Helper function to parse salary from string (might not be used in current workflow but kept for consistency)\n",
        "def parse_salary(salary_str: str) -> int:\n",
        "    if not salary_str: return 0\n",
        "    # Remove non-numeric characters except comma, then remove comma, then convert to int\n",
        "    numeric_str = re.sub(r'[^\\\\d,]', '', salary_str)\n",
        "    numeric_str = numeric_str.replace(',', '')\n",
        "    try:\n",
        "        return int(numeric_str)\n",
        "    except ValueError:\n",
        "        return 0\n",
        "\n",
        "# Tool and Agent class definitions\n",
        "class Tool:\n",
        "    def __init__(self, func, name, description):\n",
        "        self.func = func\n",
        "        self.name = name\n",
        "        self.description = description\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, name, instruction, tools: list):\n",
        "        self.name = name\n",
        "        self.instruction = instruction\n",
        "        self.tools = tools\n",
        "\n",
        "# Helper function to extract text from URL\n",
        "def extract_text_from_url(url: str) -> str:\n",
        "    \"\"\"Extracts text content from a given URL, typically for a job description.\"\"\"\n",
        "    try:\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/50 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "        }\n",
        "        response = requests.get(url, headers=headers, timeout=30)\n",
        "        response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Remove script and style elements\n",
        "        for script_or_style in soup(['script', 'style']):\n",
        "            script_or_style.extract()\n",
        "\n",
        "        # Get text and clean it\n",
        "        text = soup.get_text()\n",
        "        lines = (line.strip() for line in text.splitlines())\n",
        "        # Break multi-headlines into a line each\n",
        "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
        "        # Drop blank lines\n",
        "        text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
        "        return text\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching URL {url}: {e}\") # Replaced st.error\n",
        "        return \"\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing URL {url}: {e}\") # Replaced st.error\n",
        "        return \"\"\n",
        "\n",
        "def analyze_skills_and_gaps(resume_text: str, job_description_text: str) -> str:\n",
        "    \"\"\"Analyzes a candidate's resume against a job description using the LLM to identify skills and gaps.\"\"\"\n",
        "    # Modified: Combine system prompt into user prompt for Gemini's single-turn API\n",
        "    full_prompt = f\"\"\"\n",
        "    You are an expert HR analyst. Your task is to compare a candidate's resume with a job description.\n",
        "    Provide your output as a JSON object ONLY. Do not include any other text or explanation outside the JSON.\n",
        "\n",
        "    Here is the candidate's Resume:\n",
        "    ---\n",
        "    {resume_text}\n",
        "    ---\n",
        "\n",
        "    Here is the Job Description:\n",
        "    ---\n",
        "    {job_description_text}\n",
        "    ---\n",
        "\n",
        "    JSON Schema:\n",
        "    {{\n",
        "        \"candidate_skills\": [\"string\"], # List of key technical and soft skills explicitly mentioned in the resume.\n",
        "        \"required_job_skills\": [\"string\"], # List of essential technical and soft skills mentioned in the job description.\n",
        "        \"matched_skills\": [\"string\"], # Skills present in both the resume and the job description.\n",
        "        \"missing_skills\": [\"string\"], # Skills required by the job description but NOT found in the resume.\n",
        "        \"additional_skills\": [\"string\"], # Skills present in the resume but not explicitly required by the job description.\n",
        "        \"overall_fit_summary\": \"string\" # A brief summary of how well the candidate's skills align with the job requirements.\n",
        "    }}\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        # The gemini_model object needs to be explicitly created as it was in the original Colab. Assuming 'genai' is imported as 'generativeai'\n",
        "        # and genai.configure(api_key=GOOGLE_API_KEY) has been called.\n",
        "        model = genai.GenerativeModel(llm_model_name)\n",
        "        response = model.generate_content(contents=full_prompt)\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        return f\"Error during LLM analysis: {e}\"\n",
        "\n",
        "def analyze_resume_job_description_full(resume_text: str, job_description_text: str) -> dict:\n",
        "    \"\"\"Performs a full resume and job description analysis using the LLM.\"\"\"\n",
        "    print(f\"Initiating LLM-based analysis for resume (length: {len(resume_text)}) and job description (length: {len(job_description_text)}).\") # Replaced st.info\n",
        "    analysis_report = analyze_skills_and_gaps(resume_text, job_description_text)\n",
        "\n",
        "    if \"Error during LLM analysis\" in analysis_report:\n",
        "        return {\"analysis_status\": \"failure\", \"message\": analysis_report}\n",
        "    else:\n",
        "        try:\n",
        "            cleaned_report = analysis_report.strip()\n",
        "            if cleaned_report.startswith('```json') and cleaned_report.endswith('```'):\n",
        "                cleaned_report = cleaned_report[len('```json'):-len('```')].strip()\n",
        "\n",
        "            parsed_report = json.loads(cleaned_report)\n",
        "            return {\"analysis_status\": \"success\", \"message\": \"LLM-based analysis completed and parsed.\", \"parsed_report\": parsed_report}\n",
        "        except json.JSONDecodeError as e:\n",
        "            return {\"analysis_status\": \"failure\", \"message\": f\"Failed to parse LLM output as JSON: {e}\", \"raw_report\": analysis_report}\n",
        "        except Exception as e:\n",
        "            return {\"analysis_status\": \"failure\", \"message\": f\"An unexpected error occurred during JSON parsing: {e}\", \"raw_report\": analysis_report}\n",
        "\n",
        "# Re-define Tool instances\n",
        "analysis_tool = Tool(\n",
        "    func=analyze_resume_job_description_full,\n",
        "    name=\"analyze_resume_job_description\",\n",
        "    description=\"Analyzes a candidate's resume against a job description to identify skills and gaps using an LLM.\"\n",
        ")\n",
        "\n",
        "# Re-define Agent instances\n",
        "candidate_agent = Agent(\n",
        "    name=\"candidate_agent\",\n",
        "    instruction=\"I manage candidate profiles and analyze resumes against job descriptions.\",\n",
        "    tools=[analysis_tool]\n",
        ")\n",
        "\n",
        "# Redefine CoordinatorAgent to reflect new workflow\n",
        "class CoordinatorAgent(Agent):\n",
        "    def __init__(self, name: str, instruction: str, tools: list = None, sub_agents: list = None):\n",
        "        super().__init__(name, instruction, tools if tools is not None else [])\n",
        "        self.sub_agents = sub_agents if sub_agents is not None else []\n",
        "\n",
        "    async def run_live(self, resume_text: str, job_description_text: str):\n",
        "        yield f\"üöÄ CoordinatorAgent '{self.name}' initiating resume and job description analysis...\"\n",
        "\n",
        "        candidate_agent_found = next((agent for agent in self.sub_agents if agent.name == \"candidate_agent\"), None)\n",
        "        if not candidate_agent_found:\n",
        "            yield \"‚ùå Error: candidate_agent not found.\"\n",
        "            return\n",
        "\n",
        "        analysis_tool_instance = next((tool for tool in candidate_agent_found.tools if tool.name == \"analyze_resume_job_description\"), None)\n",
        "        if not analysis_tool_instance:\n",
        "            yield \"‚ùå Error: analyze_resume_job_description tool not found for candidate_agent.\"\n",
        "            return\n",
        "\n",
        "        yield f\"‚öôÔ∏è Delegating analysis to {candidate_agent_found.name} using {analysis_tool_instance.name} tool...\"\n",
        "        try:\n",
        "            analysis_result = analysis_tool_instance.func(resume_text, job_description_text)\n",
        "            if analysis_result.get('analysis_status') == 'success':\n",
        "                yield f\"‚úÖ Analysis complete: {analysis_result.get('message', 'No message provided.')}\"\n",
        "                yield \"<h2>Analysis Report</h2>\"\n",
        "\n",
        "                parsed_report = analysis_result.get('parsed_report', {})\n",
        "\n",
        "                report_html_parts = []\n",
        "\n",
        "                overall_fit_summary = parsed_report.get('overall_fit_summary', 'N/A')\n",
        "                report_html_parts.append(f\"<p><b>Overall Fit Summary:</b> {overall_fit_summary}</p>\")\n",
        "\n",
        "                candidate_skills = parsed_report.get('candidate_skills', [])\n",
        "                if candidate_skills:\n",
        "                    report_html_parts.append(\"<h4>Candidate Skills:</h4><ul>\")\n",
        "                    for skill in candidate_skills:\n",
        "                        report_html_parts.append(f\"<li>{skill}</li>\")\n",
        "                    report_html_parts.append(\"</ul>\")\n",
        "\n",
        "                required_job_skills = parsed_report.get('required_job_skills', [])\n",
        "                if required_job_skills:\n",
        "                    report_html_parts.append(\"<h4>Required Job Skills:</h4><ul>\")\n",
        "                    for skill in required_job_skills:\n",
        "                        report_html_parts.append(f\"<li>{skill}</li>\")\n",
        "                    report_html_parts.append(\"</ul>\")\n",
        "\n",
        "                matched_skills = parsed_report.get('matched_skills', [])\n",
        "                if matched_skills:\n",
        "                    report_html_parts.append(\"<h4>Matched Skills:</h4><ul>\")\n",
        "                    for skill in matched_skills:\n",
        "                        report_html_parts.append(f\"<li>{skill}</li>\")\n",
        "                    report_html_parts.append(\"</ul>\")\n",
        "\n",
        "                missing_skills = parsed_report.get('missing_skills', [])\n",
        "                if missing_skills:\n",
        "                    report_html_parts.append(\"<h4 style=\\\"color:red;\\\">Missing Skills (Gaps):</h4><ul>\")\n",
        "                    for skill in missing_skills:\n",
        "                        report_html_parts.append(f\"<li style=\\\"color:red;\\\">{skill}</li>\")\n",
        "                    report_html_parts.append(\"</ul>\")\n",
        "\n",
        "                additional_skills = parsed_report.get('additional_skills', [])\n",
        "                if additional_skills:\n",
        "                    report_html_parts.append(\"<h4>Additional Skills:</h4><ul>\")\n",
        "                    for skill in additional_skills:\n",
        "                        report_html_parts.append(f\"<li>{skill}</li>\")\n",
        "                    report_html_parts.append(\"</ul>\")\n",
        "\n",
        "                yield \"\\n\".join(report_html_parts)\n",
        "\n",
        "            else:\n",
        "                yield f\"‚ùå Analysis failed: {analysis_result.get('message', 'Unknown error.')}\"\n",
        "                if 'raw_report' in analysis_result:\n",
        "                    yield f\"Raw LLM output: {analysis_result['raw_report']}\"\n",
        "        except Exception as e:\n",
        "            yield f\"‚ùå Error during analysis: {e}\"\n",
        "            return\n",
        "\n",
        "# Instantiate the CoordinatorAgent with the new class definition and updated sub-agents\n",
        "root_agent = CoordinatorAgent(\n",
        "    name=\"root_agent\",\n",
        "    instruction=\"I orchestrate the resume and job description analysis process.\",\n",
        "    sub_agents=[candidate_agent]\n",
        ")\n",
        "print(\"Backend AI logic initialized successfully.\")"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to load GOOGLE_API_KEY from .env file or environment variables.\n",
            "ERROR: GOOGLE_API_KEY not found. Please set it as an environment variable, in your .env file, or in Colab secrets.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "GOOGLE_API_KEY not found. Please set it as an environment variable, in your .env file, or in Colab secrets.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-308706009.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0merror_message\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"GOOGLE_API_KEY not found. Please set it as an environment variable, in your .env file, or in Colab secrets.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"ERROR: {error_message}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m# Configure the Gemini client\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: GOOGLE_API_KEY not found. Please set it as an environment variable, in your .env file, or in Colab secrets."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfee678f"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed because it was incorrectly checking `COLAB_ENVIRONMENT` in `os.environ` instead of the global variable. The corrected code will check the global `COLAB_ENVIRONMENT` variable to correctly load the `GOOGLE_API_KEY` from Colab secrets when running in a Colab environment.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "9e983bb5",
        "outputId": "847ee173-2c50-40d0-87a0-828fba1ba931"
      },
      "source": [
        "import os\n",
        "import io\n",
        "import PyPDF2\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "import random\n",
        "import re\n",
        "import json\n",
        "from google import generativeai as genai\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Apply nest_asyncio for compatibility in environments like Colab\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# --- API Key Handling ---\n",
        "GOOGLE_API_KEY = None\n",
        "\n",
        "# Check if running in Colab and COLAB_ENVIRONMENT is set as a global variable\n",
        "if globals().get('COLAB_ENVIRONMENT', False) == True:\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "        print(\"GOOGLE_API_KEY loaded from Colab secrets.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading GOOGLE_API_KEY from Colab secrets: {e}. Falling back to .env.\")\n",
        "else:\n",
        "    # Load .env file for local development\n",
        "    load_dotenv()\n",
        "    GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')\n",
        "    print(\"Attempting to load GOOGLE_API_KEY from .env file or environment variables.\")\n",
        "\n",
        "if not GOOGLE_API_KEY:\n",
        "    error_message = \"GOOGLE_API_KEY not found. Please set it as an environment variable, in your .env file, or in Colab secrets.\"\n",
        "    print(f\"ERROR: {error_message}\")\n",
        "    raise ValueError(error_message)\n",
        "\n",
        "# Configure the Gemini client\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "llm_model_name = \"gemini-2.5-flash\" # Using a suitable Gemini model\n",
        "\n",
        "# Helper function to parse salary from string (might not be used in current workflow but kept for consistency)\n",
        "def parse_salary(salary_str: str) -> int:\n",
        "    if not salary_str: return 0\n",
        "    # Remove non-numeric characters except comma, then remove comma, then convert to int\n",
        "    numeric_str = re.sub(r'[^\\\\d,]', '', salary_str)\n",
        "    numeric_str = numeric_str.replace(',', '')\n",
        "    try:\n",
        "        return int(numeric_str)\n",
        "    except ValueError:\n",
        "        return 0\n",
        "\n",
        "# Tool and Agent class definitions\n",
        "class Tool:\n",
        "    def __init__(self, func, name, description):\n",
        "        self.func = func\n",
        "        self.name = name\n",
        "        self.description = description\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, name, instruction, tools: list):\n",
        "        self.name = name\n",
        "        self.instruction = instruction\n",
        "        self.tools = tools\n",
        "\n",
        "# Helper function to extract text from URL\n",
        "def extract_text_from_url(url: str) -> str:\n",
        "    \"\"\"Extracts text content from a given URL, typically for a job description.\"\"\"\n",
        "    try:\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/50 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "        }\n",
        "        response = requests.get(url, headers=headers, timeout=30)\n",
        "        response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Remove script and style elements\n",
        "        for script_or_style in soup(['script', 'style']):\n",
        "            script_or_style.extract()\n",
        "\n",
        "        # Get text and clean it\n",
        "        text = soup.get_text()\n",
        "        lines = (line.strip() for line in text.splitlines())\n",
        "        # Break multi-headlines into a line each\n",
        "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
        "        # Drop blank lines\n",
        "        text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
        "        return text\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching URL {url}: {e}\") # Replaced st.error\n",
        "        return \"\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing URL {url}: {e}\") # Replaced st.error\n",
        "        return \"\"\n",
        "\n",
        "def analyze_skills_and_gaps(resume_text: str, job_description_text: str) -> str:\n",
        "    \"\"\"Analyzes a candidate's resume against a job description using the LLM to identify skills and gaps.\"\"\"\n",
        "    # Modified: Combine system prompt into user prompt for Gemini's single-turn API\n",
        "    full_prompt = f\"\"\"\n",
        "    You are an expert HR analyst. Your task is to compare a candidate's resume with a job description.\n",
        "    Provide your output as a JSON object ONLY. Do not include any other text or explanation outside the JSON.\n",
        "\n",
        "    Here is the candidate's Resume:\n",
        "    ---\n",
        "    {resume_text}\n",
        "    ---\n",
        "\n",
        "    Here is the Job Description:\n",
        "    ---\n",
        "    {job_description_text}\n",
        "    ---\n",
        "\n",
        "    JSON Schema:\n",
        "    {{\n",
        "        \"candidate_skills\": [\"string\"], # List of key technical and soft skills explicitly mentioned in the resume.\n",
        "        \"required_job_skills\": [\"string\"], # List of essential technical and soft skills mentioned in the job description.\n",
        "        \"matched_skills\": [\"string\"], # Skills present in both the resume and the job description.\n",
        "        \"missing_skills\": [\"string\"], # Skills required by the job description but NOT found in the resume.\n",
        "        \"additional_skills\": [\"string\"], # Skills present in the resume but not explicitly required by the job description.\n",
        "        \"overall_fit_summary\": \"string\" # A brief summary of how well the candidate's skills align with the job requirements.\n",
        "    }}\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        # The gemini_model object needs to be explicitly created as it was in the original Colab. Assuming 'genai' is imported as 'generativeai'\n",
        "        # and genai.configure(api_key=GOOGLE_API_KEY) has been called.\n",
        "        model = genai.GenerativeModel(llm_model_name)\n",
        "        response = model.generate_content(contents=full_prompt)\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        return f\"Error during LLM analysis: {e}\"\n",
        "\n",
        "def analyze_resume_job_description_full(resume_text: str, job_description_text: str) -> dict:\n",
        "    \"\"\"Performs a full resume and job description analysis using the LLM.\"\"\"\n",
        "    print(f\"Initiating LLM-based analysis for resume (length: {len(resume_text)}) and job description (length: {len(job_description_text)}).\") # Replaced st.info\n",
        "    analysis_report = analyze_skills_and_gaps(resume_text, job_description_text)\n",
        "\n",
        "    if \"Error during LLM analysis\" in analysis_report:\n",
        "        return {\"analysis_status\": \"failure\", \"message\": analysis_report}\n",
        "    else:\n",
        "        try:\n",
        "            cleaned_report = analysis_report.strip()\n",
        "            if cleaned_report.startswith('```json') and cleaned_report.endswith('```'):\n",
        "                cleaned_report = cleaned_report[len('```json'):-len('```')].strip()\n",
        "\n",
        "            parsed_report = json.loads(cleaned_report)\n",
        "            return {\"analysis_status\": \"success\", \"message\": \"LLM-based analysis completed and parsed.\", \"parsed_report\": parsed_report}\n",
        "        except json.JSONDecodeError as e:\n",
        "            return {\"analysis_status\": \"failure\", \"message\": f\"Failed to parse LLM output as JSON: {e}\", \"raw_report\": analysis_report}\n",
        "        except Exception as e:\n",
        "            return {\"analysis_status\": \"failure\", \"message\": f\"An unexpected error occurred during JSON parsing: {e}\", \"raw_report\": analysis_report}\n",
        "\n",
        "# Re-define Tool instances\n",
        "analysis_tool = Tool(\n",
        "    func=analyze_resume_job_description_full,\n",
        "    name=\"analyze_resume_job_description\",\n",
        "    description=\"Analyzes a candidate's resume against a job description to identify skills and gaps using an LLM.\"\n",
        ")\n",
        "\n",
        "# Re-define Agent instances\n",
        "candidate_agent = Agent(\n",
        "    name=\"candidate_agent\",\n",
        "    instruction=\"I manage candidate profiles and analyze resumes against job descriptions.\",\n",
        "    tools=[analysis_tool]\n",
        ")\n",
        "\n",
        "# Redefine CoordinatorAgent to reflect new workflow\n",
        "class CoordinatorAgent(Agent):\n",
        "    def __init__(self, name: str, instruction: str, tools: list = None, sub_agents: list = None):\n",
        "        super().__init__(name, instruction, tools if tools is not None else [])\n",
        "        self.sub_agents = sub_agents if sub_agents is not None else []\n",
        "\n",
        "    async def run_live(self, resume_text: str, job_description_text: str):\n",
        "        yield f\"üöÄ CoordinatorAgent '{self.name}' initiating resume and job description analysis...\"\n",
        "\n",
        "        candidate_agent_found = next((agent for agent in self.sub_agents if agent.name == \"candidate_agent\"), None)\n",
        "        if not candidate_agent_found:\n",
        "            yield \"‚ùå Error: candidate_agent not found.\"\n",
        "            return\n",
        "\n",
        "        analysis_tool_instance = next((tool for tool in candidate_agent_found.tools if tool.name == \"analyze_resume_job_description\"), None)\n",
        "        if not analysis_tool_instance:\n",
        "            yield \"‚ùå Error: analyze_resume_job_description tool not found for candidate_agent.\"\n",
        "            return\n",
        "\n",
        "        yield f\"‚öôÔ∏è Delegating analysis to {candidate_agent_found.name} using {analysis_tool_instance.name} tool...\"\n",
        "        try:\n",
        "            analysis_result = analysis_tool_instance.func(resume_text, job_description_text)\n",
        "            if analysis_result.get('analysis_status') == 'success':\n",
        "                yield f\"‚úÖ Analysis complete: {analysis_result.get('message', 'No message provided.')}\"\n",
        "                yield \"<h2>Analysis Report</h2>\"\n",
        "\n",
        "                parsed_report = analysis_result.get('parsed_report', {})\n",
        "\n",
        "                report_html_parts = []\n",
        "\n",
        "                overall_fit_summary = parsed_report.get('overall_fit_summary', 'N/A')\n",
        "                report_html_parts.append(f\"<p><b>Overall Fit Summary:</b> {overall_fit_summary}</p>\")\n",
        "\n",
        "                candidate_skills = parsed_report.get('candidate_skills', [])\n",
        "                if candidate_skills:\n",
        "                    report_html_parts.append(\"<h4>Candidate Skills:</h4><ul>\")\n",
        "                    for skill in candidate_skills:\n",
        "                        report_html_parts.append(f\"<li>{skill}</li>\")\n",
        "                    report_html_parts.append(\"</ul>\")\n",
        "\n",
        "                required_job_skills = parsed_report.get('required_job_skills', [])\n",
        "                if required_job_skills:\n",
        "                    report_html_parts.append(\"<h4>Required Job Skills:</h4><ul>\")\n",
        "                    for skill in required_job_skills:\n",
        "                        report_html_parts.append(f\"<li>{skill}</li>\")\n",
        "                    report_html_parts.append(\"</ul>\")\n",
        "\n",
        "                matched_skills = parsed_report.get('matched_skills', [])\n",
        "                if matched_skills:\n",
        "                    report_html_parts.append(\"<h4>Matched Skills:</h4><ul>\")\n",
        "                    for skill in matched_skills:\n",
        "                        report_html_parts.append(f\"<li>{skill}</li>\")\n",
        "                    report_html_parts.append(\"</ul>\")\n",
        "\n",
        "                missing_skills = parsed_report.get('missing_skills', [])\n",
        "                if missing_skills:\n",
        "                    report_html_parts.append(\"<h4 style=\\\"color:red;\\\">Missing Skills (Gaps):</h4><ul>\")\n",
        "                    for skill in missing_skills:\n",
        "                        report_html_parts.append(f\"<li style=\\\"color:red;\\\">{skill}</li>\")\n",
        "                    report_html_parts.append(\"</ul>\")\n",
        "\n",
        "                additional_skills = parsed_report.get('additional_skills', [])\n",
        "                if additional_skills:\n",
        "                    report_html_parts.append(\"<h4>Additional Skills:</h4><ul>\")\n",
        "                    for skill in additional_skills:\n",
        "                        report_html_parts.append(f\"<li>{skill}</li>\")\n",
        "                    report_html_parts.append(\"</ul>\")\n",
        "\n",
        "                yield \"\\n\".join(report_html_parts)\n",
        "\n",
        "            else:\n",
        "                yield f\"‚ùå Analysis failed: {analysis_result.get('message', 'Unknown error.')}\"\n",
        "                if 'raw_report' in analysis_result:\n",
        "                    yield f\"Raw LLM output: {analysis_result['raw_report']}\"\n",
        "        except Exception as e:\n",
        "            yield f\"‚ùå Error during analysis: {e}\"\n",
        "            return\n",
        "\n",
        "# Instantiate the CoordinatorAgent with the new class definition and updated sub-agents\n",
        "root_agent = CoordinatorAgent(\n",
        "    name=\"root_agent\",\n",
        "    instruction=\"I orchestrate the resume and job description analysis process.\",\n",
        "    sub_agents=[candidate_agent]\n",
        ")\n",
        "print(\"Backend AI logic initialized successfully.\")\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to load GOOGLE_API_KEY from .env file or environment variables.\n",
            "ERROR: GOOGLE_API_KEY not found. Please set it as an environment variable, in your .env file, or in Colab secrets.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "GOOGLE_API_KEY not found. Please set it as an environment variable, in your .env file, or in Colab secrets.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4180293683.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0merror_message\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"GOOGLE_API_KEY not found. Please set it as an environment variable, in your .env file, or in Colab secrets.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"ERROR: {error_message}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m# Configure the Gemini client\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: GOOGLE_API_KEY not found. Please set it as an environment variable, in your .env file, or in Colab secrets."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e1b00b0"
      },
      "source": [
        "## Simulate Backend API Call and Test\n",
        "\n",
        "### Subtask:\n",
        "Simulate an API call to the refactored backend logic to demonstrate its functionality.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f95b5382"
      },
      "source": [
        "**Reasoning**:\n",
        "To simulate the backend API call, I need to define dummy resume and job description texts, then call the `root_agent.run_live` method asynchronously to collect its output into a list.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "cb64c08e",
        "outputId": "f0cc61f0-8717-4485-cf77-6004016030a6"
      },
      "source": [
        "dummy_resume_text = '''\n",
        "John Doe\n",
        "123 Main Street, Anytown USA\n",
        "(123) 456-7890 | john.doe@email.com\n",
        "\n",
        "Summary\n",
        "A highly motivated and detail-oriented Software Engineer with 5 years of experience in developing and deploying scalable web applications. Proficient in Python, Java, and SQL, with a strong background in Machine Learning and data analysis. Seeking to leverage technical skills and problem-solving abilities to contribute to innovative projects.\n",
        "\n",
        "Skills\n",
        "Programming Languages: Python (Proficient), Java (Intermediate), JavaScript (Intermediate), C++ (Basic)\n",
        "Web Technologies: Flask, Django, React, Node.js\n",
        "Databases: SQL (PostgreSQL, MySQL), MongoDB\n",
        "Cloud Platforms: AWS (EC2, S3, Lambda), Google Cloud Platform (Compute Engine, Cloud Storage)\n",
        "Tools & Others: Git, Docker, Kubernetes, JIRA, Agile Methodologies, Machine Learning (Scikit-learn, TensorFlow, Keras), Data Analysis (Pandas, NumPy)\n",
        "\n",
        "Experience\n",
        "Software Engineer | Tech Solutions Inc. | Anytown, USA | 2020 - Present\n",
        "- Developed and maintained high-performance web applications using Python/Django.\n",
        "- Implemented RESTful APIs for various internal and external services.\n",
        "- Collaborated with product teams to gather requirements and define project scope.\n",
        "- Optimized database queries, reducing response times by 30%.\n",
        "- Mentored junior developers and conducted code reviews.\n",
        "\n",
        "Junior Developer | Web Innovations LLC. | Anytown, USA | 2018 - 2020\n",
        "- Assisted in the development of front-end components using React.js.\n",
        "- Wrote unit and integration tests to ensure code quality.\n",
        "- Participated in daily stand-ups and sprint planning meetings.\n",
        "- Contributed to database design and schema migrations.\n",
        "\n",
        "Education\n",
        "B.S. in Computer Science | University of Anytown | 2014 - 2018\n",
        "- Graduated Cum Laude\n",
        "- Relevant Coursework: Data Structures, Algorithms, Database Management, Artificial Intelligence\n",
        "\n",
        "Certifications\n",
        "AWS Certified Solutions Architect ‚Äì Associate\n",
        "'''\n",
        "\n",
        "dummy_job_description_text = '''\n",
        "Job Title: Data Analyst\n",
        "Company: Data Insights Corp.\n",
        "Location: Anytown, USA\n",
        "\n",
        "Job Description\n",
        "Data Insights Corp. is seeking a talented Data Analyst to join our growing team. The ideal candidate will be responsible for collecting, analyzing, and interpreting large datasets to identify trends and insights. This role requires strong analytical skills, proficiency in SQL for data extraction and manipulation, and excellent communication skills to present findings to stakeholders.\n",
        "\n",
        "Responsibilities:\n",
        "- Perform data extraction, transformation, and loading (ETL) from various sources.\n",
        "- Develop and maintain databases, data systems, and data pipelines.\n",
        "- Analyze complex datasets to provide actionable insights and recommendations.\n",
        "- Create visualizations and reports to communicate findings effectively.\n",
        "- Collaborate with cross-functional teams to understand business requirements.\n",
        "- Ensure data quality and integrity across all data initiatives.\n",
        "\n",
        "Qualifications:\n",
        "- Bachelor's degree in Computer Science, Statistics, Mathematics, or a related field.\n",
        "- 3+ years of experience as a Data Analyst or similar role.\n",
        "- Proven proficiency in SQL and experience with relational databases.\n",
        "- Strong analytical and problem-solving skills.\n",
        "- Experience with data visualization tools (e.g., Tableau, Power BI).\n",
        "- Excellent communication and presentation skills.\n",
        "- Familiarity with Python or R for data analysis is a plus.\n",
        "- Experience with cloud data platforms (e.g., Google BigQuery, AWS Redshift) is a plus.\n",
        "\n",
        "Benefits:\n",
        "- Competitive salary and benefits package.\n",
        "- Opportunities for professional growth and development.\n",
        "- Collaborative and innovative work environment.\n",
        "'''\n",
        "\n",
        "final_report = []\n",
        "\n",
        "# Using asyncio.run to execute the async generator directly\n",
        "# This is suitable for demonstrating in a synchronous context like a Colab cell\n",
        "for step_output in asyncio.run(root_agent.run_live(dummy_resume_text, dummy_job_description_text)):\n",
        "    final_report.append(step_output)\n",
        "\n",
        "print(final_report)\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'asyncio' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4143748677.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;31m# Using asyncio.run to execute the async generator directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;31m# This is suitable for demonstrating in a synchronous context like a Colab cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mstep_output\u001b[0m \u001b[0;32min\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_live\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdummy_resume_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummy_job_description_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m     \u001b[0mfinal_report\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'asyncio' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b476f34e"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed with a `NameError` because `asyncio` was not imported in the current cell's execution context. I need to add `import asyncio` at the beginning of the cell to resolve this.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "b56cd22f",
        "outputId": "3b1fd06a-ccd0-4f13-fcde-a47460e876af"
      },
      "source": [
        "import asyncio\n",
        "\n",
        "dummy_resume_text = '''\n",
        "John Doe\n",
        "123 Main Street, Anytown USA\n",
        "(123) 456-7890 | john.doe@email.com\n",
        "\n",
        "Summary\n",
        "A highly motivated and detail-oriented Software Engineer with 5 years of experience in developing and deploying scalable web applications. Proficient in Python, Java, and SQL, with a strong background in Machine Learning and data analysis. Seeking to leverage technical skills and problem-solving abilities to contribute to innovative projects.\n",
        "\n",
        "Skills\n",
        "Programming Languages: Python (Proficient), Java (Intermediate), JavaScript (Intermediate), C++ (Basic)\n",
        "Web Technologies: Flask, Django, React, Node.js\n",
        "Databases: SQL (PostgreSQL, MySQL), MongoDB\n",
        "Cloud Platforms: AWS (EC2, S3, Lambda), Google Cloud Platform (Compute Engine, Cloud Storage)\n",
        "Tools & Others: Git, Docker, Kubernetes, JIRA, Agile Methodologies, Machine Learning (Scikit-learn, TensorFlow, Keras), Data Analysis (Pandas, NumPy)\n",
        "\n",
        "Experience\n",
        "Software Engineer | Tech Solutions Inc. | Anytown, USA | 2020 - Present\n",
        "- Developed and maintained high-performance web applications using Python/Django.\n",
        "- Implemented RESTful APIs for various internal and external services.\n",
        "- Collaborated with product teams to gather requirements and define project scope.\n",
        "- Optimized database queries, reducing response times by 30%.\n",
        "- Mentored junior developers and conducted code reviews.\n",
        "\n",
        "Junior Developer | Web Innovations LLC. | Anytown, USA | 2018 - 2020\n",
        "- Assisted in the development of front-end components using React.js.\n",
        "- Wrote unit and integration tests to ensure code quality.\n",
        "- Participated in daily stand-ups and sprint planning meetings.\n",
        "- Contributed to database design and schema migrations.\n",
        "\n",
        "Education\n",
        "B.S. in Computer Science | University of Anytown | 2014 - 2018\n",
        "- Graduated Cum Laude\n",
        "- Relevant Coursework: Data Structures, Algorithms, Database Management, Artificial Intelligence\n",
        "\n",
        "Certifications\n",
        "AWS Certified Solutions Architect ‚Äì Associate\n",
        "'''\n",
        "\n",
        "dummy_job_description_text = '''\n",
        "Job Title: Data Analyst\n",
        "Company: Data Insights Corp.\n",
        "Location: Anytown, USA\n",
        "\n",
        "Job Description\n",
        "Data Insights Corp. is seeking a talented Data Analyst to join our growing team. The ideal candidate will be responsible for collecting, analyzing, and interpreting large datasets to identify trends and insights. This role requires strong analytical skills, proficiency in SQL for data extraction and manipulation, and excellent communication skills to present findings to stakeholders.\n",
        "\n",
        "Responsibilities:\n",
        "- Perform data extraction, transformation, and loading (ETL) from various sources.\n",
        "- Develop and maintain databases, data systems, and data pipelines.\n",
        "- Analyze complex datasets to provide actionable insights and recommendations.\n",
        "- Create visualizations and reports to communicate findings effectively.\n",
        "- Collaborate with cross-functional teams to understand business requirements.\n",
        "- Ensure data quality and integrity across all data initiatives.\n",
        "\n",
        "Qualifications:\n",
        "- Bachelor's degree in Computer Science, Statistics, Mathematics, or a related field.\n",
        "- 3+ years of experience as a Data Analyst or similar role.\n",
        "- Proven proficiency in SQL and experience with relational databases.\n",
        "- Strong analytical and problem-solving skills.\n",
        "- Experience with data visualization tools (e.g., Tableau, Power BI).\n",
        "- Excellent communication and presentation skills.\n",
        "- Familiarity with Python or R for data analysis is a plus.\n",
        "- Experience with cloud data platforms (e.g., Google BigQuery, AWS Redshift) is a plus.\n",
        "\n",
        "Benefits:\n",
        "- Competitive salary and benefits package.\n",
        "- Opportunities for professional growth and development.\n",
        "- Collaborative and innovative work environment.\n",
        "'''\n",
        "\n",
        "final_report = []\n",
        "\n",
        "# Using asyncio.run to execute the async generator directly\n",
        "# This is suitable for demonstrating in a synchronous context like a Colab cell\n",
        "for step_output in asyncio.run(root_agent.run_live(dummy_resume_text, dummy_job_description_text)):\n",
        "    final_report.append(step_output)\n",
        "\n",
        "print(final_report)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'root_agent' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1632117668.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;31m# Using asyncio.run to execute the async generator directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;31m# This is suitable for demonstrating in a synchronous context like a Colab cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mstep_output\u001b[0m \u001b[0;32min\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_live\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdummy_resume_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummy_job_description_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m     \u001b[0mfinal_report\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'root_agent' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2c1b43be"
      },
      "source": [
        "# Task\n",
        "Create a Streamlit frontend application code that interacts with a hypothetical deployed backend API, and provide a summary of the separated code structure with deployment instructions for both components on Google Cloud Run, acknowledging the limitations of interactive testing in Colab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d551824"
      },
      "source": [
        "## Create Frontend Streamlit Application Code\n",
        "\n",
        "### Subtask:\n",
        "Generate a new code cell containing the Streamlit UI code, modified to interact with a hypothetical deployed backend API.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffaf197c"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask is to generate a new code cell containing the Streamlit UI code, modified to interact with a hypothetical deployed backend API. This code block will set up the Streamlit frontend, including UI elements for inputting a job description URL and uploading a resume, along with the logic to extract text, send it to a placeholder backend API, and display the analysis results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bdae668",
        "outputId": "db9de24e-78d5-4468-d283-e7e39d0b2a9d"
      },
      "source": [
        "import streamlit as st\n",
        "import os\n",
        "import io\n",
        "import PyPDF2\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "\n",
        "# --- Helper function for URL extraction (copied from backend) ---\n",
        "def extract_text_from_url(url: str) -> str:\n",
        "    \"\"\"Extracts text content from a given URL, typically for a job description.\"\"\"\n",
        "    try:\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "        }\n",
        "        response = requests.get(url, headers=headers, timeout=30)\n",
        "        response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Remove script and style elements\n",
        "        for script_or_style in soup(['script', 'style']):\n",
        "            script_or_style.extract()\n",
        "\n",
        "        # Get text and clean it\n",
        "        text = soup.get_text()\n",
        "        lines = (line.strip() for line in text.splitlines())\n",
        "        # Break multi-headlines into a line each\n",
        "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
        "        # Drop blank lines\n",
        "        text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
        "        return text\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        st.error(f\"Error fetching URL {url}: {e}\")\n",
        "        return \"\"\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error processing URL {url}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "# --- Streamlit UI and Workflow Integration ---\n",
        "\n",
        "st.set_page_config(\n",
        "    page_title=\"AI-Powered Resume and Job Description Analyzer (Frontend)\",\n",
        "    layout=\"wide\",\n",
        "    initial_sidebar_state=\"expanded\"\n",
        ")\n",
        "st.markdown(\"<h1 style='text-align: center; color: #4CAF50;'> üîç AI Job Search Assistant (Frontend) </h1> <p style='text-align:center; font-size:18px;'> Discover tailored job recommendations powered by Agentic AI. </p>\", unsafe_allow_html=True)\n",
        "st.sidebar.header(\"User Inputs\")\n",
        "\n",
        "# Hypothetical backend API endpoint\n",
        "# In a real deployment, this URL would point to your deployed Google Cloud Run service.\n",
        "BACKEND_API_URL = os.getenv(\"BACKEND_API_URL\", \"http://localhost:8000/analyze\") # Placeholder URL for local testing or environment variable\n",
        "\n",
        "st.sidebar.markdown(f\"*(Backend API URL: `{BACKEND_API_URL}`)*\")\n",
        "st.sidebar.markdown(\"---\")\n",
        "\n",
        "\n",
        "job_url_input = st.sidebar.text_input(\n",
        "    \"Job Description URL\",\n",
        "    value=\"https://example.com/job_description\",\n",
        "    help=\"Enter the URL of the job description webpage.\"\n",
        ")\n",
        "\n",
        "uploaded_resume_file = st.sidebar.file_uploader(\n",
        "    \"Upload Your Resume (PDF)\",\n",
        "    type=[\"pdf\"],\n",
        "    help=\"Upload your resume in PDF format.\"\n",
        ")\n",
        "\n",
        "is_valid_job_url = False\n",
        "if job_url_input:\n",
        "    if job_url_input.startswith(\"http://\") or job_url_input.startswith(\"https://\"):\n",
        "        is_valid_job_url = True\n",
        "    else:\n",
        "        st.sidebar.error(\"Please enter a valid URL (starting with http:// or https://).\")\n",
        "\n",
        "is_resume_uploaded = False\n",
        "if uploaded_resume_file is not None:\n",
        "    is_resume_uploaded = True\n",
        "\n",
        "if st.sidebar.button(\"Run Analysis\", disabled=(not is_valid_job_url or not is_resume_uploaded)):\n",
        "    if is_valid_job_url and is_resume_uploaded:\n",
        "        with st.spinner(\"Processing resume and fetching job description...\"):\n",
        "            resume_text = \"\"\n",
        "            try:\n",
        "                pdf_reader = PyPDF2.PdfReader(io.BytesIO(uploaded_resume_file.getvalue()))\n",
        "                resume_text = \"\".join([page.extract_text() for page in pdf_reader.pages])\n",
        "                st.success(\"Resume extracted successfully.\")\n",
        "            except Exception as e:\n",
        "                st.error(f\"Error reading resume PDF: {e}\")\n",
        "                resume_text = \"\"\n",
        "\n",
        "            job_description_text = \"\"\n",
        "            try:\n",
        "                job_description_text = extract_text_from_url(job_url_input)\n",
        "                if job_description_text:\n",
        "                    st.success(\"Job description fetched successfully.\")\n",
        "                else:\n",
        "                    st.error(\"Failed to fetch job description. Please check the URL.\")\n",
        "            except Exception as e:\n",
        "                st.error(f\"Error fetching job description from URL: {e}\")\n",
        "                job_description_text = \"\"\n",
        "\n",
        "        if resume_text and job_description_text:\n",
        "            st.subheader(\"AI Analysis Report\")\n",
        "            progress_bar = st.progress(0)\n",
        "            status_text = st.empty()\n",
        "            report_container = st.empty()\n",
        "\n",
        "            status_text.text(\"Sending data to backend for analysis...\")\n",
        "            progress_bar.progress(25)\n",
        "\n",
        "            try:\n",
        "                payload = {\n",
        "                    \"resume_text\": resume_text,\n",
        "                    \"job_description_text\": job_description_text\n",
        "                }\n",
        "                headers = {\"Content-Type\": \"application/json\"}\n",
        "\n",
        "                # Make a POST request to the backend API\n",
        "                response = requests.post(BACKEND_API_URL, json=payload, headers=headers, timeout=120) # Increased timeout for LLM calls\n",
        "                response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)\n",
        "\n",
        "                analysis_result = response.json()\n",
        "\n",
        "                if analysis_result.get('analysis_status') == 'success':\n",
        "                    status_text.text(\"‚úÖ Analysis complete: Received report from backend.\")\n",
        "                    progress_bar.progress(100)\n",
        "\n",
        "                    parsed_report = analysis_result.get('parsed_report', {})\n",
        "                    report_html_parts = []\n",
        "\n",
        "                    report_html_parts.append(\"<hr/>\")\n",
        "                    # Overall Fit Summary\n",
        "                    overall_fit_summary = parsed_report.get('overall_fit_summary', 'N/A')\n",
        "                    report_html_parts.append(f\"<p><b>Overall Fit Summary:</b> {overall_fit_summary}</p>\")\n",
        "                    report_html_parts.append(\"<hr/>\")\n",
        "\n",
        "                    # Candidate Skills\n",
        "                    candidate_skills = parsed_report.get('candidate_skills', [])\n",
        "                    if candidate_skills:\n",
        "                        report_html_parts.append(\"<h4>Candidate Skills:</h4><ul>\")\n",
        "                        for skill in candidate_skills:\n",
        "                            report_html_parts.append(f\"<li>{skill}</li>\")\n",
        "                        report_html_parts.append(\"</ul>\")\n",
        "\n",
        "                    # Required Job Skills\n",
        "                    required_job_skills = parsed_report.get('required_job_skills', [])\n",
        "                    if required_job_skills:\n",
        "                        report_html_parts.append(\"<h4>Required Job Skills:</h4><ul>\")\n",
        "                        for skill in required_job_skills:\n",
        "                            report_html_parts.append(f\"<li>{skill}</li>\")\n",
        "                        report_html_parts.append(\"</ul>\")\n",
        "\n",
        "                    # Matched Skills\n",
        "                    matched_skills = parsed_report.get('matched_skills', [])\n",
        "                    if matched_skills:\n",
        "                        report_html_parts.append(\"<h4>Matched Skills:</h4><ul>\")\n",
        "                        for skill in matched_skills:\n",
        "                            report_html_parts.append(f\"<li>{skill}</li>\")\n",
        "                        report_html_parts.append(\"</ul>\")\n",
        "\n",
        "                    # Missing Skills\n",
        "                    missing_skills = parsed_report.get('missing_skills', [])\n",
        "                    if missing_skills:\n",
        "                        report_html_parts.append(\"<h4 style=\\\"color:red;\\\">Missing Skills (Gaps):</h4><ul>\")\n",
        "                        for skill in missing_skills:\n",
        "                            report_html_parts.append(f\"<li style=\\\"color:red;\\\">{skill}</li>\")\n",
        "                        report_html_parts.append(\"</ul>\")\n",
        "\n",
        "                    # Additional Skills\n",
        "                    additional_skills = parsed_report.get('additional_skills', [])\n",
        "                    if additional_skills:\n",
        "                        report_html_parts.append(\"<h4>Additional Skills:</h4><ul>\")\n",
        "                        for skill in additional_skills:\n",
        "                            report_html_parts.append(f\"<li>{skill}</li>\")\n",
        "                        report_html_parts.append(\"</ul>\")\n",
        "\n",
        "                    report_container.markdown(\"\\n\".join(report_html_parts), unsafe_allow_html=True)\n",
        "\n",
        "                else:\n",
        "                    status_text.text(f\"‚ùå Analysis failed: {analysis_result.get('message', 'Unknown error from backend.')}\")\n",
        "                    st.error(f\"Backend analysis failed: {analysis_result.get('message', 'Unknown error.')}\")\n",
        "                    if 'raw_report' in analysis_result:\n",
        "                        st.json(analysis_result['raw_report']) # Display raw report if available for debugging\n",
        "            except requests.exceptions.ConnectionError as ce:\n",
        "                st.error(f\"‚ùå Connection Error: Could not connect to the backend API at {BACKEND_API_URL}. Please ensure the backend is running and accessible. Details: {ce}\")\n",
        "            except requests.exceptions.Timeout as te:\n",
        "                st.error(f\"‚ùå Request Timeout: The backend API at {BACKEND_API_URL} took too long to respond. Details: {te}\")\n",
        "            except requests.exceptions.RequestException as re:\n",
        "                st.error(f\"‚ùå Error during API call to backend: {re}\")\n",
        "                if hasattr(re, 'response') and re.response is not None:\n",
        "                    try:\n",
        "                        st.error(f\"Backend Response: {re.response.json()}\")\n",
        "                    except json.JSONDecodeError:\n",
        "                        st.error(f\"Backend Response (raw): {re.response.text}\")\n",
        "            except Exception as e:\n",
        "                st.error(f\"‚ùå An unexpected error occurred during backend communication: {e}\")\n",
        "            finally:\n",
        "                progress_bar.empty()\n",
        "                status_text.empty()\n",
        "\n",
        "        else:\n",
        "            st.error(\"Analysis cannot be performed due to missing resume text or job description text.\")\n",
        "    else:\n",
        "        st.error(\"Please fix the input errors before running analysis.\")\n",
        "else:\n",
        "    if not is_valid_job_url or not is_resume_uploaded:\n",
        "        st.warning(\"Please provide a valid Job URL and upload your resume to proceed.\")\n",
        "    else:\n",
        "        st.success(\"Job URL and Resume uploaded successfully. Ready for analysis!\")\n",
        "\n",
        "st.markdown(\"\"\"\n",
        "---\n",
        "**Note for Colab Users:**\n",
        "This Streamlit application is designed to run as a separate frontend, interacting with a *deployed* backend API.\n",
        "It cannot be run interactively within this Colab notebook due to how Streamlit serves applications and Colab's environment limitations.\n",
        "To test this frontend, you would typically:\n",
        "1. Save this code as `frontend_app.py`.\n",
        "2. Deploy your backend AI logic (e.g., as a FastAPI service on Google Cloud Run).\n",
        "3. Set the `BACKEND_API_URL` environment variable (or hardcode it for testing) to your deployed backend's URL.\n",
        "4. Run `streamlit run frontend_app.py` in your local environment or deploy it to a service like Streamlit Community Cloud or Google Cloud Run.\n",
        "\"\"\", unsafe_allow_html=True)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-21 16:13:40.857 \n",
            "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
            "  command:\n",
            "\n",
            "    streamlit run /usr/local/lib/python3.12/dist-packages/colab_kernel_launcher.py [ARGUMENTS]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeltaGenerator()"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a3e4e0b"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code block for the Streamlit frontend was generated correctly, but its output (`stderr` and `stdout`) indicates how Streamlit behaves when run in a non-interactive environment like Colab, not an error in the code itself. The instructions request to generate the code cell again as a new step. This regenerated code cell will set up the Streamlit frontend to interact with a hypothetical deployed backend API.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0f00251d",
        "outputId": "21bad7d1-e738-49ec-e38c-dc702947474f"
      },
      "source": [
        "import streamlit as st\n",
        "import os\n",
        "import io\n",
        "import PyPDF2\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "\n",
        "# --- Helper function for URL extraction (copied from backend) ---\n",
        "def extract_text_from_url(url: str) -> str:\n",
        "    \"\"\"Extracts text content from a given URL, typically for a job description.\"\"\"\n",
        "    try:\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "        }\n",
        "        response = requests.get(url, headers=headers, timeout=30)\n",
        "        response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Remove script and style elements\n",
        "        for script_or_style in soup(['script', 'style']):\n",
        "            script_or_style.extract()\n",
        "\n",
        "        # Get text and clean it\n",
        "        text = soup.get_text()\n",
        "        lines = (line.strip() for line in text.splitlines())\n",
        "        # Break multi-headlines into a line each\n",
        "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
        "        # Drop blank lines\n",
        "        text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
        "        return text\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        st.error(f\"Error fetching URL {url}: {e}\")\n",
        "        return \"\"\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error processing URL {url}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "# --- Streamlit UI and Workflow Integration ---\n",
        "\n",
        "st.set_page_config(\n",
        "    page_title=\"AI-Powered Resume and Job Description Analyzer (Frontend)\",\n",
        "    layout=\"wide\",\n",
        "    initial_sidebar_state=\"expanded\"\n",
        ")\n",
        "st.markdown(\"<h1 style='text-align: center; color: #4CAF50;'> üîç AI Job Search Assistant (Frontend) </h1> <p style='text-align:center; font-size:18px;'> Discover tailored job recommendations powered by Agentic AI. </p>\", unsafe_allow_html=True)\n",
        "st.sidebar.header(\"User Inputs\")\n",
        "\n",
        "# Hypothetical backend API endpoint\n",
        "# In a real deployment, this URL would point to your deployed Google Cloud Run service.\n",
        "BACKEND_API_URL = os.getenv(\"BACKEND_API_URL\", \"http://localhost:8000/analyze\") # Placeholder URL for local testing or environment variable\n",
        "\n",
        "st.sidebar.markdown(f\"*(Backend API URL: `{BACKEND_API_URL}`)*\")\n",
        "st.sidebar.markdown(\"---\")\n",
        "\n",
        "\n",
        "job_url_input = st.sidebar.text_input(\n",
        "    \"Job Description URL\",\n",
        "    value=\"https://example.com/job_description\",\n",
        "    help=\"Enter the URL of the job description webpage.\"\n",
        ")\n",
        "\n",
        "uploaded_resume_file = st.sidebar.file_uploader(\n",
        "    \"Upload Your Resume (PDF)\",\n",
        "    type=[\"pdf\"],\n",
        "    help=\"Upload your resume in PDF format.\"\n",
        ")\n",
        "\n",
        "is_valid_job_url = False\n",
        "if job_url_input:\n",
        "    if job_url_input.startswith(\"http://\") or job_url_input.startswith(\"https://\"):\n",
        "        is_valid_job_url = True\n",
        "    else:\n",
        "        st.sidebar.error(\"Please enter a valid URL (starting with http:// or https://).\")\n",
        "\n",
        "is_resume_uploaded = False\n",
        "if uploaded_resume_file is not None:\n",
        "    is_resume_uploaded = True\n",
        "\n",
        "if st.sidebar.button(\"Run Analysis\", disabled=(not is_valid_job_url or not is_resume_uploaded)):\n",
        "    if is_valid_job_url and is_resume_uploaded:\n",
        "        with st.spinner(\"Processing resume and fetching job description...\"):\n",
        "            resume_text = \"\"\n",
        "            try:\n",
        "                pdf_reader = PyPDF2.PdfReader(io.BytesIO(uploaded_resume_file.getvalue()))\n",
        "                resume_text = \"\".join([page.extract_text() for page in pdf_reader.pages])\n",
        "                st.success(\"Resume extracted successfully.\")\n",
        "            except Exception as e:\n",
        "                st.error(f\"Error reading resume PDF: {e}\")\n",
        "                resume_text = \"\"\n",
        "\n",
        "            job_description_text = \"\"\n",
        "            try:\n",
        "                job_description_text = extract_text_from_url(job_url_input)\n",
        "                if job_description_text:\n",
        "                    st.success(\"Job description fetched successfully.\")\n",
        "                else:\n",
        "                    st.error(\"Failed to fetch job description. Please check the URL.\")\n",
        "            except Exception as e:\n",
        "                st.error(f\"Error fetching job description from URL: {e}\")\n",
        "                job_description_text = \"\"\n",
        "\n",
        "        if resume_text and job_description_text:\n",
        "            st.subheader(\"AI Analysis Report\")\n",
        "            progress_bar = st.progress(0)\n",
        "            status_text = st.empty()\n",
        "            report_container = st.empty()\n",
        "\n",
        "            status_text.text(\"Sending data to backend for analysis...\")\n",
        "            progress_bar.progress(25)\n",
        "\n",
        "            try:\n",
        "                payload = {\n",
        "                    \"resume_text\": resume_text,\n",
        "                    \"job_description_text\": job_description_text\n",
        "                }\n",
        "                headers = {\"Content-Type\": \"application/json\"}\n",
        "\n",
        "                # Make a POST request to the backend API\n",
        "                response = requests.post(BACKEND_API_URL, json=payload, headers=headers, timeout=120) # Increased timeout for LLM calls\n",
        "                response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)\n",
        "\n",
        "                analysis_result = response.json()\n",
        "\n",
        "                if analysis_result.get('analysis_status') == 'success':\n",
        "                    status_text.text(\"‚úÖ Analysis complete: Received report from backend.\")\n",
        "                    progress_bar.progress(100)\n",
        "\n",
        "                    parsed_report = analysis_result.get('parsed_report', {})\n",
        "                    report_html_parts = []\n",
        "\n",
        "                    report_html_parts.append(\"<hr/>\")\n",
        "                    # Overall Fit Summary\n",
        "                    overall_fit_summary = parsed_report.get('overall_fit_summary', 'N/A')\n",
        "                    report_html_parts.append(f\"<p><b>Overall Fit Summary:</b> {overall_fit_summary}</p>\")\n",
        "                    report_html_parts.append(\"<hr/>\")\n",
        "\n",
        "                    # Candidate Skills\n",
        "                    candidate_skills = parsed_report.get('candidate_skills', [])\n",
        "                    if candidate_skills:\n",
        "                        report_html_parts.append(\"<h4>Candidate Skills:</h4><ul>\")\n",
        "                        for skill in candidate_skills:\n",
        "                            report_html_parts.append(f\"<li>{skill}</li>\")\n",
        "                        report_html_parts.append(\"</ul>\")\n",
        "\n",
        "                    # Required Job Skills\n",
        "                    required_job_skills = parsed_report.get('required_job_skills', [])\n",
        "                    if required_job_skills:\n",
        "                        report_html_parts.append(\"<h4>Required Job Skills:</h4><ul>\")\n",
        "                        for skill in required_job_skills:\n",
        "                            report_html_parts.append(f\"<li>{skill}</li>\")\n",
        "                        report_html_parts.append(\"</ul>\")\n",
        "\n",
        "                    # Matched Skills\n",
        "                    matched_skills = parsed_report.get('matched_skills', [])\n",
        "                    if matched_skills:\n",
        "                        report_html_parts.append(\"<h4>Matched Skills:</h4><ul>\")\n",
        "                        for skill in matched_skills:\n",
        "                            report_html_parts.append(f\"<li>{skill}</li>\")\n",
        "                        report_html_parts.append(\"</ul>\")\n",
        "\n",
        "                    # Missing Skills\n",
        "                    missing_skills = parsed_report.get('missing_skills', [])\n",
        "                    if missing_skills:\n",
        "                        report_html_parts.append(\"<h4 style=\\\"color:red;\\\">Missing Skills (Gaps):</h4><ul>\")\n",
        "                        for skill in missing_skills:\n",
        "                            report_html_parts.append(f\"<li style=\\\"color:red;\\\">{skill}</li>\")\n",
        "                        report_html_parts.append(\"</ul>\")\n",
        "\n",
        "                    # Additional Skills\n",
        "                    additional_skills = parsed_report.get('additional_skills', [])\n",
        "                    if additional_skills:\n",
        "                        report_html_parts.append(\"<h4>Additional Skills:</h4><ul>\")\n",
        "                        for skill in additional_skills:\n",
        "                            report_html_parts.append(f\"<li>{skill}</li>\")\n",
        "                        report_html_parts.append(\"</ul>\")\n",
        "\n",
        "                    report_container.markdown(\"\\n\".join(report_html_parts), unsafe_allow_html=True)\n",
        "\n",
        "                else:\n",
        "                    status_text.text(f\"‚ùå Analysis failed: {analysis_result.get('message', 'Unknown error from backend.')}\")\n",
        "                    st.error(f\"Backend analysis failed: {analysis_result.get('message', 'Unknown error.')}\")\n",
        "                    if 'raw_report' in analysis_result:\n",
        "                        st.json(analysis_result['raw_report']) # Display raw report if available for debugging\n",
        "            except requests.exceptions.ConnectionError as ce:\n",
        "                st.error(f\"‚ùå Connection Error: Could not connect to the backend API at {BACKEND_API_URL}. Please ensure the backend is running and accessible. Details: {ce}\")\n",
        "            except requests.exceptions.Timeout as te:\n",
        "                st.error(f\"‚ùå Request Timeout: The backend API at {BACKEND_API_URL} took too long to respond. Details: {te}\")\n",
        "            except requests.exceptions.RequestException as re:\n",
        "                st.error(f\"‚ùå Error during API call to backend: {re}\")\n",
        "                if hasattr(re, 'response') and re.response is not None:\n",
        "                    try:\n",
        "                        st.error(f\"Backend Response: {re.response.json()}\")\n",
        "                    except json.JSONDecodeError:\n",
        "                        st.error(f\"Backend Response (raw): {re.response.text}\")\n",
        "            except Exception as e:\n",
        "                st.error(f\"‚ùå An unexpected error occurred during backend communication: {e}\")\n",
        "            finally:\n",
        "                progress_bar.empty()\n",
        "                status_text.empty()\n",
        "\n",
        "        else:\n",
        "            st.error(\"Analysis cannot be performed due to missing resume text or job description text.\")\n",
        "    else:\n",
        "        st.error(\"Please fix the input errors before running analysis.\")\n",
        "else:\n",
        "    if not is_valid_job_url or not is_resume_uploaded:\n",
        "        st.warning(\"Please provide a valid Job URL and upload your resume to proceed.\")\n",
        "    else:\n",
        "        st.success(\"Job URL and Resume uploaded successfully. Ready for analysis!\")\n",
        "\n",
        "st.markdown(\"\"\"\n",
        "---\n",
        "**Note for Colab Users:**\n",
        "This Streamlit application is designed to run as a separate frontend, interacting with a *deployed* backend API.\n",
        "It cannot be run interactively within this Colab notebook due to how Streamlit serves applications and Colab's environment limitations.\n",
        "To test this frontend, you would typically:\n",
        "1. Save this code as `frontend_app.py`.\n",
        "2. Deploy your backend AI logic (e.g., as a FastAPI service on Google Cloud Run).\n",
        "3. Set the `BACKEND_API_URL` environment variable (or hardcode it for testing) to your deployed backend's URL.\n",
        "4. Run `streamlit run frontend_app.py` in your local environment or deploy it to a service like Streamlit Community Cloud or Google Cloud Run.\n",
        "\"\"\", unsafe_allow_html=True)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeltaGenerator()"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acc96224"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code block for the Streamlit frontend was generated correctly, but its output (`stderr` and `stdout`) indicates how Streamlit behaves when run in a non-interactive environment like Colab, not an error in the code itself. The instructions request to generate the code cell again as a new step. This regenerated code cell will set up the Streamlit frontend to interact with a hypothetical deployed backend API.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3e0a8cf3",
        "outputId": "2d3b6bf1-b8f1-483c-985e-0abd47760ceb"
      },
      "source": [
        "import streamlit as st\n",
        "import os\n",
        "import io\n",
        "import PyPDF2\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "\n",
        "# --- Helper function for URL extraction (copied from backend) ---\n",
        "def extract_text_from_url(url: str) -> str:\n",
        "    \"\"\"Extracts text content from a given URL, typically for a job description.\"\"\"\n",
        "    try:\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "        }\n",
        "        response = requests.get(url, headers=headers, timeout=30)\n",
        "        response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Remove script and style elements\n",
        "        for script_or_style in soup(['script', 'style']):\n",
        "            script_or_style.extract()\n",
        "\n",
        "        # Get text and clean it\n",
        "        text = soup.get_text()\n",
        "        lines = (line.strip() for line in text.splitlines())\n",
        "        # Break multi-headlines into a line each\n",
        "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
        "        # Drop blank lines\n",
        "        text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
        "        return text\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        st.error(f\"Error fetching URL {url}: {e}\")\n",
        "        return \"\"\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error processing URL {url}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "# --- Streamlit UI and Workflow Integration ---\n",
        "\n",
        "st.set_page_config(\n",
        "    page_title=\"AI-Powered Resume and Job Description Analyzer (Frontend)\",\n",
        "    layout=\"wide\",\n",
        "    initial_sidebar_state=\"expanded\"\n",
        ")\n",
        "st.markdown(\"<h1 style='text-align: center; color: #4CAF50;'> üîç AI Job Search Assistant (Frontend) </h1> <p style='text-align:center; font-size:18px;'> Discover tailored job recommendations powered by Agentic AI. </p>\", unsafe_allow_html=True)\n",
        "st.sidebar.header(\"User Inputs\")\n",
        "\n",
        "# Hypothetical backend API endpoint\n",
        "# In a real deployment, this URL would point to your deployed Google Cloud Run service.\n",
        "BACKEND_API_URL = os.getenv(\"BACKEND_API_URL\", \"http://localhost:8000/analyze\") # Placeholder URL for local testing or environment variable\n",
        "\n",
        "st.sidebar.markdown(f\"*(Backend API URL: `{BACKEND_API_URL}`)*\")\n",
        "st.sidebar.markdown(\"---\")\n",
        "\n",
        "\n",
        "job_url_input = st.sidebar.text_input(\n",
        "    \"Job Description URL\",\n",
        "    value=\"https://example.com/job_description\",\n",
        "    help=\"Enter the URL of the job description webpage.\"\n",
        ")\n",
        "\n",
        "uploaded_resume_file = st.sidebar.file_uploader(\n",
        "    \"Upload Your Resume (PDF)\",\n",
        "    type=[\"pdf\"],\n",
        "    help=\"Upload your resume in PDF format.\"\n",
        ")\n",
        "\n",
        "is_valid_job_url = False\n",
        "if job_url_input:\n",
        "    if job_url_input.startswith(\"http://\") or job_url_input.startswith(\"https://\"):\n",
        "        is_valid_job_url = True\n",
        "    else:\n",
        "        st.sidebar.error(\"Please enter a valid URL (starting with http:// or https://).\")\n",
        "\n",
        "is_resume_uploaded = False\n",
        "if uploaded_resume_file is not None:\n",
        "    is_resume_uploaded = True\n",
        "\n",
        "if st.sidebar.button(\"Run Analysis\", disabled=(not is_valid_job_url or not is_resume_uploaded)):\n",
        "    if is_valid_job_url and is_resume_uploaded:\n",
        "        with st.spinner(\"Processing resume and fetching job description...\"):\n",
        "            resume_text = \"\"\n",
        "            try:\n",
        "                pdf_reader = PyPDF2.PdfReader(io.BytesIO(uploaded_resume_file.getvalue()))\n",
        "                resume_text = \"\".join([page.extract_text() for page in pdf_reader.pages])\n",
        "                st.success(\"Resume extracted successfully.\")\n",
        "            except Exception as e:\n",
        "                st.error(f\"Error reading resume PDF: {e}\")\n",
        "                resume_text = \"\"\n",
        "\n",
        "            job_description_text = \"\"\n",
        "            try:\n",
        "                job_description_text = extract_text_from_url(job_url_input)\n",
        "                if job_description_text:\n",
        "                    st.success(\"Job description fetched successfully.\")\n",
        "                else:\n",
        "                    st.error(\"Failed to fetch job description. Please check the URL.\")\n",
        "            except Exception as e:\n",
        "                st.error(f\"Error fetching job description from URL: {e}\")\n",
        "                job_description_text = \"\"\n",
        "\n",
        "        if resume_text and job_description_text:\n",
        "            st.subheader(\"AI Analysis Report\")\n",
        "            progress_bar = st.progress(0)\n",
        "            status_text = st.empty()\n",
        "            report_container = st.empty()\n",
        "\n",
        "            status_text.text(\"Sending data to backend for analysis...\")\n",
        "            progress_bar.progress(25)\n",
        "\n",
        "            try:\n",
        "                payload = {\n",
        "                    \"resume_text\": resume_text,\n",
        "                    \"job_description_text\": job_description_text\n",
        "                }\n",
        "                headers = {\"Content-Type\": \"application/json\"}\n",
        "\n",
        "                # Make a POST request to the backend API\n",
        "                response = requests.post(BACKEND_API_URL, json=payload, headers=headers, timeout=120) # Increased timeout for LLM calls\n",
        "                response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)\n",
        "\n",
        "                analysis_result = response.json()\n",
        "\n",
        "                if analysis_result.get('analysis_status') == 'success':\n",
        "                    status_text.text(\"‚úÖ Analysis complete: Received report from backend.\")\n",
        "                    progress_bar.progress(100)\n",
        "\n",
        "                    parsed_report = analysis_result.get('parsed_report', {})\n",
        "                    report_html_parts = []\n",
        "\n",
        "                    report_html_parts.append(\"<hr/>\")\n",
        "                    # Overall Fit Summary\n",
        "                    overall_fit_summary = parsed_report.get('overall_fit_summary', 'N/A')\n",
        "                    report_html_parts.append(f\"<p><b>Overall Fit Summary:</b> {overall_fit_summary}</p>\")\n",
        "                    report_html_parts.append(\"<hr/>\")\n",
        "\n",
        "                    # Candidate Skills\n",
        "                    candidate_skills = parsed_report.get('candidate_skills', [])\n",
        "                    if candidate_skills:\n",
        "                        report_html_parts.append(\"<h4>Candidate Skills:</h4><ul>\")\n",
        "                        for skill in candidate_skills:\n",
        "                            report_html_parts.append(f\"<li>{skill}</li>\")\n",
        "                        report_html_parts.append(\"</ul>\")\n",
        "\n",
        "                    # Required Job Skills\n",
        "                    required_job_skills = parsed_report.get('required_job_skills', [])\n",
        "                    if required_job_skills:\n",
        "                        report_html_parts.append(\"<h4>Required Job Skills:</h4><ul>\")\n",
        "                        for skill in required_job_skills:\n",
        "                            report_html_parts.append(f\"<li>{skill}</li>\")\n",
        "                        report_html_parts.append(\"</ul>\")\n",
        "\n",
        "                    # Matched Skills\n",
        "                    matched_skills = parsed_report.get('matched_skills', [])\n",
        "                    if matched_skills:\n",
        "                        report_html_parts.append(\"<h4>Matched Skills:</h4><ul>\")\n",
        "                        for skill in matched_skills:\n",
        "                            report_html_parts.append(f\"<li>{skill}</li>\")\n",
        "                        report_html_parts.append(\"</ul>\")\n",
        "\n",
        "                    # Missing Skills\n",
        "                    missing_skills = parsed_report.get('missing_skills', [])\n",
        "                    if missing_skills:\n",
        "                        report_html_parts.append(\"<h4 style=\\\"color:red;\\\">Missing Skills (Gaps):</h4><ul>\")\n",
        "                        for skill in missing_skills:\n",
        "                            report_html_parts.append(f\"<li style=\\\"color:red;\\\">{skill}</li>\")\n",
        "                        report_html_parts.append(\"</ul>\")\n",
        "\n",
        "                    # Additional Skills\n",
        "                    additional_skills = parsed_report.get('additional_skills', [])\n",
        "                    if additional_skills:\n",
        "                        report_html_parts.append(\"<h4>Additional Skills:</h4><ul>\")\n",
        "                        for skill in additional_skills:\n",
        "                            report_html_parts.append(f\"<li>{skill}</li>\")\n",
        "                        report_html_parts.append(\"</ul>\")\n",
        "\n",
        "                    report_container.markdown(\"\\n\".join(report_html_parts), unsafe_allow_html=True)\n",
        "\n",
        "                else:\n",
        "                    status_text.text(f\"‚ùå Analysis failed: {analysis_result.get('message', 'Unknown error from backend.')}\")\n",
        "                    st.error(f\"Backend analysis failed: {analysis_result.get('message', 'Unknown error.')}\")\n",
        "                    if 'raw_report' in analysis_result:\n",
        "                        st.json(analysis_result['raw_report']) # Display raw report if available for debugging\n",
        "            except requests.exceptions.ConnectionError as ce:\n",
        "                st.error(f\"‚ùå Connection Error: Could not connect to the backend API at {BACKEND_API_URL}. Please ensure the backend is running and accessible. Details: {ce}\")\n",
        "            except requests.exceptions.Timeout as te:\n",
        "                st.error(f\"‚ùå Request Timeout: The backend API at {BACKEND_API_URL} took too long to respond. Details: {te}\")\n",
        "            except requests.exceptions.RequestException as re:\n",
        "                st.error(f\"‚ùå Error during API call to backend: {re}\")\n",
        "                if hasattr(re, 'response') and re.response is not None:\n",
        "                    try:\n",
        "                        st.error(f\"Backend Response: {re.response.json()}\")\n",
        "                    except json.JSONDecodeError:\n",
        "                        st.error(f\"Backend Response (raw): {re.response.text}\")\n",
        "            except Exception as e:\n",
        "                st.error(f\"‚ùå An unexpected error occurred during backend communication: {e}\")\n",
        "            finally:\n",
        "                progress_bar.empty()\n",
        "                status_text.empty()\n",
        "\n",
        "        else:\n",
        "            st.error(\"Analysis cannot be performed due to missing resume text or job description text.\")\n",
        "    else:\n",
        "        st.error(\"Please fix the input errors before running analysis.\")\n",
        "else:\n",
        "    if not is_valid_job_url or not is_resume_uploaded:\n",
        "        st.warning(\"Please provide a valid Job URL and upload your resume to proceed.\")\n",
        "    else:\n",
        "        st.success(\"Job URL and Resume uploaded successfully. Ready for analysis!\")\n",
        "\n",
        "st.markdown(\"\"\"\n",
        "---\n",
        "**Note for Colab Users:**\n",
        "This Streamlit application is designed to run as a separate frontend, interacting with a *deployed* backend API.\n",
        "It cannot be run interactively within this Colab notebook due to how Streamlit serves applications and Colab's environment limitations.\n",
        "To test this frontend, you would typically:\n",
        "1. Save this code as `frontend_app.py`.\n",
        "2. Deploy your backend AI logic (e.g., as a FastAPI service on Google Cloud Run).\n",
        "3. Set the `BACKEND_API_URL` environment variable (or hardcode it for testing) to your deployed backend's URL.\n",
        "4. Run `streamlit run frontend_app.py` in your local environment or deploy it to a service like Streamlit Community Cloud or Google Cloud Run.\n",
        "\"\"\", unsafe_allow_html=True)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeltaGenerator()"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59c4a667"
      },
      "source": [
        "# Task\n",
        "Here's a summary of the separated code structure, instructions for deploying each component to Google Cloud Run, and an acknowledgment of the testing limitations within Colab:\n",
        "\n",
        "---\n",
        "\n",
        "## Separated Code Structure Summary\n",
        "\n",
        "The original monolithic notebook has been refactored into two distinct components: a **backend agentic AI service** and a **frontend Streamlit application**. This separation allows for independent development, deployment, and scaling of each part.\n",
        "\n",
        "1.  **`backend.py` (Agentic AI Logic)**\n",
        "    *   **Purpose**: Contains the core business logic, including the `Tool`, `Agent`, and `CoordinatorAgent` classes, the LLM integration (`gemini-2.5-flash`), URL text extraction, and the resume/job description analysis functions.\n",
        "    *   **Dependencies**: `google-generativeai`, `requests`, `BeautifulSoup4`, `PyPDF2`, `nest_asyncio`, `python-dotenv`.\n",
        "    *   **API Key Handling**: Securely loads `GOOGLE_API_KEY` from Colab secrets (if running in Colab) or from a `.env` file/environment variables for local/cloud deployment.\n",
        "    *   **Interface**: Designed to be exposed via a REST API (e.g., using FastAPI or Flask) when deployed, accepting `resume_text` and `job_description_text` and returning a JSON analysis report. The `run_live` method of the `CoordinatorAgent` is the primary entry point for triggering the analysis.\n",
        "    *   **Streamlit-Free**: All Streamlit-specific UI calls have been replaced with `print` statements or removed to ensure it can run as a standalone service.\n",
        "\n",
        "2.  **`frontend_app.py` (Streamlit Application)**\n",
        "    *   **Purpose**: Provides the interactive user interface for uploading resumes, entering job description URLs, and displaying the analysis results.\n",
        "    *   **Dependencies**: `streamlit`, `requests`, `BeautifulSoup4`, `PyPDF2`.\n",
        "    *   **Backend Interaction**: This frontend is designed to interact with a *deployed* backend service. It sends the extracted resume and job description text to a specified `BACKEND_API_URL` (which would be your deployed backend's endpoint) and processes the JSON response.\n",
        "    *   **Local URL Extraction**: The `extract_text_from_url` helper function is duplicated here to allow the frontend to fetch job descriptions directly from the user's browser context before sending the content to the backend. This prevents cross-origin issues and offloads some work from the backend.\n",
        "    *   **User Experience**: Includes progress indicators, error handling for API calls, and a structured display of the analysis report.\n",
        "\n",
        "---\n",
        "\n",
        "## Deployment Instructions for Google Cloud Run\n",
        "\n",
        "Both the backend and frontend components can be deployed independently to Google Cloud Run, a fully managed compute platform for stateless containers.\n",
        "\n",
        "### 1. Backend Service Deployment (e.g., as `fastapi-backend`)\n",
        "\n",
        "To deploy the backend, you'll typically wrap the core logic in a web framework like FastAPI and containerize it.\n",
        "\n",
        "**`backend_main.py` (Example FastAPI application)**:\n",
        "```python\n",
        "# Save this content as backend_main.py\n",
        "import os\n",
        "import io\n",
        "import PyPDF2\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "import random\n",
        "import re\n",
        "import json\n",
        "from google import generativeai as genai\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from dotenv import load_dotenv\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from pydantic import BaseModel\n",
        "\n",
        "# Apply nest_asyncio for compatibility\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# --- API Key Handling ---\n",
        "load_dotenv() # Load environment variables from .env file\n",
        "GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')\n",
        "\n",
        "if not GOOGLE_API_KEY:\n",
        "    raise ValueError(\"GOOGLE_API_KEY not found. Please set it as an environment variable.\")\n",
        "\n",
        "# Configure the Gemini client\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "llm_model_name = \"gemini-2.5-flash\"\n",
        "\n",
        "# Helper function to extract text from URL (same as in backend_logic.py)\n",
        "def extract_text_from_url(url: str) -> str:\n",
        "    # ... (same as the extract_text_from_url function in the backend code cell)\n",
        "    try:\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/50 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "        }\n",
        "        response = requests.get(url, headers=headers, timeout=30)\n",
        "        response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Remove script and style elements\n",
        "        for script_or_style in soup(['script', 'style']):\n",
        "            script_or_style.extract()\n",
        "\n",
        "        # Get text and clean it\n",
        "        text = soup.get_text()\n",
        "        lines = (line.strip() for line in text.splitlines())\n",
        "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
        "        text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
        "        return text\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching URL {url}: {e}\")\n",
        "        return \"\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing URL {url}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def analyze_skills_and_gaps(resume_text: str, job_description_text: str) -> str:\n",
        "    # ... (same as the analyze_skills_and_gaps function in the backend code cell)\n",
        "    full_prompt = f\"\"\"\n",
        "    You are an expert HR analyst. Your task is to compare a candidate's resume with a job description.\n",
        "    Provide your output as a JSON object ONLY. Do not include any other text or explanation outside the JSON.\n",
        "\n",
        "    Here is the candidate's Resume:\n",
        "    ---\n",
        "    {resume_text}\n",
        "    ---\n",
        "\n",
        "    Here is the Job Description:\n",
        "    ---\n",
        "    {job_description_text}\n",
        "    ---\n",
        "\n",
        "    JSON Schema:\n",
        "    {{\n",
        "        \"candidate_skills\": [\"string\"], # List of key technical and soft skills explicitly mentioned in the resume.\n",
        "        \"required_job_skills\": [\"string\"], # List of essential technical and soft skills mentioned in the job description.\n",
        "        \"matched_skills\": [\"string\"], # Skills present in both the resume and the job description.\n",
        "        \"missing_skills\": [\"string\"], # Skills required by the job description but NOT found in the resume.\n",
        "        \"additional_skills\": [\"string\"], # Skills present in the resume but not explicitly required by the job description.\n",
        "        \"overall_fit_summary\": \"string\" # A brief summary of how well the candidate's skills align with the job requirements.\n",
        "    }}\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        model = genai.GenerativeModel(llm_model_name)\n",
        "        response = model.generate_content(contents=full_prompt)\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        return f\"Error during LLM analysis: {e}\"\n",
        "\n",
        "def analyze_resume_job_description_full(resume_text: str, job_description_text: str) -> dict:\n",
        "    # ... (same as the analyze_resume_job_description_full function in the backend code cell)\n",
        "    print(f\"Initiating LLM-based analysis for resume (length: {len(resume_text)}) and job description (length: {len(job_description_text)}).\")\n",
        "    analysis_report = analyze_skills_and_gaps(resume_text, job_description_text)\n",
        "\n",
        "    if \"Error during LLM analysis\" in analysis_report:\n",
        "        return {\"analysis_status\": \"failure\", \"message\": analysis_report}\n",
        "    else:\n",
        "        try:\n",
        "            cleaned_report = analysis_report.strip()\n",
        "            if cleaned_report.startswith('```json') and cleaned_report.endswith('```'):\n",
        "                cleaned_report = cleaned_report[len('```json'):-len('```')].strip()\n",
        "\n",
        "            parsed_report = json.loads(cleaned_report)\n",
        "            return {\"analysis_status\": \"success\", \"message\": \"LLM-based analysis completed and parsed.\", \"parsed_report\": parsed_report}\n",
        "        except json.JSONDecodeError as e:\n",
        "            return {\"analysis_status\": \"failure\", \"message\": f\"Failed to parse LLM output as JSON: {e}\", \"raw_report\": analysis_report}\n",
        "        except Exception as e:\n",
        "            return {\"analysis_status\": \"failure\", \"message\": f\"An unexpected error occurred during JSON parsing: {e}\", \"raw_report\": analysis_report}\n",
        "\n",
        "\n",
        "# Tool and Agent class definitions (same as in backend code cell)\n",
        "class Tool:\n",
        "    def __init__(self, func, name, description):\n",
        "        self.func = func\n",
        "        self.name = name\n",
        "        self.description = description\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, name, instruction, tools: list):\n",
        "        self.name = name\n",
        "        self.instruction = instruction\n",
        "        self.tools = tools\n",
        "\n",
        "class CoordinatorAgent(Agent):\n",
        "    def __init__(self, name: str, instruction: str, tools: list = None, sub_agents: list = None):\n",
        "        super().__init__(name, instruction, tools if tools is not None else [])\n",
        "        self.sub_agents = sub_agents if sub_agents is not None else []\n",
        "\n",
        "    async def run_live(self, resume_text: str, job_description_text: str):\n",
        "        # This async generator needs to be adapted for a direct API call.\n",
        "        # For a backend, we just want the final result, not streaming messages.\n",
        "        candidate_agent_found = next((agent for agent in self.sub_agents if agent.name == \"candidate_agent\"), None)\n",
        "        if not candidate_agent_found:\n",
        "            raise ValueError(\"candidate_agent not found.\")\n",
        "\n",
        "        analysis_tool_instance = next((tool for tool in candidate_agent_found.tools if tool.name == \"analyze_resume_job_description\"), None)\n",
        "        if not analysis_tool_instance:\n",
        "            raise ValueError(\"analyze_resume_job_description tool not found for candidate_agent.\")\n",
        "\n",
        "        analysis_result = analysis_tool_instance.func(resume_text, job_description_text)\n",
        "        return analysis_result\n",
        "\n",
        "# Re-define Tool and Agent instances for FastAPI context\n",
        "analysis_tool = Tool(\n",
        "    func=analyze_resume_job_description_full,\n",
        "    name=\"analyze_resume_job_description\",\n",
        "    description=\"Analyzes a candidate's resume against a job description to identify skills and gaps using an LLM.\"\n",
        ")\n",
        "\n",
        "candidate_agent = Agent(\n",
        "    name=\"candidate_agent\",\n",
        "    instruction=\"I manage candidate profiles and analyze resumes against job descriptions.\",\n",
        "    tools=[analysis_tool]\n",
        ")\n",
        "\n",
        "root_agent = CoordinatorAgent(\n",
        "    name=\"root_agent\",\n",
        "    instruction=\"I orchestrate the resume and job description analysis process.\",\n",
        "    sub_agents=[candidate_agent]\n",
        ")\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "class AnalysisRequest(BaseModel):\n",
        "    resume_text: str\n",
        "    job_description_text: str\n",
        "\n",
        "@app.post(\"/analyze\")\n",
        "async def analyze(request: AnalysisRequest):\n",
        "    try:\n",
        "        # Call the synchronous analysis function directly or adapt CoordinatorAgent's run_live\n",
        "        # For this backend, we'll call the core function that returns a dict\n",
        "        analysis_result = root_agent.sub_agents[0].tools[0].func(request.resume_text, request.job_description_text)\n",
        "        if analysis_result.get('analysis_status') == 'failure':\n",
        "            raise HTTPException(status_code=500, detail=analysis_result)\n",
        "        return analysis_result\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail={\"message\": str(e)})\n",
        "\n",
        "@app.get(\"/\")\n",
        "async def health_check():\n",
        "    return {\"status\": \"ok\", \"message\": \"AI Resume Analyzer Backend is running\"}\n",
        "\n",
        "```\n",
        "\n",
        "**`requirements.txt` (for backend)**:\n",
        "```\n",
        "fastapi\n",
        "uvicorn\n",
        "python-dotenv\n",
        "google-generativeai==0.8.5 # Ensure correct version based on previous steps\n",
        "requests\n",
        "beautifulsoup4\n",
        "pypdf2\n",
        "nest-asyncio\n",
        "```\n",
        "\n",
        "**`Dockerfile` (for backend)**:\n",
        "```dockerfile\n",
        "# Use the official Python image as a base\n",
        "FROM python:3.9-slim-buster\n",
        "\n",
        "# Set the working directory in the container\n",
        "WORKDIR /app\n",
        "\n",
        "# Copy the requirements file into the container\n",
        "COPY requirements.txt .\n",
        "\n",
        "# Install the Python dependencies\n",
        "RUN pip install --no-cache-dir -r requirements.txt\n",
        "\n",
        "# Copy the application code into the container\n",
        "COPY backend_main.py .\n",
        "\n",
        "# Expose the port that the application listens on\n",
        "EXPOSE 8000\n",
        "\n",
        "# Run the application using Uvicorn\n",
        "CMD [\"uvicorn\", \"backend_main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
        "```\n",
        "\n",
        "**Deployment Steps (Backend)**:\n",
        "1.  **Save Files**: Create `backend_main.py`, `requirements.txt`, and `Dockerfile` in a new directory (e.g., `backend_service`).\n",
        "2.  **Authenticate GCP**: Ensure you are authenticated to Google Cloud. If using Colab, you'd typically do this via `gcloud auth login` in your local terminal or configure Colab for Cloud SDK if deploying from Colab itself (though local deployment is usually preferred for Cloud Run).\n",
        "3.  **Set Project**: `gcloud config set project YOUR_GCP_PROJECT_ID`\n",
        "4.  **Build and Push Docker Image**:\n",
        "    ```bash\n",
        "    gcloud builds submit --tag gcr.io/YOUR_GCP_PROJECT_ID/ai-resume-analyzer-backend ./backend_service\n",
        "    ```\n",
        "5.  **Deploy to Cloud Run**:\n",
        "    ```bash\n",
        "    gcloud run deploy ai-resume-analyzer-backend \\\n",
        "      --image gcr.io/YOUR_GCP_PROJECT_ID/ai-resume-analyzer-backend \\\n",
        "      --platform managed \\\n",
        "      --region YOUR_GCP_REGION \\\n",
        "      --allow-unauthenticated \\\n",
        "      --set-env-vars GOOGLE_API_KEY=YOUR_GEMINI_API_KEY \\\n",
        "      --max-instances 1 \\\n",
        "      --cpu 1 \\\n",
        "      --memory 2Gi \\\n",
        "      --timeout 300\n",
        "    ```\n",
        "    *   Replace `YOUR_GCP_PROJECT_ID`, `YOUR_GCP_REGION`, and `YOUR_GEMINI_API_KEY`.\n",
        "    *   `--allow-unauthenticated` makes the service publicly accessible. Remove for private services.\n",
        "    *   `--max-instances`, `--cpu`, `--memory`, `--timeout` are important for performance and cost. Adjust as needed. LLM calls can be slow, so `timeout` should be generous.\n",
        "    *   Make note of the **Service URL** provided after deployment; this will be your `BACKEND_API_URL`.\n",
        "\n",
        "### 2. Frontend Service Deployment (Streamlit Application)\n",
        "\n",
        "The Streamlit frontend also needs to be containerized and deployed.\n",
        "\n",
        "**`frontend_app.py` (Same content as the last Streamlit code cell output)**:\n",
        "```python\n",
        "# Save this content as frontend_app.py\n",
        "import streamlit as st\n",
        "import os\n",
        "import io\n",
        "import PyPDF2\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "\n",
        "# --- Helper function for URL extraction (copied from backend) ---\n",
        "def extract_text_from_url(url: str) -> str:\n",
        "    \"\"\"Extracts text content from a given URL, typically for a job description.\"\"\"\n",
        "    try:\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "        }\n",
        "        response = requests.get(url, headers=headers, timeout=30)\n",
        "        response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Remove script and style elements\n",
        "        for script_or_style in soup(['script', 'style']):\n",
        "            script_or_style.extract()\n",
        "\n",
        "        # Get text and clean it\n",
        "        text = soup.get_text()\n",
        "        lines = (line.strip() for line in text.splitlines())\n",
        "        # Break multi-headlines into a line each\n",
        "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
        "        # Drop blank lines\n",
        "        text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
        "        return text\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        st.error(f\"Error fetching URL {url}: {e}\")\n",
        "        return \"\"\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error processing URL {url}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "# --- Streamlit UI and Workflow Integration ---\n",
        "\n",
        "st.set_page_config(\n",
        "    page_title=\"AI-Powered Resume and Job Description Analyzer (Frontend)\",\n",
        "    layout=\"wide\",\n",
        "    initial_sidebar_state=\"expanded\"\n",
        ")\n",
        "st.markdown(\"<h1 style='text-align: center; color: #4CAF50;'> üîç AI Job Search Assistant (Frontend) </h1> <p style='text-align:center; font-size:18px;'> Discover tailored job recommendations powered by Agentic AI. </p>\", unsafe_allow_html=True)\n",
        "st.sidebar.header(\"User Inputs\")\n",
        "\n",
        "# Hypothetical backend API endpoint\n",
        "# In a real deployment, this URL would point to your deployed Google Cloud Run service.\n",
        "BACKEND_API_URL = os.getenv(\"BACKEND_API_URL\", \"http://localhost:8000/analyze\") # Placeholder URL for local testing or environment variable\n",
        "\n",
        "st.sidebar.markdown(f\"*(Backend API URL: `{BACKEND_API_URL}`)*\")\n",
        "st.sidebar.markdown(\"---\")\n",
        "\n",
        "\n",
        "job_url_input = st.sidebar.text_input(\n",
        "    \"Job Description URL\",\n",
        "    value=\"https://example.com/job_description\",\n",
        "    help=\"Enter the URL of the job description webpage.\"\n",
        ")\n",
        "\n",
        "uploaded_resume_file = st.sidebar.file_uploader(\n",
        "    \"Upload Your Resume (PDF)\",\n",
        "    type=[\"pdf\"],\n",
        "    help=\"Upload your resume in PDF format.\"\n",
        ")\n",
        "\n",
        "is_valid_job_url = False\n",
        "if job_url_input:\n",
        "    if job_url_input.startswith(\"http://\") or job_url_input.startswith(\"https://\"):\n",
        "        is_valid_job_url = True\n",
        "    else:\n",
        "        st.sidebar.error(\"Please enter a valid URL (starting with http:// or https://).\")\n",
        "\n",
        "is_resume_uploaded = False\n",
        "if uploaded_resume_file is not None:\n",
        "    is_resume_uploaded = True\n",
        "\n",
        "if st.sidebar.button(\"Run Analysis\", disabled=(not is_valid_job_url or not is_resume_uploaded)):\n",
        "    if is_valid_job_url and is_resume_uploaded:\n",
        "        with st.spinner(\"Processing resume and fetching job description...\"):\n",
        "            resume_text = \"\"\n",
        "            try:\n",
        "                pdf_reader = PyPDF2.PdfReader(io.BytesIO(uploaded_resume_file.getvalue()))\n",
        "                resume_text = \"\".join([page.extract_text() for page in pdf_reader.pages])\n",
        "                st.success(\"Resume extracted successfully.\")\n",
        "            except Exception as e:\n",
        "                st.error(f\"Error reading resume PDF: {e}\")\n",
        "                resume_text = \"\"\n",
        "\n",
        "            job_description_text = \"\"\n",
        "            try:\n",
        "                job_description_text = extract_text_from_url(job_url_input)\n",
        "                if job_description_text:\n",
        "                    st.success(\"Job description fetched successfully.\")\n",
        "                else:\n",
        "                    st.error(\"Failed to fetch job description. Please check the URL.\")\n",
        "            except Exception as e:\n",
        "                st.error(f\"Error fetching job description from URL: {e}\")\n",
        "                job_description_text = \"\"\n",
        "\n",
        "        if resume_text and job_description_text:\n",
        "            st.subheader(\"AI Analysis Report\")\n",
        "            progress_bar = st.progress(0)\n",
        "            status_text = st.empty()\n",
        "            report_container = st.empty()\n",
        "\n",
        "            status_text.text(\"Sending data to backend for analysis...\")\n",
        "            progress_bar.progress(25)\n",
        "\n",
        "            try:\n",
        "                payload = {\n",
        "                    \"resume_text\": resume_text,\n",
        "                    \"job_description_text\": job_description_text\n",
        "                }\n",
        "                headers = {\"Content-Type\": \"application/json\"}\n",
        "\n",
        "                # Make a POST request to the backend API\n",
        "                response = requests.post(BACKEND_API_URL, json=payload, headers=headers, timeout=120) # Increased timeout for LLM calls\n",
        "                response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)\n",
        "\n",
        "                analysis_result = response.json()\n",
        "\n",
        "                if analysis_result.get('analysis_status') == 'success':\n",
        "                    status_text.text(\"‚úÖ Analysis complete: Received report from backend.\")\n",
        "                    progress_bar.progress(100)\n",
        "\n",
        "                    parsed_report = analysis_result.get('parsed_report', {})\n",
        "                    report_html_parts = []\n",
        "\n",
        "                    report_html_parts.append(\"<hr/>\")\n",
        "                    # Overall Fit Summary\n",
        "                    overall_fit_summary = parsed_report.get('overall_fit_summary', 'N/A')\n",
        "                    report_html_parts.append(f\"<p><b>Overall Fit Summary:</b> {overall_fit_summary}</p>\")\n",
        "                    report_html_parts.append(\"<hr/>\")\n",
        "\n",
        "                    # Candidate Skills\n",
        "                    candidate_skills = parsed_report.get('candidate_skills', [])\n",
        "                    if candidate_skills:\n",
        "                        report_html_parts.append(\"<h4>Candidate Skills:</h4><ul>\")\n",
        "                        for skill in candidate_skills:\n",
        "                            report_html_parts.append(f\"<li>{skill}</li>\")\n",
        "                        report_html_parts.append(\"</ul>\")\n",
        "\n",
        "                    # Required Job Skills\n",
        "                    required_job_skills = parsed_report.get('required_job_skills', [])\n",
        "                    if required_job_skills:\n",
        "                        report_html_parts.append(\"<h4>Required Job Skills:</h4><ul>\")\n",
        "                        for skill in required_job_skills:\n",
        "                            report_html_parts.append(f\"<li>{skill}</li>\")\n",
        "                        report_html_parts.append(\"</ul>\")\n",
        "\n",
        "                    # Matched Skills\n",
        "                    matched_skills = parsed_report.get('matched_skills', [])\n",
        "                    if matched_skills:\n",
        "                        report_html_parts.append(\"<h4>Matched Skills:</h4><ul>\")\n",
        "                        for skill in matched_skills:\n",
        "                            report_html_parts.append(f\"<li>{skill}</li>\")\n",
        "                        report_html_parts.append(\"</ul>\")\n",
        "\n",
        "                    # Missing Skills\n",
        "                    missing_skills = parsed_report.get('missing_skills', [])\n",
        "                    if missing_skills:\n",
        "                        report_html_parts.append(\"<h4 style=\\\"color:red;\\\">Missing Skills (Gaps):</h4><ul>\")\n",
        "                        for skill in missing_skills:\n",
        "                            report_html_parts.append(f\"<li style=\\\"color:red;\\\">{skill}</li>\")\n",
        "                        report_html_parts.append(\"</ul>\")\n",
        "\n",
        "                    # Additional Skills\n",
        "                    additional_skills = parsed_report.get('additional_skills', [])\n",
        "                    if additional_skills:\n",
        "                        report_html_parts.append(\"<h4>Additional Skills:</h4><ul>\")\n",
        "                        for skill in additional_skills:\n",
        "                            report_html_parts.append(f\"<li>{skill}</li>\")\n",
        "                        report_html_parts.append(\"</ul>\")\n",
        "\n",
        "                    report_container.markdown(\"\\n\".join(report_html_parts), unsafe_allow_html=True)\n",
        "\n",
        "                else:\n",
        "                    status_text.text(f\"‚ùå Analysis failed: {analysis_result.get('message', 'Unknown error from backend.')}\")\n",
        "                    st.error(f\"Backend analysis failed: {analysis_result.get('message', 'Unknown error.')}\")\n",
        "                    if 'raw_report' in analysis_result:\n",
        "                        st.json(analysis_result['raw_report']) # Display raw report if available for debugging\n",
        "            except requests.exceptions.ConnectionError as ce:\n",
        "                st.error(f\"‚ùå Connection Error: Could not connect to the backend API at {BACKEND_API_URL}. Please ensure the backend is running and accessible. Details: {ce}\")\n",
        "            except requests.exceptions.Timeout as te:\n",
        "                st.error(f\"‚ùå Request Timeout: The backend API at {BACKEND_API_URL} took too long to respond. Details: {te}\")\n",
        "            except requests.exceptions.RequestException as re:\n",
        "                st.error(f\"‚ùå Error during API call to backend: {re}\")\n",
        "                if hasattr(re, 'response') and re.response is not None:\n",
        "                    try:\n",
        "                        st.error(f\"Backend Response: {re.response.json()}\")\n",
        "                    except json.JSONDecodeError:\n",
        "                        st.error(f\"Backend Response (raw): {re.response.text}\")\n",
        "            except Exception as e:\n",
        "                st.error(f\"‚ùå An unexpected error occurred during backend communication: {e}\")\n",
        "            finally:\n",
        "                progress_bar.empty()\n",
        "                status_text.empty()\n",
        "\n",
        "        else:\n",
        "            st.error(\"Analysis cannot be performed due to missing resume text or job description text.\")\n",
        "    else:\n",
        "        st.error(\"Please fix the input errors before running analysis.\")\n",
        "else:\n",
        "    if not is_valid_job_url or not is_resume_uploaded:\n",
        "        st.warning(\"Please provide a valid Job URL and upload your resume to proceed.\")\n",
        "    else:\n",
        "        st.success(\"Job URL and Resume uploaded successfully. Ready for analysis!\")\n",
        "\n",
        "st.markdown(\"\"\"\n",
        "---\n",
        "**Note for Colab Users:**\n",
        "This Streamlit application is designed to run as a separate frontend, interacting with a *deployed* backend API.\n",
        "It cannot be run interactively within this Colab notebook due to how Streamlit serves applications and Colab's environment limitations.\n",
        "To test this frontend, you would typically:\n",
        "1. Save this code as `frontend_app.py`.\n",
        "2. Deploy your backend AI logic (e.g., as a FastAPI service on Google Cloud Run).\n",
        "3. Set the `BACKEND_API_URL` environment variable (or hardcode it for testing) to your deployed backend's URL.\n",
        "4. Run `streamlit run frontend_app.py` in your local environment or deploy it to a service like Streamlit Community Cloud or Google Cloud Run.\n",
        "\"\"\", unsafe_allow_html=True)\n",
        "```\n",
        "\n",
        "**`.streamlit/config.toml` (Optional, for Streamlit server configuration)**:\n",
        "```toml\n",
        "# Save this content as .streamlit/config.toml inside a .streamlit folder\n",
        "[server]\n",
        "port = 8080\n",
        "enableCORS = false\n",
        "headless = true\n",
        "```\n",
        "\n",
        "**`requirements.txt` (for frontend)**:\n",
        "```\n",
        "streamlit==1.33.0\n",
        "PyPDF2==3.0.1\n",
        "requests==2.32.3\n",
        "beautifulsoup4==4.12.3\n",
        "```\n",
        "\n",
        "**`Dockerfile` (for frontend)**:\n",
        "```dockerfile\n",
        "# Use the official Python image as a base\n",
        "FROM python:3.9-slim-buster\n",
        "\n",
        "# Set the working directory in the container\n",
        "WORKDIR /app\n",
        "\n",
        "# Copy the Streamlit config folder\n",
        "COPY .streamlit/ .streamlit/\n",
        "\n",
        "# Copy the requirements file into the container\n",
        "COPY requirements.txt .\n",
        "\n",
        "# Install the Python dependencies\n",
        "RUN pip install --no-cache-dir -r requirements.txt\n",
        "\n",
        "# Copy the Streamlit application file into the container\n",
        "COPY frontend_app.py .\n",
        "\n",
        "# Expose the port that the Streamlit app listens on\n",
        "EXPOSE 8080\n",
        "\n",
        "# Command to run the Streamlit application\n",
        "CMD [\"streamlit\", \"run\", \"frontend_app.py\", \"--server.port=8080\", \"--server.enableCORS=false\", \"--server.headless=true\"]\n",
        "```\n",
        "\n",
        "**Deployment Steps (Frontend)**:\n",
        "1.  **Save Files**: Create `frontend_app.py`, `requirements.txt`, and a `.streamlit` folder containing `config.toml` in a new directory (e.g., `frontend_service`).\n",
        "2.  **Authenticate GCP**: Ensure you are authenticated to Google Cloud.\n",
        "3.  **Set Project**: `gcloud config set project YOUR_GCP_PROJECT_ID`\n",
        "4.  **Build and Push Docker Image**:\n",
        "    ```bash\n",
        "    gcloud builds submit --tag gcr.io/YOUR_GCP_PROJECT_ID/ai-resume-analyzer-frontend ./frontend_service\n",
        "    ```\n",
        "5.  **Deploy to Cloud Run**:\n",
        "    ```bash\n",
        "    gcloud run deploy ai-resume-analyzer-frontend \\\n",
        "      --image gcr.io/YOUR_GCP_PROJECT_ID/ai-resume-analyzer-frontend \\\n",
        "      --platform managed \\\n",
        "      --region YOUR_GCP_REGION \\\n",
        "      --allow-unauthenticated \\\n",
        "      --set-env-vars BACKEND_API_URL=YOUR_DEPLOYED_BACKEND_SERVICE_URL \\\n",
        "      --max-instances 1 \\\n",
        "      --cpu 1 \\\n",
        "      --memory 1Gi \\\n",
        "      --timeout 300\n",
        "    ```\n",
        "    *   Replace `YOUR_GCP_PROJECT_ID`, `YOUR_GCP_REGION`, and `YOUR_DEPLOYED_BACKEND_SERVICE_URL` (this is the URL you got from the backend deployment).\n",
        "    *   `--allow-unauthenticated` makes the service publicly accessible.\n",
        "    *   Streamlit apps can be memory-intensive; adjust `--memory` as needed.\n",
        "\n",
        "---\n",
        "\n",
        "## Limitations of Testing the Interactive Streamlit Frontend within Colab\n",
        "\n",
        "As noted in the Streamlit code itself:\n",
        "\n",
        "*   **Non-Interactive Execution**: Streamlit applications are web applications that require a dedicated server to run and serve content to a browser. When you execute Streamlit code directly in a Colab cell, it doesn't launch this web server in a way that's easily accessible or interactive within the Colab notebook environment itself.\n",
        "*   **\"Streamlit script run context\"**: You might see messages like \"Streamlit script run context detected.\" or output related to `DeltaGenerator()`, but this indicates that Streamlit's internal components are being initialized, not that a full interactive UI is being rendered that you can use.\n",
        "*   **External Access Required**: To interact with a Streamlit app, you would typically run `streamlit run your_app.py` in a terminal, and then access the app via a local URL (e.g., `http://localhost:8501`). Colab's architecture doesn't directly support this external web server paradigm for interactive UI within the notebook's output cells.\n",
        "*   **Workarounds (not fully interactive)**: While some methods exist (like `ngrok` or `localtunnel`) to expose a local server running in Colab to the internet, these are generally complex to set up and maintain for dynamic interactive sessions and are outside the scope of a typical Colab notebook's intended use.\n",
        "\n",
        "Therefore, the recommended way to test the interactive Streamlit frontend is to save the code to a `.py` file and run it locally on your machine, or deploy it to a platform like Streamlit Community Cloud or Google Cloud Run, pointing it to your deployed backend API."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8e8acbb1"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Provide a summary of the separated code structure, instructions on how to deploy each component to Google Cloud Run, and acknowledge the limitations of fully testing the interactive Streamlit frontend within the Colab environment.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d069ac0f"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   **Modular Architecture**: The original monolithic code has been successfully refactored into two distinct services: a `backend.py` component containing the core AI logic (LLM integration, agentic workflow, URL/PDF text extraction) and a `frontend_app.py` component for the interactive Streamlit user interface.\n",
        "*   **Backend Service (`backend.py`)**:\n",
        "    *   Wrapped with FastAPI (`backend_main.py`) to expose its functionality as a REST API.\n",
        "    *   Uses `google-generativeai`, `requests`, `BeautifulSoup4`, `PyPDF2` for its core operations.\n",
        "    *   Secures `GOOGLE_API_KEY` via environment variables or `.env` files.\n",
        "    *   Designed to accept `resume_text` and `job_description_text`, returning a JSON analysis report.\n",
        "*   **Frontend Application (`frontend_app.py`)**:\n",
        "    *   Built using Streamlit to provide an interactive user experience for uploading resumes and entering job description URLs.\n",
        "    *   Interacts with the deployed backend service via a `BACKEND_API_URL` environment variable.\n",
        "    *   Includes local URL text extraction to offload some processing and avoid cross-origin issues.\n",
        "*   **Cloud Run Deployment**: Both components are designed for independent deployment to Google Cloud Run using Docker containers, allowing for scalable and managed execution. Each requires its own `Dockerfile`, `requirements.txt`, and specific `gcloud run deploy` commands with environment variables (e.g., `GOOGLE_API_KEY` for backend, `BACKEND_API_URL` for frontend).\n",
        "*   **Colab Testing Limitations**: The interactive Streamlit frontend cannot be fully tested or run interactively within the Colab environment due to its web application nature and Colab's architecture. Local execution or deployment to a separate hosting platform (like Cloud Run) is necessary for full interactivity.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   **Embrace Microservices**: The separation into backend and frontend services significantly enhances maintainability, scalability, and independent development cycles, which is crucial for modern AI applications.\n",
        "*   **Production Deployment Best Practices**: For a production environment, implement robust error logging, monitoring, and consider authentication/authorization for the backend API to secure it beyond `--allow-unauthenticated`.\n",
        "*   **Local Development Setup**: To streamline development and testing, set up a local environment where both the FastAPI backend and Streamlit frontend can run concurrently, simulating the deployed Cloud Run interaction before actual deployment.\n"
      ]
    }
  ]
}