{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit PyPDF2 beautifulsoup4 requests google-generativeai nest-asyncio"
      ],
      "metadata": {
        "id": "ltu3SAojn68Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "KxKOZqX9l86E"
      },
      "outputs": [],
      "source": [
        "import streamlit as st\n",
        "import os\n",
        "import io\n",
        "import PyPDF2\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "import random\n",
        "import re\n",
        "import json\n",
        "from google import genai\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from dotenv import load_dotenv\n",
        "# Load .env file\n",
        "load_dotenv()\n",
        "\n",
        "# Check if Streamlit is running in a script context\n",
        "# This is a common way to check if the script is being executed by Streamlit or directly by Python\n",
        "IS_STREAMLIT_RUNNING = False\n",
        "if 'st' in globals() and hasattr(st, '_is_running_with_streamlit'): # Using the more common internal check\n",
        "    IS_STREAMLIT_RUNNING = st._is_running_with_streamlit\n",
        "    if IS_STREAMLIT_RUNNING:\n",
        "        print(\"Streamlit script run context detected.\")\n",
        "    else:\n",
        "        print(\"Streamlit module imported, but not running in a script run context.\")\n",
        "elif 'st' in globals() and hasattr(st, 'runtime') and hasattr(st.runtime, 'scriptrunner') and hasattr(st.runtime.scriptrunner, 'is_in_script_run_context'):\n",
        "    IS_STREAMLIT_RUNNING = st.runtime.scriptrunner.is_in_script_run_context()\n",
        "    if IS_STREAMLIT_RUNNING:\n",
        "        print(\"Streamlit script run context detected via runtime check.\")\n",
        "    else:\n",
        "        print(\"Streamlit module imported, but not running in a script run context via runtime check.\")\n",
        "else:\n",
        "    print(\"Streamlit module not fully initialized or relevant runtime attributes missing for context check.\")\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Helper function to parse salary from string (might not be used in current workflow but kept for consistency)\n",
        "def parse_salary(salary_str: str) -> int:\n",
        "    if not salary_str: return 0\n",
        "    # Remove non-numeric characters except comma, then remove comma, then convert to int\n",
        "    numeric_str = re.sub(r'[^\\\\d,]', '', salary_str)\n",
        "    numeric_str = numeric_str.replace(',', '')\n",
        "    try:\n",
        "        return int(numeric_str)\n",
        "    except ValueError:\n",
        "        return 0\n",
        "\n",
        "# Tool and Agent class definitions\n",
        "class Tool:\n",
        "    def __init__(self, func, name, description):\n",
        "        self.func = func\n",
        "        self.name = name\n",
        "        self.description = description\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, name, instruction, tools: list):\n",
        "        self.name = name\n",
        "        self.instruction = instruction\n",
        "        self.tools = tools\n",
        "\n",
        "# Helper function to extract text from URL\n",
        "def extract_text_from_url(url: str) -> str:\n",
        "    \"\"\"Extracts text content from a given URL, typically for a job description.\"\"\"\n",
        "    try:\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/50 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "        }\n",
        "        # Modified: Increased timeout from 10 to 30 seconds\n",
        "        response = requests.get(url, headers=headers, timeout=30)\n",
        "        response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Remove script and style elements\n",
        "        for script_or_style in soup(['script', 'style']):\n",
        "            script_or_style.extract()\n",
        "\n",
        "        # Get text and clean it\n",
        "        text = soup.get_text()\n",
        "        lines = (line.strip() for line in text.splitlines())\n",
        "        # Break multi-headlines into a line each\n",
        "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
        "        # Drop blank lines\n",
        "        text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
        "        return text\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        if IS_STREAMLIT_RUNNING:\n",
        "            st.error(f\"Error fetching URL {url}: {e}\")\n",
        "        else:\n",
        "            print(f\"Error fetching URL {url}: {e}\")\n",
        "        return \"\"\n",
        "    except Exception as e:\n",
        "        if IS_STREAMLIT_RUNNING:\n",
        "            st.error(f\"Error processing URL {url}: {e}\")\n",
        "        else:\n",
        "            print(f\"Error processing URL {url}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "llm_model_name = \"gemini-2.5-flash\" # Using a suitable Gemini model\n",
        "\n",
        "# Configure the Gemini client for google-genai 1.52.0+\n",
        "GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')\n",
        "gemini_model = genai.Client(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "def analyze_skills_and_gaps(resume_text: str, job_description_text: str) -> str:\n",
        "    \"\"\"Analyzes a candidate's resume against a job description using the LLM to identify skills and gaps.\"\"\"\n",
        "    # Modified: Combine system prompt into user prompt for Gemini's single-turn API\n",
        "    full_prompt = f\"\"\"\n",
        "    You are an expert HR analyst. Your task is to compare a candidate's resume with a job description.\n",
        "    Provide your output as a JSON object ONLY. Do not include any other text or explanation outside the JSON.\n",
        "\n",
        "    Here is the candidate's Resume:\n",
        "    ---\n",
        "    {resume_text}\n",
        "    ---\n",
        "\n",
        "    Here is the Job Description:\n",
        "    ---\n",
        "    {job_description_text}\n",
        "    ---\n",
        "\n",
        "    JSON Schema:\n",
        "    {{\n",
        "        \"candidate_skills\": [\"string\"], # List of key technical and soft skills explicitly mentioned in the resume.\n",
        "        \"required_job_skills\": [\"string\"], # List of essential technical and soft skills mentioned in the job description.\n",
        "        \"matched_skills\": [\"string\"], # Skills present in both the resume and the job description.\n",
        "        \"missing_skills\": [\"string\"], # Skills required by the job description but NOT found in the resume.\n",
        "        \"additional_skills\": [\"string\"], # Skills present in the resume but not explicitly required by the job description.\n",
        "        \"overall_fit_summary\": \"string\" # A brief summary of how well the candidate's skills align with the job requirements.\n",
        "    }}\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        response = gemini_model.models.generate_content(\n",
        "            model=llm_model_name,\n",
        "            contents=full_prompt\n",
        "        )\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        return f\"Error during LLM analysis: {e}\"\n",
        "\n",
        "def analyze_resume_job_description_full(resume_text: str, job_description_text: str) -> dict:\n",
        "    \"\"\"Performs a full resume and job description analysis using the LLM.\n",
        "    This function replaces the placeholder and calls analyze_skills_and_gaps.\n",
        "    \"\"\"\n",
        "    if IS_STREAMLIT_RUNNING:\n",
        "        st.info(f\"Initiating LLM-based analysis for resume (length: {len(resume_text)}) and job description (length: {len(job_description_text)}).\")\n",
        "    else:\n",
        "        print(f\"Initiating LLM-based analysis for resume (length: {len(resume_text)}) and job description (length: {len(job_description_text)}).\")\n",
        "    analysis_report = analyze_skills_and_gaps(resume_text, job_description_text)\n",
        "\n",
        "    if \"Error during LLM analysis\" in analysis_report:\n",
        "        return {\"analysis_status\": \"failure\", \"message\": analysis_report}\n",
        "    else:\n",
        "        try:\n",
        "            cleaned_report = analysis_report.strip()\n",
        "            if cleaned_report.startswith('```json') and cleaned_report.endswith('```'):\n",
        "                cleaned_report = cleaned_report[len('```json'):-len('```')].strip()\n",
        "\n",
        "            parsed_report = json.loads(cleaned_report)\n",
        "            return {\"analysis_status\": \"success\", \"message\": \"LLM-based analysis completed and parsed.\", \"parsed_report\": parsed_report}\n",
        "        except json.JSONDecodeError as e:\n",
        "            return {\"analysis_status\": \"failure\", \"message\": f\"Failed to parse LLM output as JSON: {e}\", \"raw_report\": analysis_report}\n",
        "        except Exception as e:\n",
        "            return {\"analysis_status\": \"failure\", \"message\": f\"An unexpected error occurred during JSON parsing: {e}\", \"raw_report\": analysis_report}\n",
        "\n",
        "# Re-define Tool instances\n",
        "analysis_tool = Tool(\n",
        "    func=analyze_resume_job_description_full,\n",
        "    name=\"analyze_resume_job_description\",\n",
        "    description=\"Analyzes a candidate's resume against a job description to identify skills and gaps using an LLM.\"\n",
        ")\n",
        "\n",
        "# Re-define Agent instances\n",
        "candidate_agent = Agent(\n",
        "    name=\"candidate_agent\",\n",
        "    instruction=\"I manage candidate profiles and analyze resumes against job descriptions.\",\n",
        "    tools=[analysis_tool]\n",
        ")\n",
        "\n",
        "# Redefine CoordinatorAgent to reflect new workflow\n",
        "class CoordinatorAgent(Agent):\n",
        "    def __init__(self, name: str, instruction: str, tools: list = None, sub_agents: list = None):\n",
        "        super().__init__(name, instruction, tools if tools is not None else [])\n",
        "        self.sub_agents = sub_agents if sub_agents is not None else []\n",
        "\n",
        "    async def run_live(self, resume_text: str, job_description_text: str):\n",
        "        yield f\"üöÄ CoordinatorAgent '{self.name}' initiating resume and job description analysis...\"\n",
        "\n",
        "        candidate_agent_found = next((agent for agent in self.sub_agents if agent.name == \"candidate_agent\"), None)\n",
        "        if not candidate_agent_found:\n",
        "            yield \"‚ùå Error: candidate_agent not found.\"\n",
        "            return\n",
        "\n",
        "        analysis_tool_instance = next((tool for tool in candidate_agent_found.tools if tool.name == \"analyze_resume_job_description\"), None)\n",
        "        if not analysis_tool_instance:\n",
        "            yield \"‚ùå Error: analyze_resume_job_description tool not found for candidate_agent.\"\n",
        "            return\n",
        "\n",
        "        yield f\"‚öôÔ∏è Delegating analysis to {candidate_agent_found.name} using {analysis_tool_instance.name} tool...\"\n",
        "        try:\n",
        "            analysis_result = analysis_tool_instance.func(resume_text, job_description_text)\n",
        "            if analysis_result.get('analysis_status') == 'success':\n",
        "                yield f\"‚úÖ Analysis complete: {analysis_result.get('message', 'No message provided.')}\"\n",
        "                yield \"<h2>Analysis Report</h2>\" # Moved here to display after initial messages\n",
        "\n",
        "                parsed_report = analysis_result.get('parsed_report', {})\n",
        "\n",
        "                report_html_parts = []\n",
        "\n",
        "                # Overall Fit Summary\n",
        "                overall_fit_summary = parsed_report.get('overall_fit_summary', 'N/A')\n",
        "                report_html_parts.append(f\"<p><b>Overall Fit Summary:</b> {overall_fit_summary}</p>\")\n",
        "\n",
        "                # Candidate Skills\n",
        "                candidate_skills = parsed_report.get('candidate_skills', [])\n",
        "                if candidate_skills:\n",
        "                    report_html_parts.append(\"<h4>Candidate Skills:</h4><ul>\")\n",
        "                    for skill in candidate_skills:\n",
        "                        report_html_parts.append(f\"<li>{skill}</li>\")\n",
        "                    report_html_parts.append(\"</ul>\")\n",
        "\n",
        "                # Required Job Skills\n",
        "                required_job_skills = parsed_report.get('required_job_skills', [])\n",
        "                if required_job_skills:\n",
        "                    report_html_parts.append(\"<h4>Required Job Skills:</h4><ul>\")\n",
        "                    for skill in required_job_skills:\n",
        "                        report_html_parts.append(f\"<li>{skill}</li>\")\n",
        "                    report_html_parts.append(\"</ul>\")\n",
        "\n",
        "                # Matched Skills\n",
        "                matched_skills = parsed_report.get('matched_skills', [])\n",
        "                if matched_skills:\n",
        "                    report_html_parts.append(\"<h4>Matched Skills:</h4><ul>\")\n",
        "                    for skill in matched_skills:\n",
        "                        report_html_parts.append(f\"<li>{skill}</li>\")\n",
        "                    report_html_parts.append(\"</ul>\")\n",
        "\n",
        "                # Missing Skills\n",
        "                missing_skills = parsed_report.get('missing_skills', [])\n",
        "                if missing_skills:\n",
        "                    report_html_parts.append(\"<h4 style=\\\"color:red;\\\">Missing Skills (Gaps):</h4><ul>\")\n",
        "                    for skill in missing_skills:\n",
        "                        report_html_parts.append(f\"<li style=\\\"color:red;\\\">{skill}</li>\")\n",
        "                    report_html_parts.append(\"</ul>\")\n",
        "\n",
        "                # Additional Skills\n",
        "                additional_skills = parsed_report.get('additional_skills', [])\n",
        "                if additional_skills:\n",
        "                    report_html_parts.append(\"<h4>Additional Skills:</h4><ul>\")\n",
        "                    for skill in additional_skills:\n",
        "                        report_html_parts.append(f\"<li>{skill}</li>\")\n",
        "                    report_html_parts.append(\"</ul>\")\n",
        "\n",
        "                yield \"\\n\".join(report_html_parts) # Yield the complete HTML string\n",
        "\n",
        "            else:\n",
        "                yield f\"‚ùå Analysis failed: {analysis_result.get('message', 'Unknown error.')}\"\n",
        "                if 'raw_report' in analysis_result:\n",
        "                    yield f\"Raw LLM output: {analysis_result['raw_report']}\"\n",
        "        except Exception as e:\n",
        "            yield f\"‚ùå Error during analysis: {e}\"\n",
        "            return\n",
        "\n",
        "# Re-instantiate the CoordinatorAgent with the new class definition and updated sub-agents\n",
        "root_agent = CoordinatorAgent(\n",
        "    name=\"root_agent\",\n",
        "    instruction=\"I orchestrate the resume and job description analysis process.\",\n",
        "    sub_agents=[candidate_agent]\n",
        ")\n",
        "\n",
        "\n",
        "# --- Streamlit UI and Workflow Integration ---\n",
        "\n",
        "st.set_page_config(\n",
        "    page_title=\"AI-Powered Resume and Job Description Analyzer\",\n",
        "    layout=\"wide\",\n",
        "    initial_sidebar_state=\"expanded\"\n",
        ")\n",
        "st.markdown(\"<h1 style='text-align: center; color: #4CAF50;'> üîç AI Job Search Assistant </h1> <p style='text-align:center; font-size:18px;'> Discover tailored job recommendations powered by Agentic AI. </p>\", unsafe_allow_html=True)\n",
        "st.sidebar.header(\"User Inputs\")\n",
        "\n",
        "job_url_input = st.sidebar.text_input(\n",
        "    \"Job Description URL\",\n",
        "    value=\"https://example.com/job_description\",\n",
        "    help=\"Enter the URL of the job description webpage.\"\n",
        ")\n",
        "\n",
        "uploaded_resume_file = st.sidebar.file_uploader(\n",
        "    \"Upload Your Resume (PDF)\",\n",
        "    type=[\"pdf\"],\n",
        "    help=\"Upload your resume in PDF format.\"\n",
        ")\n",
        "\n",
        "is_valid_job_url = False\n",
        "if job_url_input:\n",
        "    if job_url_input.startswith(\"http://\") or job_url_input.startswith(\"https://\"):\n",
        "        is_valid_job_url = True\n",
        "    else:\n",
        "        st.sidebar.error(\"Please enter a valid URL (starting with http:// or https://).\")\n",
        "\n",
        "is_resume_uploaded = False\n",
        "if uploaded_resume_file is not None:\n",
        "    is_resume_uploaded = True\n",
        "\n",
        "if st.sidebar.button(\"Run Analysis\", disabled=(not is_valid_job_url or not is_resume_uploaded)):\n",
        "    if is_valid_job_url and is_resume_uploaded:\n",
        "        with st.spinner(\"Processing resume and fetching job description...\"):\n",
        "            resume_text = \"\"\n",
        "            try:\n",
        "                pdf_reader = PyPDF2.PdfReader(io.BytesIO(uploaded_resume_file.getvalue()))\n",
        "                resume_text = \"\".join([page.extract_text() for page in pdf_reader.pages])\n",
        "                st.success(\"Resume extracted successfully.\")\n",
        "            except Exception as e:\n",
        "                st.error(f\"Error reading resume PDF: {e}\")\n",
        "                resume_text = \"\"\n",
        "\n",
        "            job_description_text = \"\"\n",
        "            try:\n",
        "                job_description_text = extract_text_from_url(job_url_input)\n",
        "                if job_description_text:\n",
        "                    st.success(\"Job description fetched successfully.\")\n",
        "                else:\n",
        "                    st.error(\"Failed to fetch job description. Please check the URL.\")\n",
        "            except Exception as e:\n",
        "                st.error(f\"Error fetching job description from URL: {e}\")\n",
        "                job_description_text = \"\"\n",
        "\n",
        "        if resume_text and job_description_text:\n",
        "            # st.subheader(\"Analysis Report\") # Removed this line\n",
        "            progress_bar = st.progress(0)\n",
        "            status_text = st.empty()\n",
        "            report_container = st.empty()\n",
        "\n",
        "            async def run_analysis_workflow_streamlit(res_text: str, jd_text: str):\n",
        "                output_lines = []\n",
        "                total_steps = 7\n",
        "                current_step = 0\n",
        "                async for step_output in root_agent.run_live(res_text, jd_text):\n",
        "                    output_lines.append(step_output)\n",
        "                    status_text.text(step_output)\n",
        "                    current_step += 1\n",
        "                    progress_bar.progress(min(current_step / total_steps, 1.0))\n",
        "                return output_lines\n",
        "\n",
        "            st.write(\"Starting AI analysis...\")\n",
        "            full_report_lines = []\n",
        "            # Use asyncio.run to execute the async generator\n",
        "            for step_output in asyncio.run(run_analysis_workflow_streamlit(resume_text, job_description_text)):\n",
        "                 full_report_lines.append(step_output)\n",
        "\n",
        "            progress_bar.empty()\n",
        "            status_text.empty()\n",
        "            report_container.markdown(\"\\n\".join(full_report_lines), unsafe_allow_html=True)\n",
        "\n",
        "        else:\n",
        "            st.error(\"Analysis cannot be performed due to missing resume text or job description text.\")\n",
        "    else:\n",
        "        st.error(\"Please fix the input errors before running analysis.\")\n",
        "else:\n",
        "    if not is_valid_job_url or not is_resume_uploaded:\n",
        "        st.warning(\"Please provide a valid Job URL and upload your resume to proceed.\")\n",
        "    else:\n",
        "        st.success(\"Job URL and Resume uploaded successfully. Ready for analysis!\")"
      ]
    }
  ]
}